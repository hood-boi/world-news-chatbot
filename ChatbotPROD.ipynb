{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatbotPROD.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/ChatbotPROD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN6xgjtZ9qye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY5Pu52_-B2l",
        "colab_type": "code",
        "outputId": "89df9de9-80d8-4ed5-d438-d1ecbe3fb798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swLeAlWu93D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\"\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "dataset1 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rCasualConversation.txt\", \"tsv\", fields); # name of the file\n",
        "dataset2 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rAskReddit.txt\", \"tsv\", fields); # name of the file\n",
        "dataset3 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rnews.txt\", \"tsv\", fields); # name of the file\n",
        "dataset4 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rworldnews.txt\", \"tsv\", fields); # name of the file\n",
        "dataset5 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/routoftheloop.txt\", \"tsv\", fields); # name of the file\n",
        "dataset6 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rnottheonion.txt\", \"tsv\", fields); # name of the file\n",
        "#dataset7 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rCasualConversation.txt\", \"tsv\", fields); # name of the file\n",
        "dataset = torch.utils.data.ConcatDataset([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLAl_ZT4AFYr",
        "colab_type": "code",
        "outputId": "52bb1171-e0ab-4ace-dd97-ef2a3ec39e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train = torchtext.data.Dataset(dataset, fields)\n",
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)\n",
        "print(text_field.vocab.itos)\n",
        "print(label_field.vocab.itos)\n",
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'y', 'g', 'p', 'f', 'w', 'b', '.', 'k', 'v', 'I', ',', 'T', 'S', 'A', '?', 'W', 'C', '0', '-', '/', 'M', 'x', 'j', 'P', 'H', 'D', 'B', 'N', 'E', 'R', ':', '1', 'F', 'O', '2', 'z', 'G', '!', 'L', 'U', 'Y', '*', 'J', '5', 'K', ')', 'q', '(', '3', '4', '9', ';', '8', '6', '7', 'V', '$', '_', ']', '[', '%', '=', '&', '^', '~', 'Z', 'Q', 'X', '#', '+', '\\\\', '|', '@', '`', '}', '{']\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'm', 'c', 'y', 'g', 'w', 'p', 'f', '.', 'b', 'k', 'v', 'I', ',', 'T', 'A', 'S', '/', '?', 'j', 'W', 'x', '-', 'H', '!', '0', 'C', 'M', 'N', 'E', 'B', 'O', 'D', 'P', '1', 'Y', 'R', ':', 'z', 'F', '*', 'L', 'G', '2', ')', '(', 'U', 'q', ';', '5', 'J', '3', 'K', '4', '9', '_', '8', '6', '7', 'V', ']', '[', '$', '^', '%', '=', '&', '~', 'Q', '#', 'Z', 'X', '+', '\\\\', '@', '|', '`', '{', '}']\n",
            "Input Vocab Size:  95\n",
            "Reply Vocab size:  95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKtPCLEWBQIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size,  \n",
        "                 reply_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ChatBot, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers =generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.LSTM(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Generator\n",
        "        self.reply_ident = torch.eye(reply_vocab_size)\n",
        "        self.decode_rnn = nn.LSTM(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, reply_vocab_size)\n",
        "        \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor)\n",
        "        \n",
        "        # >>> Generator\n",
        "        reply_tensor = self.reply_ident[response] #Type: batch.reply\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo2hAQN-lMfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ChatBot(input_vocab_size, reply_vocab_size, encoder_layers = 2, generator_layers = 2)\n",
        "model = model.cuda();\n",
        "model.context_ident = model.context_ident.cuda();\n",
        "model.reply_ident = model.reply_ident.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJn3trg9HrgM",
        "colab_type": "code",
        "outputId": "53fe33bf-d822-4b68-81fd-1880e46f2085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1499611"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcR-6mLdl_M6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fcn(model, data, reply_vocab_size, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    \n",
        "    data_iter = torchtext.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.context),\n",
        "                                              sort_within_batch=True)\n",
        "    print(\"Here #1 : \")\n",
        "    for e in range(num_epochs):\n",
        "        torch.save(model.state_dict(), 'weights_'+str(e))\n",
        "        print(\"Epoch : \", e);\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for batch in data_iter:\n",
        "            optimizer.zero_grad();\n",
        "            inp = batch.context[0] # BOS + EOS\n",
        "            inp = inp.cuda()\n",
        "            reply = batch.reply[:, :-1]\n",
        "            reply = reply.cuda()\n",
        "            \n",
        "            out, hidden = model(inp, reply) # no eos\n",
        "            out_reshaped = out.reshape(-1,reply_vocab_size)\n",
        "            target = (batch.reply[:,1:]).reshape(-1) # no bos\n",
        "            target = target.cuda()\n",
        "            #print(batch.reply[:,1:])\n",
        "            loss = criterion(out_reshaped, target)\n",
        "            \n",
        "            loss.backward();\n",
        "            optimizer.step();\n",
        "            \n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                avg_loss = 0\n",
        "        torch.save(model.state_dict(), 'weights_'+str(e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meh9s6ctmVws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_fcn(model, train, reply_vocab_size, batch_size =128, num_epochs = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRPK1Ni9_2Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_sequence(model, context, text_field, label_field, max_len=100, temperature=0.5):\n",
        "    generated_sequence = \"\"\n",
        "    \n",
        "    data_inp = [\"<BOS>\"] + list(context) + [\"<EOS>\"];\n",
        "    data_inp_indices = [text_field.vocab.stoi[ch] for ch in data_inp];\n",
        "    data_inp_tensor = torch.Tensor(data_inp_indices).long().unsqueeze(0);\n",
        "    \n",
        "    inp = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None;\n",
        "    for p in range(max_len):\n",
        "        #print(inp)\n",
        "        output, hidden = model(data_inp_tensor, inp.unsqueeze(0), hidden)\n",
        "        #print(output)\n",
        "        #output = F.softmax(output, dim=2)\n",
        "        #print(torch.argmax(output, dim=2))\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        #print(output_dist)\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = label_field.vocab.itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_veH1BXzpSX",
        "colab_type": "code",
        "outputId": "01b6fe3a-8a6f-437f-f55b-8281f784f02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "test_model = ChatBot(input_vocab_size, reply_vocab_size, encoder_layers = 2, generator_layers = 2)\n",
        "test_model.load_state_dict(torch.load('weights_2'))\n",
        "test_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatBot(\n",
              "  (encode_rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "  (decode_rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "  (fcnn): Linear(in_features=100, out_features=95, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_EnG6FpIRkq",
        "colab_type": "code",
        "outputId": "ddcbf438-7075-4407-a889-a4b89c0dd18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "sample_sequence(test_model, \"What is net neutrality?\", text_field, label_field)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Im a little some people are with the property of the country that would have seen a better comment t'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}