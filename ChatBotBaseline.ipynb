{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBotBaseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/ChatBotBaseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8a-_Oyx-23",
        "colab_type": "text"
      },
      "source": [
        "#Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr_ZuWCsxqYf",
        "colab_type": "code",
        "outputId": "ac34d044-b4bc-4bfa-c4be-a0da52d67ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKHeEMtFyCI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FisntbAjyEv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\"\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "dataset = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/routoftheloop.txt\", # name of the file\n",
        "                                        \"tsv\",               # fields are separated by a tab\n",
        "                                        fields)\n",
        "train = torchtext.data.Dataset(dataset, fields)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IMPjIEhzdP5",
        "colab_type": "code",
        "outputId": "8f7bd3dd-f2b9-4e0f-b31b-c5c6b0671322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)\n",
        "print(\"Context Vocab : \", text_field.vocab.itos)\n",
        "print(\"Reply Vocab : \", label_field.vocab.itos)\n",
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context Vocab :  ['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'p', 'g', 'w', 'y', 'f', 'b', '.', 'v', 'k', ',', '/', 'I', '?', 'T', 'W', 'A', '-', 'S', ':', 'x', 'j', '0', 'B', 'C', 'M', 'P', 'D', 'H', 'R', '1', 'E', 'N', ')', '(', '2', '_', 'F', 'z', 'O', 'L', 'G', '*', '3', 'Y', 'q', '4', '5', '9', '6', 'U', 'J', '8', '7', '[', ']', ';', 'K', '!', '=', 'V', 'Q', '~', 'Z', 'X', '&', '$', '%', '^', '#', '@', '+', '|', '\\\\', '{', '}', '`']\n",
            "Reply Vocab :  ['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'o', 'a', 'i', 's', 'n', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'y', 'g', 'p', 'w', 'f', '.', 'b', 'k', 'v', 'I', ',', '/', 'T', 'A', 'S', '?', '-', 'j', 'x', 'W', 'H', '0', ':', 'B', 'C', 'P', 'M', 'N', 'E', 'O', 'D', 'R', '1', ')', '(', 'Y', 'z', '*', 'L', 'F', '2', '_', 'q', 'G', '!', 'U', '3', ';', '4', '5', 'J', '8', 'K', '9', '6', ']', '[', '7', 'V', '=', '^', '%', '&', 'Q', '$', 'X', 'Z', '~', '#', '\\\\', '+', '@', '|', '`', '{', '}']\n",
            "Input Vocab Size:  95\n",
            "Reply Vocab size:  95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYY8_vGtTmVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size = 100,\n",
        "                hidden_layers = 1):\n",
        "        \n",
        "        super(EncoderAE, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size;\n",
        "        self.layers = hidden_layers;\n",
        "        self.hidden_size = hidden_size;\n",
        "        \n",
        "        self.ident = torch.eye(vocab_size)\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, hidden_layers, batch_first=True)\n",
        "        \n",
        "    def forward(self, target, hidden=None):\n",
        "        target_tensor = self.ident[target] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        h0 = torch.zeros(self.layers * 1, target.shape[0], self.hidden_size); # (num layers * direction, batch size, hidden size)\n",
        "        out, last_hidden = self.rnn(target_tensor, h0)\n",
        "        \n",
        "        return out, last_hidden;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MXlqBepVumS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size = 100,\n",
        "                hidden_layers = 1):\n",
        "        \n",
        "        super(DecoderAE, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size;\n",
        "        self.layers = hidden_layers;\n",
        "        self.hidden_size = hidden_size;\n",
        "        \n",
        "        self.ident = torch.eye(vocab_size)\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, hidden_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, target, embedding, hidden=None):\n",
        "        target_tensor =  self.ident[target]\n",
        "        if(hidden == None):\n",
        "            out, last_hidden = self.rnn(target_tensor, embedding)\n",
        "        else:\n",
        "            out, last_hidden = self.rnn(target_tensor, hidden)\n",
        "        out_final = self.fcnn(out)\n",
        "        return out_final, last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkaJgkQ4aV81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContextAE(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ContextAE, self).__init__()\n",
        "        \n",
        "        self.encoder = EncoderAE(context_vocab_size,encoder_hidden_size,encoder_layers)\n",
        "        self.decoder = DecoderAE(context_vocab_size,generator_hidden_size,generator_layers)\n",
        "        \n",
        "    def forward(self, target, hidden = None):\n",
        "        encode_out, encode_last_hidden = self.encoder(target, hidden)\n",
        "        decode_out, decode_last_hidden = self.decoder(target, encode_last_hidden,hidden)\n",
        "        \n",
        "        return decode_out, decode_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-PeKLRR01zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContextAE(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ContextAE, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers = generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.GRU(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Decoder\n",
        "        self.reply_ident = torch.eye(context_vocab_size)\n",
        "        self.decode_rnn = nn.GRU(context_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, context_vocab_size)\n",
        "        \n",
        "    def forward(self, context, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        h0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size); # (num layers * direction, batch size, hidden size)\n",
        "        #c0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size);\n",
        "        #encode_out, encode_last_hidden = self.encode_rnn(context_tensor, (h0,c0))\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor, h0)\n",
        "        # >>> Decoder\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(context_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(context_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P0qBTwC8MX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fcn(model, data, vocab_size, batch_size=1, num_epochs=1, lr=0.001, print_every=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    \n",
        "    data_iter = torchtext.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.context),\n",
        "                                              sort_within_batch=True)\n",
        "    print(\"Here #1 : \")\n",
        "    for e in range(num_epochs):\n",
        "        print(\"Epoch : \", e);\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for batch in data_iter:\n",
        "            inp = batch.context[0] # BOS + EOS\n",
        "            print(inp.shape)\n",
        "            target = inp;\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, _ = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                avg_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTiDC8w3-BDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_AE = ContextAE(input_vocab_size, encoder_hidden_size = 100, generator_hidden_size = 100 )\n",
        "train_fcn(model_AE, train, input_vocab_size, batch_size = 128, num_epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcC7_9slwjmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_AE.state_dict(), \"/content/gdrive/My Drive/Chatbot/AutoEncoderWeights2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwoTm52ndTRf",
        "colab_type": "code",
        "outputId": "af5da9a9-cbb2-4ba7-8191-f5a730efd570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model_AE.encoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderAE(\n",
              "  (rnn): GRU(95, 100, batch_first=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea0xgPu0eKp5",
        "colab_type": "code",
        "outputId": "6352b458-43b4-4f3d-c283-88752772efff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loadedModel = ContextAE(input_vocab_size, encoder_hidden_size = 100, generator_hidden_size = 100 );\n",
        "loadedModel.load_state_dict(torch.load('/content/gdrive/My Drive/Chatbot/AutoEncoderWeights2'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ84sdhweivq",
        "colab_type": "code",
        "outputId": "d4dbf9ba-ed70-4c0a-842b-da2f0d9dd6b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(loadedModel.encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderAE(\n",
            "  (rnn): GRU(95, 100, batch_first=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbhaFiI-prta",
        "colab_type": "code",
        "outputId": "90019848-90a8-42dc-9f1a-afb91eb27880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "k = 0\n",
        "for i in dataset:\n",
        "    print(i.reply)\n",
        "    k += 1;\n",
        "    if(k > 10):\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Here](https://www.reddit.com/r/OutOfTheLoop/comments/6mrjvt/why_does_snoo_say_monthly_bandwidth_exceeded/) is a question from July 12, which was the Internet-wide Day of Action for Net Neutrality. A lot of the top-level comments there ask other questions regarding net neutrality, so you may be able to find some more answers there.\n",
            "Whats the difference between each round of voting? It seems like every couple of months they have a new vote but Ive yet to see a post about the outcome of the vote, just more posts about a new upcoming vote.\n",
            "I see this as well. From my understanding, each time they get a huge push back, they let us forget and set a new time to try and push their decision. No matter what, they keep trying again and again. What keeps them from just doing this until they win?\n",
            "Nothing, except for a law. Which would require a different Administration and Congress.\n",
            "We need to make sure they always lose.\n",
            "That would make sense, no real verdict is reached, give it time for the storm to settle, then go back at it. Until a solid law against it is passed we will keep on having these votes.\n",
            ";let us forget *Try* to let us forget. As long as you keep on getting basically everyone to call in on a *bipartisan* line... I mean, from a rightist standpoint, that means Comcast can take down right-wing sites. From a leftist standpoint, Comcast can take down left-wing sites. And Im pretty sure gaming and movie streaming is something quite bipartisan...\n",
            "We have the same problem with Healthcare. We fight and fight and fight and they finally relent - and then a month later they try to kill it some other way again. The only solution is to vote them out and keep them out. Ask every candidate to go on the record in support of legislation that ensures a fully free and open internet. Dont stop fighting til we get it in writing.\n",
            "Im kinda confused on who actually does the voting. Is it congress or the fcc?\n",
            "I believe it is members of the FCC committee, with 5? commissioners including the chairman (Ajit Pai). All are appointees of the president for 5 years and are confirmed by the Senate. I believe a number of the commissioners were appointed by President Obama, but I am not sure who or how many of them there are.\n",
            "It is the FCC. The FCC is comprised of 5 commissioners (who matter). Two democrats and three republicans as it is right now. Pai was originally appointed by Obama and reinstated by Trump. However, Obama appointed someone else as commissioner at the time so he wasnt a concern. There always has to be 2 republicans and 2 democrats. Then the tie breakers is mostly chosen by presidential party.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2l0LIaUseBh",
        "colab_type": "code",
        "outputId": "f1efc56b-c4aa-4aeb-aa30-3c32e2d9e5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "i.reply"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It is the FCC. The FCC is comprised of 5 commissioners (who matter). Two democrats and three republicans as it is right now. Pai was originally appointed by Obama and reinstated by Trump. However, Obama appointed someone else as commissioner at the time so he wasnt a concern. There always has to be 2 republicans and 2 democrats. Then the tie breakers is mostly chosen by presidential party.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khy0QX9dsfTI",
        "colab_type": "code",
        "outputId": "06575ced-b73d-4ad0-c17b-abc5ffa8c4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "i.context"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Im kinda confused on who actually does the voting. Is it congress or the fcc?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNt0XkDsy8z",
        "colab_type": "code",
        "outputId": "8d661b29-7a2c-4285-f1d6-138ac55fd103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inp = [\"<BOS>\"] + list(i.context) + [\"<EOS>\"]\n",
        "inp_indices = [text_field.vocab.stoi[ch] for ch in inp]\n",
        "inp_tensor = torch.Tensor(inp_indices).long().unsqueeze(0)\n",
        "print(inp_tensor.shape)\n",
        "_, embedding = loadedModel.encoder(inp_tensor)\n",
        "#_, embedding2 = model_AE.encoder(inp_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 79])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoHY-xMdwbCQ",
        "colab_type": "code",
        "outputId": "2b829f63-791a-4740-e8b3-e9176c3219ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "print(embedding)\n",
        "#print(embedding2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.9983,  0.9935,  0.9973,  0.9991,  0.9986, -0.9985, -0.9987,\n",
            "           0.9971, -0.9997,  0.9975,  0.9993, -0.9975,  0.9985, -0.9969,\n",
            "          -0.0449,  0.9982,  0.9994,  0.9922,  0.9901, -0.9989,  0.9974,\n",
            "           0.9872, -0.9969,  0.9985, -0.9975, -0.6657, -0.9991,  0.9976,\n",
            "          -0.5789, -0.9990,  0.9807, -0.9973,  0.9964,  0.9885, -0.9992,\n",
            "           0.9968,  0.9992, -0.9973, -0.9990,  0.9949,  0.9976, -0.9865,\n",
            "          -0.9974, -0.9978,  0.9960,  0.9976, -0.9988,  0.9966, -0.9949,\n",
            "           0.9984,  0.2384, -0.9965,  0.9978, -0.9987, -0.9986,  0.9980,\n",
            "           0.9995,  0.9976, -0.9966,  0.9966, -0.9793, -0.9977, -0.9991,\n",
            "           0.9976, -0.9978, -0.9989,  0.9964,  0.9979,  0.9988, -0.9968,\n",
            "           0.9977,  0.9944,  0.9496, -0.9904,  0.9990,  0.9840,  0.9897,\n",
            "           0.9987, -0.9986,  0.9994, -0.9986, -0.9966,  0.9981, -0.9967,\n",
            "          -0.9958, -0.9974, -0.9923, -0.9988, -0.9977, -0.9981,  0.9998,\n",
            "           0.9993,  0.9985,  0.9951, -0.9993,  0.9986,  0.9993, -0.9965,\n",
            "           0.9971,  0.9989]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol16wnBjYJXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "6050ae22-0dd8-4cbf-9e03-4d9533f14a3c"
      },
      "source": [
        "embedding.squeeze(0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9983,  0.9935,  0.9973,  0.9991,  0.9986, -0.9985, -0.9987,  0.9971,\n",
              "         -0.9997,  0.9975,  0.9993, -0.9975,  0.9985, -0.9969, -0.0449,  0.9982,\n",
              "          0.9994,  0.9922,  0.9901, -0.9989,  0.9974,  0.9872, -0.9969,  0.9985,\n",
              "         -0.9975, -0.6657, -0.9991,  0.9976, -0.5789, -0.9990,  0.9807, -0.9973,\n",
              "          0.9964,  0.9885, -0.9992,  0.9968,  0.9992, -0.9973, -0.9990,  0.9949,\n",
              "          0.9976, -0.9865, -0.9974, -0.9978,  0.9960,  0.9976, -0.9988,  0.9966,\n",
              "         -0.9949,  0.9984,  0.2384, -0.9965,  0.9978, -0.9987, -0.9986,  0.9980,\n",
              "          0.9995,  0.9976, -0.9966,  0.9966, -0.9793, -0.9977, -0.9991,  0.9976,\n",
              "         -0.9978, -0.9989,  0.9964,  0.9979,  0.9988, -0.9968,  0.9977,  0.9944,\n",
              "          0.9496, -0.9904,  0.9990,  0.9840,  0.9897,  0.9987, -0.9986,  0.9994,\n",
              "         -0.9986, -0.9966,  0.9981, -0.9967, -0.9958, -0.9974, -0.9923, -0.9988,\n",
              "         -0.9977, -0.9981,  0.9998,  0.9993,  0.9985,  0.9951, -0.9993,  0.9986,\n",
              "          0.9993, -0.9965,  0.9971,  0.9989]], grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ-xJxofu4lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = {};\n",
        "replies = {};\n",
        "\n",
        "def buildBaseLineData():\n",
        "    k = 0;\n",
        "    for data in dataset:\n",
        "        data_inp = [\"<BOS>\"] + list(data.context) + [\"<EOS>\"]; # inp is a string\n",
        "        data_inp_indices = [text_field.vocab.stoi[ch] for ch in data_inp];\n",
        "        data_inp_tensor = torch.Tensor(data_inp_indices).long().unsqueeze(0);\n",
        "        _, emb = loadedModel.encoder(data_inp_tensor)\n",
        "        embeddings[data.context] = emb.squeeze(0)\n",
        "        replies[data.context] = data.reply\n",
        "        k += 1;\n",
        "        if(k > 10000):\n",
        "            break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB3f7XMRCID5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buildBaseLineData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6qLF5gaJDwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseLineModel(inp):\n",
        "    inp = [\"<BOS>\"] + list(inp) + [\"<EOS>\"]; # inp is a string\n",
        "    inp_indices = [text_field.vocab.stoi[ch] for ch in inp];\n",
        "    inp_tensor = torch.Tensor(inp_indices).long().unsqueeze(0);\n",
        "    _, inp_embeddings = loadedModel.encoder(inp_tensor)\n",
        "    inp_embeddings = inp_embeddings.squeeze(0)\n",
        "    \n",
        "    max_similarity = 0;\n",
        "    similar_context = \"\";\n",
        "    for context, embed in embeddings.items():\n",
        "        cosine_sim = torch.cosine_similarity(inp_embeddings, embed);\n",
        "        #print(cosine_sim , \"Context : \", context)\n",
        "        if(cosine_sim > max_similarity):\n",
        "            max_similarity = cosine_sim;\n",
        "            similar_context = context;\n",
        "    return replies[similar_context]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm-JCrI0b2U6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "071c7711-3459-4cc7-cafc-e15429cea2e2"
      },
      "source": [
        "baseLineModel(\"What do you know about Net Neutrality?\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'question: what is a Kyle? I see a lot of mentions of them in Area 51 memes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdxdHp3mEC8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(replies)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}