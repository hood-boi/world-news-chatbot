{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "World_News_Chatbot",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/World_News_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V3xTQ7i8u-M",
        "colab_type": "text"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIo7pCYeTro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renbNwSOVLW1",
        "colab_type": "code",
        "outputId": "b97d8a50-0af6-498f-920b-59ae4b4177d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0iSGmMVTaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqI0dB2JWvr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBReJMv2mhne",
        "colab_type": "code",
        "outputId": "f7096c0d-b191-42da-eeab-65074c434a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "def updateListingDict(postsJSON, postsDICT):\n",
        "    chunk_size = len(postsJSON[\"data\"][\"children\"])\n",
        "    posts = postsJSON[\"data\"][\"children\"]\n",
        "    \n",
        "    for i in range(chunk_size): #Loop through each reddit post\n",
        "        r_post_title = posts[i][\"data\"][\"title\"];\n",
        "        r_post_title = \" \".join(r_post_title.split());\n",
        "        r_post_title = r_post_title.replace('\\n', ' ');\n",
        "        r_post_title = r_post_title.replace('\\t', ' ');\n",
        "        r_post_title = r_post_title.replace('\\'', '');\n",
        "        r_post_title = r_post_title.replace('\\\"', '');\n",
        "        \n",
        "        postsDICT[posts[i][\"data\"][\"id\"]] = r_post_title;\n",
        "\n",
        "        \n",
        "    \n",
        "\n",
        "after = '';\n",
        "listing_dict = {};\n",
        "\n",
        "for i in range(0, 10):\n",
        "    if(after):\n",
        "        print(\"[AFTER] = \", after)\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100&after='+after, \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache'\n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "    else:\n",
        "        print(\"[AFTER] = NONE\")\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            #print(\"[GET] request successful\")\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "            \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[AFTER] = NONE\n",
            "[AFTER] =  t3_8cmonj\n",
            "[AFTER] =  t3_ajo3wy\n",
            "[AFTER] =  t3_5d2cwf\n",
            "[AFTER] =  t3_51l6zy\n",
            "[AFTER] =  t3_aek7g4\n",
            "[AFTER] =  t3_7cirxj\n",
            "[AFTER] =  t3_6ho8wq\n",
            "[AFTER] =  t3_ajg0p5\n",
            "[AFTER] =  t3_6y0tpf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LKYyZ9ytpXw",
        "colab_type": "code",
        "outputId": "1f43670b-8415-4e63-9431-194665f017db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(listing_dict))\n",
        "\n",
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82\n",
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8tHFJZYYzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXRYBVro193S",
        "colab_type": "code",
        "outputId": "7ee7173e-bc40-498d-d9e5-167c87bc00ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(key,value)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDiM4y_KYaSk",
        "colab_type": "code",
        "outputId": "71fb6780-db25-4e11-81b7-dfcc4c811846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#4/igGrpBUzgmmOTdhG77m8_Ej4eL8V8wcgV1kMDSd2zZel0dhlp_w-eek"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Ty605NYbf1",
        "colab_type": "code",
        "outputId": "1e66930d-1b84-42eb-867d-030b8ea171ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! mkdir /content/gdrive/My\\ Drive/Chatbot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Chatbot’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWVjdPuuYmSH",
        "colab_type": "code",
        "outputId": "ab587dbc-527b-4dcd-b386-189fc0f66906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! rm /content/gdrive/My\\ Drive/Chatbot/rWorldNews.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/Chatbot/rWorldNews.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0z7m3Tt57tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_handle = open('/content/gdrive/My Drive/Chatbot/rWorldNews.txt', 'a+')\n",
        "\n",
        "#Expecting array of JSON children\n",
        "def recursePostComments(context, commentsJSON):\n",
        "   \n",
        "    size = len(commentsJSON)\n",
        "    for i in range(0, size):\n",
        "        #print(commentsJSON[i])\n",
        "        if('data' in commentsJSON[i].keys()):\n",
        "            if('body' in commentsJSON[i]['data'].keys()):\n",
        "                reply = commentsJSON[i]['data']['body'];\n",
        "                if(\"[deleted]\" in reply or \"[removed]\" in reply):\n",
        "                    continue;\n",
        "                else:\n",
        "                    reply = reply.replace('\\n', ' ');\n",
        "                    reply = reply.replace('\\t', ' ');\n",
        "                    reply = reply.replace('\\'', '');\n",
        "                    reply = reply.replace('\\\"', '');\n",
        "                    reply = reply.replace('&gt', '');\n",
        "                    reply = reply.replace('&lt', '');\n",
        "                    #print(\"Context : \", context);\n",
        "                    \n",
        "                    #if (len(reply) > 500):\n",
        "                        #print(\"\\tLen :\", len(reply));\n",
        "                    #    t_reply = summarizeText(reply);\n",
        "                    #    if (len(t_reply) > 0):\n",
        "                    #        reply = t_reply;\n",
        "                    #print(\"\\tReply :\", reply);\n",
        "                    \n",
        "                    context.replace('\\t', ' ');\n",
        "                    \n",
        "                    reply.replace('\\t', ' ');\n",
        "                    reply = \" \".join(reply.split());\n",
        "                    \n",
        "                    if(\"Article has nothing\" in context):\n",
        "                        print(\"NIqqa\")\n",
        "                    \n",
        "                    if(reply.isspace() or context.isspace()):\n",
        "                        print(\"Error : \", reply, \", \", context)\n",
        "                        continue;                    \n",
        "                    else:\n",
        "                        line_to_write = \"{}\\t{}\\n\".format(reply.strip(), context.strip());\n",
        "                        file_handle.write(line_to_write)\n",
        "                    \n",
        "                    if('replies' in commentsJSON[i]['data'].keys()):\n",
        "                        if(commentsJSON[i]['data']['replies']):\n",
        "                            if('data' in commentsJSON[i]['data']['replies'].keys()):\n",
        "                                if(commentsJSON[i]['data']['replies']['data']):\n",
        "                                    if('children' in commentsJSON[i]['data']['replies']['data'].keys()):\n",
        "                                        recursePostComments(reply, commentsJSON[i]['data']['replies']['data']['children']);\n",
        "            else:\n",
        "                continue;\n",
        "        else:\n",
        "            continue;\n",
        "    return;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv0XIxKc7wdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    r_comments = requests.get('https://www.reddit.com/r/worldnews/comments/' + key +'.json?limit=10000&sort=confidence&depth=4', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "    r_comments_json = r_comments.json();\n",
        "    context = value;\n",
        "    commentsJSON = r_comments_json[1][\"data\"][\"children\"]\n",
        "    recursePostComments(context, commentsJSON)\n",
        "    \n",
        "file_handle.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBYwzxcD4Q5J",
        "colab_type": "text"
      },
      "source": [
        "#Overfitting and Testing Generative model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2rYrmca4QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1313H7z8i2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\"\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "#dataset = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rWorldNews.txt\", # name of the file\n",
        "dataset = torchtext.data.TabularDataset(\"test.txt\",\n",
        "                                        \"tsv\",               # fields are separated by a tab\n",
        "                                        fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqtnkW5vq0p1",
        "colab_type": "code",
        "outputId": "38dab91a-3f88-4e6e-fcbb-81579e07ef5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6HfmfsMrRFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = torchtext.data.Dataset(dataset, fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkEjPWJ9wM4",
        "colab_type": "code",
        "outputId": "6f80122d-c141-4232-e0e6-7291b83665f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AVuOzo97fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey-KokGc9-zC",
        "colab_type": "code",
        "outputId": "0a4bd4a6-e471-4b70-d437-f6e906bfd43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(text_field.vocab.itos)\n",
        "print(label_field.vocab.itos)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'o', 'e', 'l', ',', '?', 'H', 'a', 'h', 'r', 'u', 'w', 'y']\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'o', 'a', 'd', 'g', 'n', '!', 'I', 'h', 'i', 'k', 'm', 't', 'u', 'y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnH0XefaGkke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = torchtext.data.BucketIterator(train,\n",
        "                                           batch_size=1,\n",
        "                                           sort_key=lambda x: len(x.context), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False,\n",
        "                                          )                  # repeat the iterator for many epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeFntm9UbKB0",
        "colab_type": "code",
        "outputId": "d733cfff-e703-4efd-f4e6-010672a6a602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train_iter)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZVdxcMzHD8i",
        "colab_type": "code",
        "outputId": "deee7cfd-2545-4c03-dc1e-f987a2d1cce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "k = 0;\n",
        "for batch in train_iter:\n",
        "    \n",
        "    #max_length_seq = batch.context[0].shape[1];\n",
        "    #print(\"-----Batch #\", k+1, \"-----\");\n",
        "    #print(\"Maximum length of the input sequence : \", max_length_seq);\n",
        "    #padding_per_batch = int(torch.sum(max_length_seq - batch.sms[1]))\n",
        "    #print(\"Number of padding for batch : \", padding_per_batch)\n",
        "    \n",
        "    print(\"Batch Size : \", len(batch))\n",
        "    #print(\"Batch Context : \", batch.context)\n",
        "    print(\"Batch Context Shape: \", batch.context[0].shape)\n",
        "    #print(batch.reply)\n",
        "    print(\"Batch Context Shape: \", batch.reply.shape)\n",
        "    k += 1;\n",
        "    \n",
        "    if(k >= 1):\n",
        "        break;"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size :  1\n",
            "Batch Context Shape:  torch.Size([1, 21])\n",
            "Batch Context Shape:  torch.Size([1, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xQ-kcu4YD6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_to_string(token, context_field, reply_field):\n",
        "    context_array =[];\n",
        "    for i in range(0, token.context[0].shape[1]):\n",
        "        context_array.append(context_field.vocab.itos[(token.context[0][0][i])])\n",
        "\n",
        "    context_str = ''.join(context_array)\n",
        "\n",
        "\n",
        "\n",
        "    reply_array =[];\n",
        "    for i in range(0, token.reply.shape[1]):\n",
        "        reply_array.append(reply_field.vocab.itos[(token.reply[0][i])])\n",
        "\n",
        "    reply_str = ''.join(reply_array)\n",
        "\n",
        "    print(\"Context : \", context_str);\n",
        "    print(\"Reply : \", reply_str);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eMIEJme6VD",
        "colab_type": "code",
        "outputId": "f12fdc73-1532-4181-a3c6-9180751a2afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "embedding_to_string(batch, text_field, label_field)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context :  <BOS>Hello, how are you?<EOS>\n",
            "Reply :  <BOS>I am doing good thank you!<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCv9loPeVGKZ",
        "colab_type": "code",
        "outputId": "5e8cfc8b-7486-4e9d-f53c-418d13781690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Vocab Size:  17\n",
            "Reply Vocab size:  19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Kkzlo9ijXY",
        "colab_type": "code",
        "outputId": "d1fa7be9-4539-4218-81fe-562872036f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(batch.context[0][0])\n",
        "print(batch.reply)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2, 10,  6,  7,  7,  5,  8,  4, 12,  5, 15,  4, 11, 13,  6,  4, 16,  5,\n",
            "        14,  9,  3])\n",
            "tensor([[ 2, 11,  4,  6, 15,  4,  7,  5, 13,  9,  8,  4,  8,  5,  5,  7,  4, 16,\n",
            "         12,  6,  9, 14,  4, 18,  5, 17, 10,  3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10hkqBrfE6Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaZEAioohThl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2WbsjshJnY",
        "colab_type": "code",
        "outputId": "dbf0bfd3-00c9-4062-92ad-792b28c1cd99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#>>> BEGIN: Encoding Region\n",
        "\n",
        "context_ident = torch.eye(input_vocab_size)\n",
        "print(\"Shape 1 : \", batch.context[0].shape) # (batch size, sequence size)\n",
        "context_tensor = context_ident[batch.context[0]]\n",
        "\n",
        "print(\"Shape 2 : \", context_tensor.shape) # (batch size, sequence size, one hot embedding per size)\n",
        "encode_rnn = nn.GRU(input_vocab_size, 100, 1, batch_first=True)\n",
        "h0 = torch.zeros(1, batch.context[0].shape[0], 100); # (num layers * direction, batch size, hidden size)\n",
        "out, last_hidden = encode_rnn(context_tensor, h0)\n",
        "print(\"Shape 3 : \", out.shape)\n",
        "print(\"Shape 4 : \", last_hidden.shape)\n",
        "\n",
        "#<<< END: Encoding Region\n",
        "\n",
        "#>>> BEGIN: Teach forcing Generation Region\n",
        "reply_ident = torch.eye(reply_vocab_size)\n",
        "print(\"Shape 5 : \", batch.reply.shape)\n",
        "reply_tensor = reply_ident[batch.reply]\n",
        "\n",
        "print(\"Shape 6 : \", reply_tensor.shape)\n",
        "decode_rnn = nn.GRU(reply_vocab_size, 100, 1, batch_first=True)\n",
        "out2, last_hidden2 = decode_rnn(reply_tensor[:,:-1,:], last_hidden) # Don't pass in <EOS> token\n",
        "print(\"Shape 7 : \", out2.shape)\n",
        "print(\"Shape 8 : \", last_hidden2.shape)\n",
        "\n",
        "target = reply_tensor[:,1:,:]\n",
        "print(\"Shape 9 : \", target.shape)\n",
        "\n",
        "fcnn = nn.Linear(100, reply_vocab_size)\n",
        "\n",
        "final_destination = fcnn(out2)\n",
        "\n",
        "print(\"Shape 10 : \", final_destination.shape)\n",
        "\n",
        "\n",
        "temp1 = final_destination.reshape(-1, reply_vocab_size);\n",
        "temp2 = batch.reply[:,1:].reshape(-1)\n",
        "\n",
        "print(\"Shape 11 : \", temp1.shape)\n",
        "print(\"Shape 12 : \", temp2.shape)\n",
        "\n",
        "loss = criterion(temp1, temp2)\n",
        "print(\"Loss : \", loss.item())\n",
        "\n",
        "#<<< END: Teach forcing Generation Region\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape 1 :  torch.Size([1, 21])\n",
            "Shape 2 :  torch.Size([1, 21, 17])\n",
            "Shape 3 :  torch.Size([1, 21, 100])\n",
            "Shape 4 :  torch.Size([1, 1, 100])\n",
            "Shape 5 :  torch.Size([1, 28])\n",
            "Shape 6 :  torch.Size([1, 28, 19])\n",
            "Shape 7 :  torch.Size([1, 27, 100])\n",
            "Shape 8 :  torch.Size([1, 1, 100])\n",
            "Shape 9 :  torch.Size([1, 27, 19])\n",
            "Shape 10 :  torch.Size([1, 27, 19])\n",
            "Shape 11 :  torch.Size([27, 19])\n",
            "Shape 12 :  torch.Size([27])\n",
            "Loss :  2.963183641433716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt4d75QfhaBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size,  \n",
        "                 reply_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ChatBot, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers =generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.LSTM(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Generator\n",
        "        self.reply_ident = torch.eye(reply_vocab_size)\n",
        "        self.decode_rnn = nn.LSTM(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, reply_vocab_size)\n",
        "        \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        h0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size); # (num layers * direction, batch size, hidden size)\n",
        "        c0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size);\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor, (h0,c0))\n",
        "        \n",
        "        # >>> Generator\n",
        "        reply_tensor = self.reply_ident[response] #Type: batch.reply\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkS9nq_Y9ou4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ChatBot(input_vocab_size, reply_vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KJOIBMC90hz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64c9663f-a2e9-44ac-84f1-b358967a5576"
      },
      "source": [
        "for i in range(0,100):\n",
        "    optimizer.zero_grad();\n",
        "    out, hidden = model(batch.context[0], batch.reply[:, :-1]) # no eos\n",
        "    #print(batch.reply[:, :-1])\n",
        "    out_reshaped = out.reshape(-1,reply_vocab_size)\n",
        "    target = (batch.reply[:,1:]).reshape(-1) # no bos\n",
        "    #print(batch.reply[:,1:])\n",
        "    loss = criterion(out_reshaped, target)\n",
        "    print(loss)\n",
        "    loss.backward();\n",
        "    optimizer.step();"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.9458, grad_fn=<NllLossBackward>)\n",
            "tensor(2.8662, grad_fn=<NllLossBackward>)\n",
            "tensor(2.7489, grad_fn=<NllLossBackward>)\n",
            "tensor(2.6344, grad_fn=<NllLossBackward>)\n",
            "tensor(2.4585, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3723, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2937, grad_fn=<NllLossBackward>)\n",
            "tensor(2.1996, grad_fn=<NllLossBackward>)\n",
            "tensor(2.1079, grad_fn=<NllLossBackward>)\n",
            "tensor(2.0176, grad_fn=<NllLossBackward>)\n",
            "tensor(1.9062, grad_fn=<NllLossBackward>)\n",
            "tensor(1.8045, grad_fn=<NllLossBackward>)\n",
            "tensor(1.6905, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5973, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5393, grad_fn=<NllLossBackward>)\n",
            "tensor(1.4342, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2334, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1202, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4234, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2427, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0409, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0286, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0244, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0046, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0044, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0039, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0037, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCrN6skeKpdw",
        "colab_type": "code",
        "outputId": "52bb9a30-9fe4-44e4-82d3-399fe6c66a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "label_field.vocab.stoi[\"<BOS>\"]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py2TZslcKcFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    inp = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None;\n",
        "    for p in range(max_len):\n",
        "        #print(inp)\n",
        "        output, hidden = model(batch.context[0], inp.unsqueeze(0), hidden)\n",
        "        #print(output)\n",
        "        #output = F.softmax(output, dim=2)\n",
        "        #print(torch.argmax(output, dim=2))\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        #print(output_dist)\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = label_field.vocab.itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPcQXnBVLWmh",
        "colab_type": "code",
        "outputId": "79648871-1fa3-496f-d58e-9f225159927b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "sample_sequence(model, temperature=1.0)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am doing good thank you!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXlPwVbUdfmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long()\n",
        "print(inp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}