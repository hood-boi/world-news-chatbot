{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "World_News_Chatbot",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/World_News_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V3xTQ7i8u-M",
        "colab_type": "text"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIo7pCYeTro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renbNwSOVLW1",
        "colab_type": "code",
        "outputId": "b97d8a50-0af6-498f-920b-59ae4b4177d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0iSGmMVTaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqI0dB2JWvr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBReJMv2mhne",
        "colab_type": "code",
        "outputId": "f7096c0d-b191-42da-eeab-65074c434a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "def updateListingDict(postsJSON, postsDICT):\n",
        "    chunk_size = len(postsJSON[\"data\"][\"children\"])\n",
        "    posts = postsJSON[\"data\"][\"children\"]\n",
        "    \n",
        "    for i in range(chunk_size): #Loop through each reddit post\n",
        "        r_post_title = posts[i][\"data\"][\"title\"];\n",
        "        r_post_title = \" \".join(r_post_title.split());\n",
        "        r_post_title = r_post_title.replace('\\n', ' ');\n",
        "        r_post_title = r_post_title.replace('\\t', ' ');\n",
        "        r_post_title = r_post_title.replace('\\'', '');\n",
        "        r_post_title = r_post_title.replace('\\\"', '');\n",
        "        \n",
        "        postsDICT[posts[i][\"data\"][\"id\"]] = r_post_title;\n",
        "\n",
        "        \n",
        "    \n",
        "\n",
        "after = '';\n",
        "listing_dict = {};\n",
        "\n",
        "for i in range(0, 10):\n",
        "    if(after):\n",
        "        print(\"[AFTER] = \", after)\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100&after='+after, \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache'\n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "    else:\n",
        "        print(\"[AFTER] = NONE\")\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            #print(\"[GET] request successful\")\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "            \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[AFTER] = NONE\n",
            "[AFTER] =  t3_8cmonj\n",
            "[AFTER] =  t3_ajo3wy\n",
            "[AFTER] =  t3_5d2cwf\n",
            "[AFTER] =  t3_51l6zy\n",
            "[AFTER] =  t3_aek7g4\n",
            "[AFTER] =  t3_7cirxj\n",
            "[AFTER] =  t3_6ho8wq\n",
            "[AFTER] =  t3_ajg0p5\n",
            "[AFTER] =  t3_6y0tpf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LKYyZ9ytpXw",
        "colab_type": "code",
        "outputId": "1f43670b-8415-4e63-9431-194665f017db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(listing_dict))\n",
        "\n",
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82\n",
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8tHFJZYYzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXRYBVro193S",
        "colab_type": "code",
        "outputId": "7ee7173e-bc40-498d-d9e5-167c87bc00ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(key,value)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDiM4y_KYaSk",
        "colab_type": "code",
        "outputId": "4e9495b0-28d0-43a3-ba94-570a9830832e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#4/igGrpBUzgmmOTdhG77m8_Ej4eL8V8wcgV1kMDSd2zZel0dhlp_w-eek"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Ty605NYbf1",
        "colab_type": "code",
        "outputId": "1e66930d-1b84-42eb-867d-030b8ea171ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! mkdir /content/gdrive/My\\ Drive/Chatbot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Chatbot’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWVjdPuuYmSH",
        "colab_type": "code",
        "outputId": "ab587dbc-527b-4dcd-b386-189fc0f66906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! rm /content/gdrive/My\\ Drive/Chatbot/rWorldNews.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/Chatbot/rWorldNews.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0z7m3Tt57tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_handle = open('/content/gdrive/My Drive/Chatbot/rWorldNews.txt', 'a+')\n",
        "\n",
        "#Expecting array of JSON children\n",
        "def recursePostComments(context, commentsJSON):\n",
        "   \n",
        "    size = len(commentsJSON)\n",
        "    for i in range(0, size):\n",
        "        #print(commentsJSON[i])\n",
        "        if('data' in commentsJSON[i].keys()):\n",
        "            if('body' in commentsJSON[i]['data'].keys()):\n",
        "                reply = commentsJSON[i]['data']['body'];\n",
        "                if(\"[deleted]\" in reply or \"[removed]\" in reply):\n",
        "                    continue;\n",
        "                else:\n",
        "                    reply = reply.replace('\\n', ' ');\n",
        "                    reply = reply.replace('\\t', ' ');\n",
        "                    reply = reply.replace('\\'', '');\n",
        "                    reply = reply.replace('\\\"', '');\n",
        "                    reply = reply.replace('&gt', '');\n",
        "                    reply = reply.replace('&lt', '');\n",
        "                    #print(\"Context : \", context);\n",
        "                    \n",
        "                    #if (len(reply) > 500):\n",
        "                        #print(\"\\tLen :\", len(reply));\n",
        "                    #    t_reply = summarizeText(reply);\n",
        "                    #    if (len(t_reply) > 0):\n",
        "                    #        reply = t_reply;\n",
        "                    #print(\"\\tReply :\", reply);\n",
        "                    \n",
        "                    context.replace('\\t', ' ');\n",
        "                    \n",
        "                    reply.replace('\\t', ' ');\n",
        "                    reply = \" \".join(reply.split());\n",
        "                    \n",
        "                    if(\"Article has nothing\" in context):\n",
        "                        print(\"NIqqa\")\n",
        "                    \n",
        "                    if(reply.isspace() or context.isspace()):\n",
        "                        print(\"Error : \", reply, \", \", context)\n",
        "                        continue;                    \n",
        "                    else:\n",
        "                        line_to_write = \"{}\\t{}\\n\".format(reply.strip(), context.strip());\n",
        "                        file_handle.write(line_to_write)\n",
        "                    \n",
        "                    if('replies' in commentsJSON[i]['data'].keys()):\n",
        "                        if(commentsJSON[i]['data']['replies']):\n",
        "                            if('data' in commentsJSON[i]['data']['replies'].keys()):\n",
        "                                if(commentsJSON[i]['data']['replies']['data']):\n",
        "                                    if('children' in commentsJSON[i]['data']['replies']['data'].keys()):\n",
        "                                        recursePostComments(reply, commentsJSON[i]['data']['replies']['data']['children']);\n",
        "            else:\n",
        "                continue;\n",
        "        else:\n",
        "            continue;\n",
        "    return;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv0XIxKc7wdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    r_comments = requests.get('https://www.reddit.com/r/worldnews/comments/' + key +'.json?limit=10000&sort=confidence&depth=4', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "    r_comments_json = r_comments.json();\n",
        "    context = value;\n",
        "    commentsJSON = r_comments_json[1][\"data\"][\"children\"]\n",
        "    recursePostComments(context, commentsJSON)\n",
        "    \n",
        "file_handle.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBYwzxcD4Q5J",
        "colab_type": "text"
      },
      "source": [
        "#Overfitting and Testing Generative model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2rYrmca4QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchtext\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1313H7z8i2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "dataset = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rWorldNews.txt\", # name of the file\n",
        "                                        \"tsv\",               # fields are separated by a tab\n",
        "                                        fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqtnkW5vq0p1",
        "colab_type": "code",
        "outputId": "a82da960-36e6-4a45-b0b2-e31c396fb1f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6HfmfsMrRFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = torchtext.data.Dataset(dataset, fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkEjPWJ9wM4",
        "colab_type": "code",
        "outputId": "b7602c55-9bf8-44d0-c5d1-d101e18b3ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AVuOzo97fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey-KokGc9-zC",
        "colab_type": "code",
        "outputId": "3b80864d-fa03-46ee-9ee5-131fcbf64d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(text_field.vocab.itos)\n",
        "print(label_field.vocab.itos)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'p', 'g', 'y', 'w', 'f', 'b', '.', 'v', 'k', ',', 'I', '-', '/', 'T', 'S', 'A', '0', '1', 'C', 'x', '*', ':', '2', 'j', 'P', 'M', 'W', 'R', 'E', 'N', '?', 'B', 'H', 'U', ')', 'D', '(', 'O', 'F', '’', 'z', ';', '5', '3', 'G', '_', '8', '9', '6', 'L', '4', '7', 'K', 'q', ']', '[', 'Y', 'J', '!', 'V', '%', '“', '”', '=', '$', '&', '#', '~', '^', '|', 'Q', 'Z', '‘', '—', 'X', '+', '\\\\', '–', '@', 'é', '£', '…', 'о', 'ü', 'е', '€', '•', '⠀', 'ó', 'и', 'н', '️', 'а', 'т', 'á', 'ñ', 'с', '°', '・', 'ä', 'л', 'р', 'ا', '§', '\\u200b', 'м', 'ы', 'в', '`', 'к', 'ن', 'д', '´', '✔', '⣿', 'п', 'í', 'ی', 'ö', '，', 'я', '😂', 'б', 'у', '。', '🤔', '\\u200d', '💦', 'ر', '¥', 'م', 'ه', 'の', 'が', 'ч', 'º', '🇧', 'و', '¯', '·', '■', 'Ä', '‚', 'ل', '¶', 'た', 'な', '🇬', 'د', 'ت', '👍', 'г', '不', 'か', 'て', 'ب', 'っ', '人', '★', '👏', '、', 'い', '☆', 'ğ', 'Ü', 'ь', '™', '一', 'з', '💘', '₂', '交', '全', '！', '🇺', 'й', '⠛', '⌐', 'ツ', '🇸', '🏻', '🔥', '🙄', 'è', '😭', 'É', 'ع', 'や', '等', 'х', 'å', 'щ', 'Á', 'ж', 'ら', '👌', '中', 'れ', 'س', '»', 'ú', 'ā', 'ک', '₁', '₃', '世', '平', '波', '界', '者', '送', '\\ufeff', '«', '„', 'ಠ', 'ま', 'Ι', 'ш', 'э', 'に', 'り', '🇷', 'ç', 'ق', '⣀', '是', '\\uf8ff', '😎', '😢', '✌', 'も', '性', '🍻', '👋', '\\xad', 'ю', '화', '\\x03', 'ذ', '⣤', '🇫', '🇮', '💩', '🤷', '的', '🇭', '🇹', '🏳', '🌈', '🎶', 'گ', 'ა', '✓', '❤', 'と', '🇦', '®', 'ộ', '♂', 'う', 'し', 'で', '影', '心', '抗', '拒', '理', '能', '😒', 'đ', '̿', '⃣', 'ス', '💕', '反', '♪', '✅', '⠶', '監', '罪', '護', '˚', '̯', 'Α', 'ش', '「', '」', '为', '😁', 'Μ', 'Ο', 'Ρ', 'Σ', 'Е', '\\u202a', '☑', '🏼', '💥', '😟', 'ë', '͡', '≠', '動', '보', '🎵', '{', '}', '˝', '،', '∼', '⠉', '⢀', '⣶', '일', '😱', '×', 'ə', 'ˈ', 'ा', '上', '但', '民', '👑', '😍', '😬', '◕', 'つ', 'ー', '主', '义', '互', '仍', '冲', '区', '又', '和', '地', '太', '威', '安', '层', '怖', '恐', '意', '我', '未', '点', '热', '相', '真', '突', '织', '络', '网', '胁', '蒙', '起', '问', '阴', '题', '🇪', '😉', '😞', 'Ô', '̊', '\\u200e', '☺', 'き', 'る', '生', 'à', '̘', '̳', '͕', 'ц', 'خ', '‽', '₇', '∏', 'く', 'け', 'そ', 'わ', 'タ', '女', '窓', '🇩', '🏽', '😊', '¡', '¢', 'İ', '̤', '͔', '͜', 'А', 'ف', 'ế', '台', 'ː', '̙', '̹', 'В', 'ي', '▀', '⠁', '日', '本', '面', '🇰', '🇱', '🇲', '🇽', 'Đ', 'ı', '̖', '͐', 'ͫ', 'Δ', 'Н', '\\u2060', '⁶', '☝', '⠃', '⠋', '⣄', '放', '釋', '근', '문', '박', '법', '어', '이', '혜', '💡', '😫', '¤', 'ð', 'ơ', '̪', '̮', '̰', '͈', '͒', 'ͤ', 'ͨ', 'У', 'ح', 'ả', 'ễ', 'ệ', 'ị', 'ọ', 'ử', '下', '們', '傷', '回', '大', '學', '撤', '暴', '林', '鄭', '🙃', 'ß', '̇', '̐', '̲', '́', '͍', 'Ν', 'ط', 'پ', 'ვ', 'რ', '\\u200c', '\\u200f', '⠿', 'こ', '😤', '😮', '²', 'ʖ', '̀', '̈', '̒', '̔', '̝', '̠', '̻', '̾', '͇', '͓', 'Ε', 'Η', 'Κ', 'Π', 'Υ', 'Φ', 'Д', 'Л', 'О', 'ლ', 'ო', '†', '↓', '♀', '♩', '⡀', '⡟', '⣷', '企', '可', '欺', '死', '港', '為', '球', '硬', '署', '聯', '香', '스', '🌳', '🍆', '😋', '🤣', 'Ö', '̆', '̗', '̜', '̫', '̬', '̱', '̶', '̈́', '͖', 'И', 'र', 'ह', 'さ', 'よ', 'ん', 'プ', '은', '）', '😕', '🙂', '🤗', '🤦', 'È', 'ê', 'î', 'ò', '̄', '̅', '̌', '̑', '̞', '̟', '̭', '̼', '̽', 'ͅ', '͙', '͛', 'Р', 'ز', 'ई', 'त', 'द', 'म', 'े', '༼', '༽', 'ი', '⢠', 'は', 'を', 'レ', 'ン', '然', '腦', '話', '開', '길', '다', '둠', '빛', '수', '없', '을', '👀', '💯', '🖐', '🖑', '😔', '😘', '😡', '😣', '😳', '🙏', 'ï', 'ē', '́', '̍', '̚', '̣', '̦', '̧', '͗', '͞', 'ͪ', 'ͭ', 'ф', 'ج', 'ك', '―', '⚠', '⣾', '『', '』', 'え', 'お', 'び', 'め', 'イ', 'ク', 'ケ', 'コ', 'サ', 'テ', 'パ', 'ヘ', 'リ', '付', '仲', '体', '何', '助', '取', '口', '度', '建', '急', '救', '残', '求', '泣', '焼', '煤', '物', '男', '繰', '聞', '見', '赤', '身', '車', '返', '顔', '飛', '騒', '黒', '（', '🍋', '🎤', '😃', '🦄', '̏', '̵', '̷', '̺', '͋', '͎', '͚', '͟', 'ͧ', 'ͩ', 'Т', 'ъ', 'ة', '▤', '⡄', '⡿', '⢿', '⣼', 'だ', 'ぬ', 'ろ', '思', '良', '違', '？', '🇿', '🏿', '🐬', '💜', '💨', '😅', '🙌', '¿', 'ė', 'ō', '̎', '̡', '̥', '͂', '͉', '͊', 'ͣ', 'ͥ', 'ͬ', 'ͯ', 'П', '℅', '⠇', '⠈', '⠘', '⠙', '⢰', '【', '】', '間', '국', '아', '통', '한', '🌎', '👐', '😑', '©', '½', 'Ĺ', '̂', '̃', '̓', '̕', '̛', '̨', '̩', '̓', '͏', '͑', 'ͮ', '҉', 'א', 'ו', 'י', 'أ', 'ئ', 'ص', 'چ', 'ე', 'მ', '₹', '☁', '☹', '♫', '♬', '⠦', '⠻', '⢤', '⢹', '⣇', '⣠', '家', '戏', '權', '游', '玩', '線', '陣', '🇨', '🎉', '👎', '📣', '🕴', '😀', '😰', '🙊', '🚨', '\\U0001f92d', '±', 'ô', 'õ', '̉', '̋', '̢', '͆', '͌', '͠', '͢', 'Г', 'З', 'С', 'ە', 'ს', '\\u202c', '✋', '⠟', '⬇', '兴', '到', '国', '奋', '感', '草', '頭', '녕', '안', '재', '🅱', '🐎', '👩', '💖', '💗', '💟', '📞', '😏', '😓', '😶', '🙈', '🙉', '\\U0001f92e', '\\x8f', 'â', 'ø', 'ş', '̀', '͝', 'М', 'Ш', 'Ы', 'Ь', 'آ', 'إ', 'ظ', 'ง', 'ว', 'გ', 'თ', 'ნ', 'უ', 'ქ', 'ც', 'ძ', 'ხ', '⊙', '▐', '❄', '⠊', '⠢', '⠳', '⠴', '⠸', '⡶', '⡼', '⡾', '⢸', '⢻', '事', '労', '北', '南', '朝', '統', '過', '韓', '領', '鮮', '교', '대', '랑', '리', '세', '에', '여', '영', '\\uf06f', '🌸', '🏛', '🏴', '👊', '👨', '👭', '👻', '💀', '💁', '💓', '📅', '📲', '🗣', '🗳', '😄', '😆', '😌', '😐', '😦', '😩', '😯', '😲', '🙅', '🙍', '🥔', '\\U0001f9b9', '\\U000e0062', '\\U000e0063', '\\U000e0067', '\\U000e0073', '\\U000e0074', '\\U000e007f', 'ĺ', 'ń', '̴', '̸', '͘', 'ͦ', 'К', 'ד', 'ה', 'ט', 'ל', 'ף', 'ר', 'ש', 'ת', 'ؤ', 'غ', 'ِ', '۰', '۵', 'ᐛ', 'ᕕ', 'ᕗ', '‐', '✨', '❌', '⣴', 'す', '八', '冇', '卦', '消', '灣', '無', '耗', '量', '거', '싸', '오', '요', '의', '🌊', '🌱', '🍷', '🏅', '🏾', '💛', '💪', '💰', '💵', '😜', '😨', '🤚', '🤢', '\\U0001f92a', '\\U0001f92b', '🥇', '\\U0001f970', 'µ', 'Å', 'Ç', 'Í', 'Î', 'ã', 'Ğ', 'ɡ', 'ˌ', 'Ф', 'Э', 'ё', 'स', '‿', '⁄', '≥', '▃', '▥', '▦', '►', '☭', '⚫', '✊', '✿', '⠄', '⠏', '⠓', '⠚', '⠞', '⠫', '⠹', '⠾', '⡆', '⡇', '⡏', '⡜', '⡷', '⢣', '⢧', '⣆', '⣕', '⣘', '⣦', '⣧', '⣹', '⣻', '《', '》', 'へ', 'ア', '仕', '件', '内', '出', '務', '史', '同', '哈', '國', '子', '將', '志', '所', '文', '方', '椿', '歷', '版', '狐', '甲', '目', '示', '科', '自', '行', '表', '要', '軍', '냅', '뜨', '라', '맙', '바', '사', '소', '전', '\\uf0b7', '：', '🍔', '🐂', '🐢', '🐻', '🐼', '👉', '💿', '🖕', '😼', '🤐', '🤞', '\\U0001f928', '³', 'æ', 'ì', 'ù', 'þ', 'Ā', 'ū', 'ż', 'ɐ', 'ɒ', 'ʇ', 'ʍ', 'ʎ', 'Β', 'Б', 'Х', 'ء', 'ض', 'ى', 'ं', 'ड', 'ी', 'দ', 'ধ', 'ন', 'ব', 'য', 'া', '্', 'ბ', 'კ', 'ტ', 'ყ', 'ẵ', 'Ứ', 'ỹ', '∀', '≈', '▽', '⛽', '⠗', '⠰', '⠷', '⡁', '⡛', '⢶', '⣥', '⣰', '〜', 'あ', 'ご', 'ざ', 'ず', 'ね', 'ゃ', 'ガ', 'ト', 'ニ', 'ノ', 'ベ', 'メ', 'ラ', 'ル', 'ヾ', '三', '串', '了', '二', '些', '会', '伯', '佳', '來', '候', '側', '分', '加', '变', '吧', '員', '四', '因', '坦', '報', '外', '官', '少', '尔', '屋', '従', '态', '悲', '憲', '扉', '断', '斯', '時', '暫', '會', '有', '木', '果', '椎', '業', '樂', '止', '汉', '油', '洋', '演', '火', '爱', '特', '玄', '用', '看', '社', '禁', '笑', '結', '緩', '艾', '認', '讚', '賞', '転', '載', '還', '部', '酸', '里', '鍵', '閉', '関', '阿', '雖', '가', '기', '년', '말', '무', '식', '신', '었', '육', '있', '쟁', '진', '짜', '할', '해', '헐', '현', '희', '｀', 'ﾉ', '￼', '🇳', '🌙', '🍑', '🍶', '🍼', '🍾', '🎊', '🐅', '🐉', '🐣', '👆', '💙', '💚', '💲', '📉', '📷', '🕸', '😈', '😾', '🚀', '🤓', '🤤', '\\U0001f92f', '🥝', '\\U0001f967', '\\U0001f9d0', '\\U0001f9d8', '\\U0001f9e0', '\\U0001f9e1']\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'y', 'g', 'p', 'w', 'f', '.', 'b', 'v', 'k', ',', 'I', 'T', '/', '-', 'A', 'S', '0', 'x', 'j', 'C', '?', 'W', '1', 'N', '*', 'E', 'H', 'M', 'P', 'R', 'B', '2', ':', 'U', 'O', 'D', '’', ')', '(', 'z', 'F', ';', 'Y', 'G', 'L', '!', 'q', '5', '3', 'K', '_', '9', '4', '8', 'J', '6', '7', ']', '[', 'V', '%', '“', '”', '=', '&', '^', '~', '$', '#', 'Q', 'Z', 'X', '\\\\', '+', '|', '—', '‘', '–', 'é', '£', '@', '⣿', '⠀', 'о', '…', '️', '😂', 'е', 'а', 'ا', 'ü', '°', 'и', 'т', 'н', '•', '€', 'с', '🤔', '`', '👀', 'ل', 'р', '👏', 'в', '¯', 'л', 'ن', 'á', 'ä', 'ö', 'ó', 'ر', 'к', '\\u200b', 'م', '─', 'ي', 'ñ', '\\u200d', '͡', 'д', 'м', '👍', 'ت', 'י', 'у', 'ب', 'و', '👌', 'п', '´', 'я', '🏻', 'ы', 'í', '🙄', '™', 'ו', 'ه', 'ツ', 'د', '❤', '̶', 'ת', '§', 'б', 'א', '·', '🎶', 'ה', 'ك', 'ь', '͜', 'ל', 'ع', '⠛', '😎', 'ğ', 'س', '🤣', '🤷', 'ʖ', 'ی', '🇺', '😭', '„', '♂', 'й', 'з', 'ف', 'ق', 'ç', '🏼', 'è', 'מ', '¥', 'ة', 'ч', 'ר', 'ಠ', '■', '⣀', '，', '🇸', '{', '}', '«', '»', 'أ', 'г', 'х', 'ש', '🇧', '🇦', '⣤', '\\u2060', '😉', 'å', 'ב', '⣶', 'à', 'ā', '،', '✔', '。', '人', '🇬', '🌈', '🎵', '😔', 'ح', '😁', '😊', '😢', '♀', '💩', '―', '💦', 'ж', '⢀', 'い', 'が', 'ა', '🙏', 'ß', 'ï', '̘', '̯', '♪', '⠿', '大', '¡', 'º', 'ú', 'ش', '⠉', '🇨', '😍', '😤', 'ə', '͕', 'ц', '⠶', 'の', '🏽', '🔥', '²', '×', '̤', 'נ', 'خ', '⌐', 'な', '民', '̫', '̼', '★', '⠁', '🤦', 'â', 'ˈ', '̊', '̙', 'ю', '、', 'と', '\\ufeff', '👉', '😬', 'Ä', 'É', '̳', '̿', '̈́', '͔', 'э', 'כ', 'ک', '☝', '⠄', '⠋', 'た', '・', '😞', '🙃', '̝', '̪', '̱', '̲', '̹', '́', 'щ', '✌', 'て', '🎉', '🏳', '😏', '😡', 'ø', '̇', '̏', '̒', '͉', '͍', '͖', '✱', '⠃', '⡿', '⣄', '⣷', '⣾', 'し', '不', '🇪', '😩', 'ô', '̄', '̖', '̰', '͒', 'Н', 'П', '‽', '≠', 'か', '天', '安', '法', '的', '💕', '😅', '😕', '¢', '¶', 'ã', 'ш', 'ד', 'ע', 'は', '自', '🌊', '😀', '😒', 'ê', 'ı', '̐', '̠', '̮', '̾', 'ͅ', '͇', '͊', '͐', '͓', 'Р', 'С', 'ח', 'ם', 'ذ', 'ص', 'ط', '‚', '☺', 'で', 'に', '反', '🅱', '🍻', '😱', '😳', '🦀', '¿', '̀', '̅', '̆', '̈', '̌', '̓', '̚', '̣', '͈', '͙', '͚', 'А', 'Д', '✊', '⡀', '⡟', '中', '🇹', '😑', 'Ü', '́', '̍', '̑', '̔', '̜', '̦', '̩', '̻', 'ͫ', 'ج', '✅', '⢿', 'っ', '一', '制', '🇩', '🇲', '🇷', '🏿', '👋', '😮', '®', 'ɐ', '̕', '̗', '̥', '̧', '̬', '̭', '̺', '͎', '͗', '͝', 'ͤ', 'ͨ', 'И', 'О', 'ф', 'ק', 'إ', '∼', '☆', '☹', '✋', '⠈', 'う', 'り', '是', '！', '💀', '💯', '💰', '😟', '🙌', '˚', '̂', '̟', '͛', 'Е', 'ג', 'ז', '☑', '✓', '⢠', 'つ', 'ら', 'れ', '平', '我', '門', '🇽', '🎺', '👐', '💜', 'Á', 'ɪ', '̉', '̽', '͠', 'В', 'Я', 'ט', 'ى', 'რ', '◕', '♫', '⠟', '⣼', 'る', 'ー', '志', '灣', '輪', '门', '🇮', '💘', '😫', '\\x03', 'ð', '̃', '̎', '̞', '͂', '͆', '͋', '͌', 'Б', 'Т', 'ך', 'ן', 'פ', 'ा', 'ი', '⠻', '⡄', '⣠', 'く', 'す', '😥', '🤙', 'î', '̛', '̡', '̨', '̵', '̸', '̓', '͑', 'گ', '\\u200f', '⁄', '♥', '❌', '⢰', 'さ', 'ま', '台', '李', '波', '真', '국', '은', '이', '💪', '😃', '😐', '🥚', 'æ', 'ː', '̷', '̀', '͘', '͟', 'ͪ', 'ͭ', 'ס', 'غ', 'ლ', 'მ', '⠙', '⣴', 'こ', 'だ', 'や', 'イ', '亂', '功', '和', '多', '抗', '洪', '生', '由', '일', '🇫', '🇰', '🇱', '🍆', '🏾', '👻', '📣', '🖕', '😋', '😘', '🙂', '🤗', '\\U0001f92e', '½', 'Ö', 'ë', 'ē', 'ī', 'İ', 'ʲ', '̋', '̢', '̴', '͞', 'ͧ', 'ͩ', 'К', 'У', 'Ф', 'љ', 'ض', 'ვ', '\\u200e', '‐', '−', '▤', '⠇', '⠘', '⢸', '⣇', 'を', 'サ', 'ス', 'ル', '事', '動', '時', '暴', '權', '論', '어', 'ﾟ', '🇭', '🎂', '👈', '💛', '😄', '😹', '\\x80', '\\xad', 'ō', 'ɥ', 'ͣ', 'ͥ', 'ͬ', 'ͯ', 'צ', 'آ', 'ئ', 'پ', 'ე', '₂', '⃣', '⣉', 'あ', 'ん', 'バ', 'ミ', '了', '京', '共', '化', '命', '國', '在', '子', '強', '心', '态', '性', '有', '死', '殺', '特', '維', '网', '言', '過', '보', '화', '🇼', '🇿', '🌎', '💅', '💙', '💚', '😓', '🙁', '🙊', '🚨', '🤑', '🤚', '🤤', '🦆', '±', 'õ', 'ù', 'œ', 'ş', 'ˣ', '͏', 'ͮ', '҉', 'ء', '༼', '༽', 'ნ', 'ო', 'ᐛ', 'ᵐ', 'ộ', '\\u202c', '₁', '₇', '₹', '℅', '↓', '☞', '⠦', '⢤', '⢹', '「', '」', 'も', 'よ', 'ク', 'ン', '主', '件', '全', '六', '劉', '动', '北', '合', '同', '報', '壓', '屠', '思', '擾', '政', '文', '春', '為', '爾', '獨', '発', '私', '等', '見', '賣', '進', '運', '還', '革', '騷', '黨', '민', '\\uf8ff', '￼', '👑', '💁', '💓', '😆', '🙅', '🙈', '🙉', '\\x9d', '©', 'đ', '͢', 'Δ', 'Г', 'Л', 'ث', 'ਾ', 'ง', 'ᕕ', 'ᕗ', '†', '⁶', '₃', '≥', '☠', '☻', '⠴', '⢁', '⢻', 'け', 'タ', 'ナ', '下', '但', '害', '意', '日', '會', '物', '理', '示', '統', '行', '議', '送', '通', '面', '스', '통', '한', '？', 'ﾉ', '🌳', '🍑', '🎄', '🎊', '🎼', '🏴', '🐝', '👎', '👨', '💖', '💨', '📷', '🔔', '😌', '😧', '😰', '😸', '🤡', '🤢', '🦏', '\\U000e0062', '\\U000e0063', '\\U000e0067', '\\U000e0073', '\\U000e0074', '\\U000e007f', '¤', 'µ', 'Ç', 'Í', 'Ô', 'ò', 'ė', 'ū', 'ǝ', 'ʇ', '˝', 'Ι', 'Х', 'Ш', 'ё', 'ز', 'ظ', 'र', 'स', 'ह', 'े', 'ਕ', 'ย', 'ს', 'უ', '≈', '┻', '╯', '▀', '█', '▐', '♩', '⚕', '⚾', '❄', '⠊', '⠗', '⠢', '⠳', '⠸', '⡶', '⡼', '⡾', '⣆', '⣦', '⣰', '⭐', '》', '【', '】', 'お', 'わ', 'プ', 'ヮ', '上', '伯', '出', '労', '博', '口', '問', '四', '地', '威', '官', '射', '族', '本', '港', '游', '演', '然', '用', '監', '看', '知', '能', '腦', '視', '評', '認', '護', '變', '走', '道', '難', '食', '高', '다', '대', '문', '수', '에', '요', '주', '\\uf06f', '）', '＝', 'ｅ', 'ｈ', '🌲', '🍺', '🐉', '🐱', '👩', '🔫', '😈', '😖', '😜', '😨', '😯', '🤖', '\\U0001f928', '\\U0001f9e1', '³', '¹', 'Ó', 'Ā', 'Đ', 'ę', 'ơ', 'ɑ', 'ɹ', 'ʎ', 'ˌ', 'ͦ', 'Α', 'Σ', 'ε', 'τ', 'З', 'М', 'Ь', 'Ю', 'ף', 'ؤ', 'چ', 'ि', 'ਦ', 'ਰ', 'ੇ', 'ੋ', 'ว', 'เ', 'თ', 'ხ', 'ả', 'ế', 'ị', 'ỹ', '\\u202a', '\\u202d', '∀', '∏', '►', '☕', '☭', '☮', '⚰', '✨', '⠏', '⠚', '⠹', '⡇', '⡏', '⢶', '⬆', '《', 'き', 'ち', 'ね', 'み', 'ア', 'グ', 'コ', 'ッ', 'パ', 'ラ', 'リ', 'レ', '丹', '么', '之', '九', '也', '些', '交', '什', '他', '代', '侵', '們', '候', '元', '八', '加', '區', '卦', '古', '可', '右', '吾', '品', '唐', '啊', '喇', '喜', '嘛', '器', '国', '土', '堕', '壞', '士', '奪', '女', '家', '實', '專', '對', '就', '度', '建', '弟', '彩', '很', '後', '想', '拐', '拒', '拷', '掠', '摘', '新', '斷', '方', '於', '暁', '曉', '朝', '样', '毒', '油', '治', '活', '派', '流', '消', '淨', '淫', '清', '激', '無', '爭', '獎', '王', '産', '略', '畫', '疆', '盧', '破', '種', '立', '策', '紀', '紫', '耀', '者', '肅', '胎', '胡', '臺', '華', '藏', '裁', '西', '要', '話', '誘', '說', '諾', '貝', '買', '賭', '賴', '趙', '躍', '身', '这', '迫', '遊', '達', '那', '邦', '都', '鄉', '鎮', '開', '間', '防', '陽', '題', '香', '驗', '體', '鬥', '魏', '鮮', '길', '김', '둠', '아', '오', '의', '있', '재', '할', '：', 'ｉ', '𝔞', '𝔢', '𝔥', '🇯', '🇳', '🇵', '🌸', '🌿', '🍄', '🍋', '🍿', '🎤', '🐦', '🐸', '👆', '👊', '📍', '🖐', '🗣', '😣', '😲', '😶', '🤓', '🤘', '🤛', '🤜', '🤞', '\\U0001f92a', '🥇', '🥑', '\\U0001f9d0', '\\U0001f9e0', '\\x8f', '\\x9c', '¨', 'Å', 'È', 'Ø', 'Ú', '÷', 'þ', 'ń', 'Ş', 'ż', 'ư', 'ț', 'ɒ', 'ɔ', 'ɴ', 'ʍ', 'ʔ', 'ʕ', 'ʰ', '˘', 'Γ', 'Μ', 'Ν', 'Ο', 'Ρ', 'θ', 'ι', 'ο', 'φ', 'ω', 'Э', 'ъ', '؟', 'ِ', '٠', '۰', '۵', 'ई', 'ड', 'त', 'द', 'म', 'य', 'ल', 'ਈ', 'ਗ', 'ਮ', 'ਵ', 'ਿ', 'ੀ', 'ಥ', 'ท', 'ม', 'อ', 'ี', 'ბ', 'გ', 'ქ', 'ყ', 'ჩ', 'ც', 'ძ', 'ᶠ', 'ễ', 'ệ', 'ọ', 'ớ', 'ử', '\\u200c', '″', '‿', '℃', '⊙', '⌒', '━', '▔', '□', '▥', '▦', '▽', '◄', '☁', '☉', '☼', '♡', '♬', '⚆', '⚡', '⚫', '⛽', '✧', '❗', '⠓', '⠞', '⠫', '⠰', '⠷', '⠾', '⡁', '⡆', '⡛', '⡜', '⡷', '⢣', '⢧', '⣕', '⣘', '⣥', '⣧', '⣹', '⣻', 'え', 'ざ', 'ず', 'せ', 'そ', 'び', 'べ', 'ゃ', 'オ', 'ギ', 'ジ', 'テ', 'ト', 'ニ', 'ピ', 'フ', 'マ', 'メ', 'ュ', 'ョ', 'ㄏ', 'ㄒ', 'ㅅ', '丈', '世', '为', '以', '仲', '价', '企', '会', '体', '你', '使', '來', '個', '傷', '内', '写', '分', '別', '到', '匚', '南', '去', '发', '取', '吧', '回', '場', '外', '太', '如', '定', '对', '將', '小', '少', '山', '帝', '影', '怪', '总', '恭', '情', '愛', '戦', '房', '所', '打', '扰', '接', '放', '数', '斯', '明', '果', '様', '模', '樣', '欺', '止', '正', '比', '水', '汉', '火', '爱', '版', '犬', '現', '球', '甲', '男', '界', '皇', '硬', '確', '科', '突', '窓', '絡', '罪', '署', '而', '聯', '聽', '草', '表', '被', '觀', '謝', '警', '讚', '让', '语', '起', '越', '這', '連', '部', '釋', '量', '関', '障', '雖', '韓', '頂', '領', '頭', '點', '거', '교', '권', '기', '나', '떡', '라', '로', '리', '마', '만', '바', '빛', '세', '식', '없', '영', '을', '전', '좋', '\\uf0b7', '︵', '（', 'ｎ', 'ｏ', 'ｔ', '･', '𝔩', '𝔫', '𝔰', '𝔱', '𝘼', '🀄', '🌍', '🌏', '🌟', '🌱', '🌴', '🍀', '🍁', '🍅', '🍊', '🍌', '🍾', '🎩', '🎯', '🏅', '🏮', '🐄', '🐕', '🐖', '🐢', '🐣', '🐷', '👦', '👭', '👴', '💉', '💊', '💥', '💲', '💵', '📅', '📲', '🔪', '🕴', '🖑', '😛', '😠', '😦', '😵', '🙍', '🤐', '\\U0001f92b', '\\U0001f92c', '\\U0001f92d', '🥁', '🥔', '🦁', '\\x14', 'À', 'Â', 'Æ', 'Î', 'Ï', 'Ù', 'ì', 'û', 'ă', 'č', 'Ğ', 'Ĺ', 'ĺ', 'ś', 'ǎ', 'ǐ', 'ɡ', 'ɢ', 'ɨ', 'ɶ', 'ʂ', 'ʊ', 'ʏ', 'ʐ', 'ʒ', 'ʞ', 'ʟ', 'ˀ', '˙', 'ˢ', 'Β', 'Ε', 'Η', 'Κ', 'Π', 'Υ', 'Φ', 'α', 'γ', 'λ', 'ρ', 'ς', 'σ', 'Ї', 'Ж', 'Ц', 'Ч', 'Ы', 'і', '٣', 'ڡ', 'ە', 'ं', 'क', 'छ', 'ज', 'ट', 'ब', 'ी', 'ु', 'ो', 'দ', 'ধ', 'ন', 'ব', 'য', 'া', '্', 'ਂ', 'ਉ', 'ਚ', 'ਜ', 'ਝ', 'ਡ', 'ਣ', 'ਤ', 'ਬ', 'ਲ', 'ਹ', 'ੂ', 'ੈ', 'ੜ', 'ੰ', 'ข', 'ค', 'ณ', 'น', 'บ', 'ป', 'ร', 'ล', 'ศ', 'ห', 'ะ', 'ุ', 'ไ', '้', 'დ', 'კ', 'პ', 'ტ', 'ᴀ', 'ᴉ', 'ᴏ', 'ᴘ', 'ᴡ', 'ᴥ', 'ᴰ', 'ᴴ', 'ᵃ', 'ᵈ', 'ᵘ', 'ᶜ', 'ḥ', 'ḫ', 'ṣ', 'ẵ', 'ặ', 'Ứ', '′', 'ⁿ', 'ℑ', '⅔', '⅞', '↑', '→', '↼', '∇', '√', '∞', '≤', '⊏', '⊐', '⊱', '⋯', '▃', '▾', '○', '◞', '◟', '☯', '⚖', '⚙', '⚠', '⚪', '✍', '✝', '✿', '❐', '➕', '⠔', '⠠', '⠺', '⡉', '⡴', '⢉', '⣈', '⣡', '⬇', '⬜', '『', '』', '〜', 'げ', 'ご', 'ど', 'ぬ', 'へ', 'め', 'ょ', 'ろ', 'ィ', 'ォ', 'カ', 'ガ', 'ケ', 'デ', 'ネ', 'ノ', 'ヘ', 'ベ', 'モ', 'ヾ', 'ㄟ', '三', '专', '且', '並', '丨', '个', '串', '乁', '乂', '乇', '义', '乎', '习', '书', '二', '互', '亞', '亡', '今', '仍', '仕', '付', '们', '住', '何', '佳', '俄', '保', '值', '假', '偏', '做', '停', '側', '偷', '傲', '像', '價', '儒', '先', '入', '公', '兴', '兽', '冇', '冲', '剛', '割', '助', '務', '区', '医', '卂', '卄', '华', '厉', '原', '參', '又', '及', '受', '变', '只', '史', '号', '吉', '向', '君', '呀', '哈', '員', '哦', '唱', '嗎', '因', '圈', '坑', '坦', '垒', '城', '域', '基', '壁', '夫', '头', '奋', '好', '妖', '存', '學', '宗', '客', '寄', '富', '察', '審', '导', '専', '尔', '尚', '尺', '尼', '层', '屋', '層', '差', '己', '希', '席', '帶', '年', '府', '式', '張', '当', '往', '待', '律', '従', '得', '必', '忘', '怎', '怕', '怖', '急', '恋', '恐', '您', '悲', '感', '態', '慢', '慣', '憲', '應', '戏', '成', '扉', '拋', '持', '指', '摧', '撤', '操', '救', '數', '断', '旗', '昌', '普', '暫', '望', '木', '未', '来', '林', '桿', '椎', '椿', '業', '槍', '樂', '歡', '歧', '歷', '残', '毁', '毎', '毕', '气', '求', '決', '泣', '洋', '津', '洲', '济', '混', '渡', '点', '烦', '热', '焚', '焼', '煤', '熊', '片', '状', '狐', '獅', '獲', '玄', '玩', '现', '玻', '璃', '田', '画', '發', '白', '百', '盟', '目', '直', '相', '礼', '社', '祖', '神', '禁', '程', '穴', '空', '竟', '笑', '第', '答', '管', '精', '糕', '糟', '糧', '細', '結', '線', '緩', '繰', '织', '经', '络', '绝', '继', '续', '维', '罗', '罢', '義', '習', '翻', '耗', '聞', '肺', '肿', '胁', '脑', '與', '般', '船', '良', '艾', '英', '萬', '著', '蒙', '蓆', '蘇', '衛', '衝', '裡', '角', '計', '討', '証', '詳', '説', '講', '譜', '访', '貞', '賞', '贵', '赤', '超', '車', '軍', '転', '載', '迎', '近', '返', '远', '连', '速', '違', '遠', '避', '鄭', '酒', '酵', '酸', '里', '重', '金', '錯', '鍵', '鐘', '长', '閉', '问', '阂', '阴', '阿', '陣', '隊', '隔', '離', '震', '韩', '頑', '顔', '须', '题', '風', '飛', '騒', '驕', '麼', '黒', '龍', '가', '갱', '걸', '게', '고', '공', '과', '굿', '근', '꺳', '냅', '녁', '년', '녕', '당', '더', '든', '뜨', '랑', '력', '막', '말', '맙', '맥', '모', '목', '무', '박', '밥', '버', '법', '볶', '부', '불', '비', '빔', '빨', '사', '생', '서', '설', '소', '신', '싸', '안', '얼', '엄', '업', '었', '여', '온', '외', '육', '으', '인', '입', '잇', '쟁', '저', '존', '진', '짜', '친', '큼', '터', '트', '티', '파', '해', '허', '헐', '현', '혜', '회', '후', '희', '\\uf0e0', '１', '；', '＠', 'Ｐ', 'Ｔ', 'Ｗ', '｀', 'ａ', 'ｃ', 'ｒ', 'ﾛ', '￦', '𝐕', '𝐚', '𝐢', '𝐯', '𝔠', '𝔣', '𝔦', '𝔬', '𝔭', '𝔲', '𝔳', '𝔴', '𝔶', '𝘾', '𝙀', '𝙄', '𝙈', '𝙉', '𝙍', '𝟸', '🅿', '🇴', '🌙', '🌠', '🌤', '🌮', '🌶', '🌼', '🍇', '🍎', '🍔', '🍞', '🍩', '🍫', '🍶', '🍷', '🍼', '🎅', '🎥', '🎪', '🎸', '🏛', '🏝', '🏵', '🐀', '🐂', '🐅', '🐇', '🐎', '🐓', '🐘', '🐡', '🐧', '🐬', '🐻', '🐼', '👃', '👇', '👬', '👶', '👽', '👿', '💔', '💗', '💞', '💟', '💡', '💧', '💻', '💿', '📉', '📜', '📞', '📰', '📴', '🔊', '🔚', '🔛', '🔝', '🕵', '🕸', '🖖', '🖤', '🗞', '🗳', '😇', '😙', '😝', '😪', '😴', '😼', '😾', '🙀', '🙇', '🚀', '🚌', '🚑', '🚓', '🚔', '🚢', '🚫', '\\U0001f6f7', '🤝', '🤠', '\\U0001f929', '\\U0001f92f', '\\U0001f932', '🥕', '🥝', '\\U0001f967', '\\U0001f970', '\\U0001f974', '🦃', '🦄', '🦊', '🦍', '\\U0001f992', '\\U0001f9b9', '\\U0001f9d8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnH0XefaGkke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = torchtext.data.BucketIterator(train,\n",
        "                                           batch_size=1,\n",
        "                                           sort_key=lambda x: len(x.context), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False,\n",
        "                                          )                  # repeat the iterator for many epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZVdxcMzHD8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "91f362cd-9145-4786-e8d8-6b3d23f7907a"
      },
      "source": [
        "k = 0;\n",
        "for batch in train_iter:\n",
        "    \n",
        "    #max_length_seq = batch.context[0].shape[1];\n",
        "    #print(\"-----Batch #\", k+1, \"-----\");\n",
        "    #print(\"Maximum length of the input sequence : \", max_length_seq);\n",
        "    #padding_per_batch = int(torch.sum(max_length_seq - batch.sms[1]))\n",
        "    #print(\"Number of padding for batch : \", padding_per_batch)\n",
        "    \n",
        "    print(\"Batch Size : \", len(batch))\n",
        "    #print(\"Batch Context : \", batch.context)\n",
        "    print(\"Batch Context Shape: \", batch.context[0].shape)\n",
        "    #print(batch.reply)\n",
        "    print(\"Batch Context Shape: \", batch.reply.shape)\n",
        "    k += 1;\n",
        "    \n",
        "    if(k >= 1):\n",
        "        break;"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size :  1\n",
            "Batch Context Shape:  torch.Size([1, 113])\n",
            "Batch Context Shape:  torch.Size([1, 118])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xQ-kcu4YD6R",
        "colab_type": "code",
        "outputId": "47533d41-ed0e-49ea-c562-addd4ebc68f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "string_array =[];\n",
        "for i in range(0, batch.reply.shape[1]):\n",
        "    string_array.append(label_field.vocab.itos[(batch.reply[0][i])])\n",
        "''.join(string_array)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<BOS>I just don\\'t get it. How do people think Trump is looking to help the little guy, aka us citizens? He wants to run the country like a business. He is basically only looking out for his wallet and his own ass.  He has praised dictators and put Putin on a pedestal above all else.  He has unethically benefited from a business standpoint by having the government spend millions at his hotels.   He has refused to demonstrate sympathy when a tragic shooting occurs.  He has mocked gold star families.  He goes on tirades on Twitter.  He fabricates \"facts\" (like the ones related to the Queen) to make himself seem better.  He had a scandal with a porn star.  He has literally been caught saying grab em by the pussy (and said some incestuous stuff about his daughter).  He constantly attacks the Constitution of Free Speech by calling various journalism \"Fake News\".   He has ruined our relationships with previous allies.  He tried to start a trade war.  His tax cuts only helped large corporations and he defended the cuts by saying the employees would benefit when in reality the tax cuts stimulated buybacks of stock.  For the love of God I don\\'t understand how people don\\'t understand that he is not on par with all of the proper presidents we have had in the past. Trump is a stain on the legacy of presidents we have. If he has been capable of all this why do people think it\\'s not possible that he is colluding with Russia or at the very least that he is obstructing Mueller from his investigation? <EOS>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCv9loPeVGKZ",
        "colab_type": "code",
        "outputId": "b7756c5d-e45b-4765-910c-49c4a49117af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Vocab Size:  1275\n",
            "Reply Vocab size:  2232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hldpm9WvUvGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(ChatBot, self).__init__()\n",
        "\n",
        "        # identiy matrix for generating one-hot vectors\n",
        "        self.ident = torch.eye(vocab_size)\n",
        "\n",
        "        # recurrent neural network\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
        "\n",
        "        # a fully-connect layer that outputs a distribution over\n",
        "        # the next token, given the RNN output\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, inp, hidden=None):\n",
        "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
        "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)          # predict distribution over next tokens\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijGYB_EU5vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ChatBot(vocab_size, 1024)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YerCCuQbXDWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJfvcGdIXOUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bos_input = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long().unsqueeze(0)\n",
        "output, hidden = model(bos_input, hidden=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0znc-MRrbbby",
        "colab_type": "code",
        "outputId": "55bbe8e4-7d82-4259-cce8-d4bafc1e1e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "batch.reply[0][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NDULIi_Xnka",
        "colab_type": "code",
        "outputId": "c2cb0e7e-bf7b-4c94-a7a6-6cb112c4d81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "target = torch.Tensor([batch.reply[0][1]]).long().unsqueeze(0);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[29]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUjzHM2YbtKS",
        "colab_type": "code",
        "outputId": "3c8a6cea-7090-4a7e-ad49-06264a6f0090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(output.shape)\n",
        "print(target.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 95])\n",
            "torch.Size([1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZMVfRueb06v",
        "colab_type": "code",
        "outputId": "514efb9e-965b-4b29-c2cc-4c99c2b7ee90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "criterion(output.reshape(-1, vocab_size), target.reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.5767, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYJQVwufYs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, context_vocab_size, encoder_hidden_size, reply_vocab_size, generator_hidden_size, encoder_layers = 1, generator_layers = 1):\n",
        "        super(ChatBot, self).__init__()\n",
        "\n",
        "        # identiy matrix for generating one-hot vectors\n",
        "        self.ident_context = torch.eye(context_vocab_size)\n",
        "        self.ident_reply = torch.eye(reply_vocab_size)\n",
        "\n",
        "        # Left side of the model which encodes the context. We will use the final hidden layer input to generator RNN\n",
        "        self.encoderRnn = nn.GRU(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # Generator RNN\n",
        "        self.generatorRNN = nn.GRU(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        \n",
        "\n",
        "        # a fully-connect layer that outputs a distribution over\n",
        "        # the next token, given the RNN output\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        context = self.ident_context[context]                  # generate one-hot vectors of input\n",
        "        response = self.ident_reply[response]\n",
        "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)          # predict distribution over next tokens\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3JHLgXBqEwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ident_context = torch.eye(input_vocab_size)\n",
        "ident_reply = torch.eye(reply_vocab_size)\n",
        "encoderRnn = nn.GRU(input_vocab_size, 100, 1, batch_first=True)\n",
        "generatorRNN = nn.GRU(reply_vocab_size, 100, 1, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9W_3guar_RC",
        "colab_type": "code",
        "outputId": "84b6a3fd-8372-412a-a48c-6dfb9a24bcd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch.context[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 226])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdQX0cVVsCTt",
        "colab_type": "code",
        "outputId": "7e482118-4bb8-4063-ce6a-6dd2b1fa5f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch.reply.shape #[0] is batch size [1] is sequence length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1506])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhnd2dJXrox3",
        "colab_type": "code",
        "outputId": "f80db84a-4c6e-47da-83f8-bf38158aa5b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "icontext = ident_context[batch.context[0]]\n",
        "out, hidden_encoder = encoderRnn(icontext, None)\n",
        "print(out.shape)\n",
        "print(hidden_encoder.shape)\n",
        "ireply = ident_reply[batch.reply]\n",
        "out2, hidden_generator = generatorRNN(ireply, hidden_encoder)\n",
        "print(out2.shape)\n",
        "print(hidden_generator.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 226, 100])\n",
            "torch.Size([1, 1, 100])\n",
            "torch.Size([1, 1506, 100])\n",
            "torch.Size([1, 1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-whM2pTj1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, context_vocab_size, encoder_hidden_size, reply_vocab_size, generator_hidden_size, encoder_layers = 1, generator_layers = 1):\n",
        "        super(ChatBot, self).__init__()\n",
        "\n",
        "        # identiy matrix for generating one-hot vectors\n",
        "        self.ident_context = torch.eye(context_vocab_size)\n",
        "        self.ident_reply = torch.eye(reply_vocab_size)\n",
        "\n",
        "        # Left side of the model which encodes the context. We will use the final hidden layer input to generator RNN\n",
        "        self.encoderRnn = nn.GRU(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # Generator RNN\n",
        "        self.generatorRNN = nn.GRU(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        \n",
        "\n",
        "        # a fully-connect layer that outputs a distribution over\n",
        "        # the next token, given the RNN output\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        context = self.ident_context[context]                  # generate one-hot vectors of input\n",
        "        response = self.ident_reply[response]\n",
        "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)          # predict distribution over next tokens\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}