{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "World_News_Chatbot",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/World_News_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V3xTQ7i8u-M",
        "colab_type": "text"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIo7pCYeTro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renbNwSOVLW1",
        "colab_type": "code",
        "outputId": "c4c7a9ef-fe3c-4adc-a84c-246c423ed543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0iSGmMVTaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-kUcpcOXqCS",
        "colab_type": "code",
        "outputId": "d86341eb-e0a8-4b46-fd98-66bdc24eebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "detect(\"Hello my name is Himanshi\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqI0dB2JWvr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBReJMv2mhne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def updateListingDict(postsJSON, postsDICT):\n",
        "    chunk_size = len(postsJSON[\"data\"][\"children\"])\n",
        "    posts = postsJSON[\"data\"][\"children\"]\n",
        "    \n",
        "    for i in range(chunk_size): #Loop through each reddit post\n",
        "        r_post_title = posts[i][\"data\"][\"title\"];\n",
        "        r_post_title = \" \".join(r_post_title.split());\n",
        "        r_post_title = r_post_title.replace('\\n', ' ');\n",
        "        r_post_title = r_post_title.replace('\\t', ' ');\n",
        "        r_post_title = r_post_title.replace('\\'', '');\n",
        "        r_post_title = r_post_title.replace('\\\"', '');\n",
        "\n",
        "        postsDICT[posts[i][\"data\"][\"id\"]] = r_post_title;\n",
        "\n",
        "        \n",
        "    \n",
        "\n",
        "after = '';\n",
        "listing_dict = {};\n",
        "\n",
        "for i in range(0, 10):\n",
        "    if(after):\n",
        "        print(\"[AFTER] = \", after)\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100&after='+after, \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache'\n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "    else:\n",
        "        print(\"[AFTER] = NONE\")\n",
        "        r_worldnews = requests.get('https://www.reddit.com/r/worldnews/top.json?t=all&limit=100', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "        if(r_worldnews.status_code == 200):\n",
        "            postsJSON = r_worldnews.json()\n",
        "            updateListingDict(postsJSON, listing_dict);\n",
        "            #print(\"[GET] request successful\")\n",
        "            after = postsJSON[\"data\"][\"after\"];\n",
        "            \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LKYyZ9ytpXw",
        "colab_type": "code",
        "outputId": "1f43670b-8415-4e63-9431-194665f017db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(listing_dict))\n",
        "\n",
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82\n",
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8tHFJZYYzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXRYBVro193S",
        "colab_type": "code",
        "outputId": "7ee7173e-bc40-498d-d9e5-167c87bc00ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(key,value)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "901p5f Two weeks before his inauguration, Donald J. Trump was shown highly classified intelligence indicating that President Vladimir V. Putin of Russia had personally ordered complex cyberattacks to sway the 2016 American election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDiM4y_KYaSk",
        "colab_type": "code",
        "outputId": "77a7c3d8-2733-4eae-b722-c87f1bb3bd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#4/igGrpBUzgmmOTdhG77m8_Ej4eL8V8wcgV1kMDSd2zZel0dhlp_w-eek"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Ty605NYbf1",
        "colab_type": "code",
        "outputId": "1e66930d-1b84-42eb-867d-030b8ea171ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! mkdir /content/gdrive/My\\ Drive/Chatbot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜/content/gdrive/My Drive/Chatbotâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWVjdPuuYmSH",
        "colab_type": "code",
        "outputId": "ab587dbc-527b-4dcd-b386-189fc0f66906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! rm /content/gdrive/My\\ Drive/Chatbot/rWorldNews.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/Chatbot/rWorldNews.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0z7m3Tt57tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_handle = open('/content/gdrive/My Drive/Chatbot/rWorldNews.txt', 'a+')\n",
        "\n",
        "#Expecting array of JSON children\n",
        "def recursePostComments(context, commentsJSON):\n",
        "   \n",
        "    size = len(commentsJSON)\n",
        "    for i in range(0, size):\n",
        "        #print(commentsJSON[i])\n",
        "        if('data' in commentsJSON[i].keys()):\n",
        "            if('body' in commentsJSON[i]['data'].keys()):\n",
        "                reply = commentsJSON[i]['data']['body'];\n",
        "                if(\"[deleted]\" in reply or \"[removed]\" in reply):\n",
        "                    continue;\n",
        "                else:\n",
        "                    reply = reply.replace('\\n', ' ');\n",
        "                    reply = reply.replace('\\t', ' ');\n",
        "                    reply = reply.replace('\\'', '');\n",
        "                    reply = reply.replace('\\\"', '');\n",
        "                    reply = reply.replace('&gt', '');\n",
        "                    reply = reply.replace('&lt', '');\n",
        "                    #print(\"Context : \", context);\n",
        "                    \n",
        "                    #if (len(reply) > 500):\n",
        "                        #print(\"\\tLen :\", len(reply));\n",
        "                    #    t_reply = summarizeText(reply);\n",
        "                    #    if (len(t_reply) > 0):\n",
        "                    #        reply = t_reply;\n",
        "                    #print(\"\\tReply :\", reply);\n",
        "                    \n",
        "                    context.replace('\\t', ' ');\n",
        "                    \n",
        "                    reply.replace('\\t', ' ');\n",
        "                    reply = \" \".join(reply.split());\n",
        "                    \n",
        "                    if(\"Article has nothing\" in context):\n",
        "                        print(\"NIqqa\")\n",
        "                    \n",
        "                    if(reply.isspace() or context.isspace()):\n",
        "                        print(\"Error : \", reply, \", \", context)\n",
        "                        continue;                    \n",
        "                    else:\n",
        "                        line_to_write = \"{}\\t{}\\n\".format(reply.strip(), context.strip());\n",
        "                        file_handle.write(line_to_write)\n",
        "                    \n",
        "                    if('replies' in commentsJSON[i]['data'].keys()):\n",
        "                        if(commentsJSON[i]['data']['replies']):\n",
        "                            if('data' in commentsJSON[i]['data']['replies'].keys()):\n",
        "                                if(commentsJSON[i]['data']['replies']['data']):\n",
        "                                    if('children' in commentsJSON[i]['data']['replies']['data'].keys()):\n",
        "                                        recursePostComments(reply, commentsJSON[i]['data']['replies']['data']['children']);\n",
        "            else:\n",
        "                continue;\n",
        "        else:\n",
        "            continue;\n",
        "    return;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv0XIxKc7wdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key, value in listing_dict.items():\n",
        "    print(key, value)\n",
        "    r_comments = requests.get('https://www.reddit.com/r/worldnews/comments/' + key +'.json?limit=10000&sort=confidence&depth=4', \n",
        "                                headers = { 'User-agent' : 'ChatBot', \n",
        "                                            'Accept' : '*/*',\n",
        "                                            'Cache-Control' : 'no-cache' \n",
        "                                          });\n",
        "    r_comments_json = r_comments.json();\n",
        "    context = value;\n",
        "    commentsJSON = r_comments_json[1][\"data\"][\"children\"]\n",
        "    recursePostComments(context, commentsJSON)\n",
        "    \n",
        "file_handle.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBYwzxcD4Q5J",
        "colab_type": "text"
      },
      "source": [
        "#Overfitting and Testing Generative model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2rYrmca4QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1313H7z8i2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\"\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "dataset = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rWorldNews.txt\", # name of the file\n",
        "#dataset = torchtext.data.TabularDataset(\"test.txt\",\n",
        "                                        \"tsv\",               # fields are separated by a tab\n",
        "                                        fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqtnkW5vq0p1",
        "colab_type": "code",
        "outputId": "5ee36f15-0b55-49cb-f2e4-8afe0e83cf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6HfmfsMrRFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = torchtext.data.Dataset(dataset, fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkEjPWJ9wM4",
        "colab_type": "code",
        "outputId": "e9d5320d-0b08-4b17-b309-d3fe24896a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AVuOzo97fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey-KokGc9-zC",
        "colab_type": "code",
        "outputId": "f33572a3-6fb8-4d18-dda5-c8503538a220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(text_field.vocab.itos)\n",
        "print(label_field.vocab.itos)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'p', 'g', 'y', 'w', 'f', 'b', '.', 'v', 'k', ',', 'I', '-', '/', 'T', 'S', 'A', '0', '1', 'C', 'x', '*', ':', '2', 'j', 'P', 'M', 'W', 'R', 'E', 'N', '?', 'B', 'H', 'U', ')', 'D', '(', 'O', 'F', 'â€™', 'z', ';', '5', '3', 'G', '_', '8', '9', '6', 'L', '4', '7', 'K', 'q', ']', '[', 'Y', 'J', '!', 'V', '%', 'â€œ', 'â€', '=', '$', '&', '#', '~', '^', '|', 'Q', 'Z', 'â€˜', 'â€”', 'X', '+', '\\\\', 'â€“', '@', 'Ã©', 'Â£', 'â€¦', 'Ğ¾', 'Ã¼', 'Ğµ', 'â‚¬', 'â€¢', 'â €', 'Ã³', 'Ğ¸', 'Ğ½', 'ï¸', 'Ğ°', 'Ñ‚', 'Ã¡', 'Ã±', 'Ñ', 'Â°', 'ãƒ»', 'Ã¤', 'Ğ»', 'Ñ€', 'Ø§', 'Â§', '\\u200b', 'Ğ¼', 'Ñ‹', 'Ğ²', '`', 'Ğº', 'Ù†', 'Ğ´', 'Â´', 'âœ”', 'â£¿', 'Ğ¿', 'Ã­', 'ÛŒ', 'Ã¶', 'ï¼Œ', 'Ñ', 'ğŸ˜‚', 'Ğ±', 'Ñƒ', 'ã€‚', 'ğŸ¤”', '\\u200d', 'ğŸ’¦', 'Ø±', 'Â¥', 'Ù…', 'Ù‡', 'ã®', 'ãŒ', 'Ñ‡', 'Âº', 'ğŸ‡§', 'Ùˆ', 'Â¯', 'Â·', 'â– ', 'Ã„', 'â€š', 'Ù„', 'Â¶', 'ãŸ', 'ãª', 'ğŸ‡¬', 'Ø¯', 'Øª', 'ğŸ‘', 'Ğ³', 'ä¸', 'ã‹', 'ã¦', 'Ø¨', 'ã£', 'äºº', 'â˜…', 'ğŸ‘', 'ã€', 'ã„', 'â˜†', 'ÄŸ', 'Ãœ', 'ÑŒ', 'â„¢', 'ä¸€', 'Ğ·', 'ğŸ’˜', 'â‚‚', 'äº¤', 'å…¨', 'ï¼', 'ğŸ‡º', 'Ğ¹', 'â ›', 'âŒ', 'ãƒ„', 'ğŸ‡¸', 'ğŸ»', 'ğŸ”¥', 'ğŸ™„', 'Ã¨', 'ğŸ˜­', 'Ã‰', 'Ø¹', 'ã‚„', 'ç­‰', 'Ñ…', 'Ã¥', 'Ñ‰', 'Ã', 'Ğ¶', 'ã‚‰', 'ğŸ‘Œ', 'ä¸­', 'ã‚Œ', 'Ø³', 'Â»', 'Ãº', 'Ä', 'Ú©', 'â‚', 'â‚ƒ', 'ä¸–', 'å¹³', 'æ³¢', 'ç•Œ', 'è€…', 'é€', '\\ufeff', 'Â«', 'â€', 'à² ', 'ã¾', 'Î™', 'Ñˆ', 'Ñ', 'ã«', 'ã‚Š', 'ğŸ‡·', 'Ã§', 'Ù‚', 'â£€', 'æ˜¯', '\\uf8ff', 'ğŸ˜', 'ğŸ˜¢', 'âœŒ', 'ã‚‚', 'æ€§', 'ğŸ»', 'ğŸ‘‹', '\\xad', 'Ñ', 'í™”', '\\x03', 'Ø°', 'â£¤', 'ğŸ‡«', 'ğŸ‡®', 'ğŸ’©', 'ğŸ¤·', 'çš„', 'ğŸ‡­', 'ğŸ‡¹', 'ğŸ³', 'ğŸŒˆ', 'ğŸ¶', 'Ú¯', 'áƒ', 'âœ“', 'â¤', 'ã¨', 'ğŸ‡¦', 'Â®', 'á»™', 'â™‚', 'ã†', 'ã—', 'ã§', 'å½±', 'å¿ƒ', 'æŠ—', 'æ‹’', 'ç†', 'èƒ½', 'ğŸ˜’', 'Ä‘', 'Ì¿', 'âƒ£', 'ã‚¹', 'ğŸ’•', 'å', 'â™ª', 'âœ…', 'â ¶', 'ç›£', 'ç½ª', 'è­·', 'Ëš', 'Ì¯', 'Î‘', 'Ø´', 'ã€Œ', 'ã€', 'ä¸º', 'ğŸ˜', 'Îœ', 'ÎŸ', 'Î¡', 'Î£', 'Ğ•', '\\u202a', 'â˜‘', 'ğŸ¼', 'ğŸ’¥', 'ğŸ˜Ÿ', 'Ã«', 'Í¡', 'â‰ ', 'å‹•', 'ë³´', 'ğŸµ', '{', '}', 'Ë', 'ØŒ', 'âˆ¼', 'â ‰', 'â¢€', 'â£¶', 'ì¼', 'ğŸ˜±', 'Ã—', 'É™', 'Ëˆ', 'à¤¾', 'ä¸Š', 'ä½†', 'æ°‘', 'ğŸ‘‘', 'ğŸ˜', 'ğŸ˜¬', 'â—•', 'ã¤', 'ãƒ¼', 'ä¸»', 'ä¹‰', 'äº’', 'ä»', 'å†²', 'åŒº', 'åˆ', 'å’Œ', 'åœ°', 'å¤ª', 'å¨', 'å®‰', 'å±‚', 'æ€–', 'æ', 'æ„', 'æˆ‘', 'æœª', 'ç‚¹', 'çƒ­', 'ç›¸', 'çœŸ', 'çª', 'ç»‡', 'ç»œ', 'ç½‘', 'èƒ', 'è’™', 'èµ·', 'é—®', 'é˜´', 'é¢˜', 'ğŸ‡ª', 'ğŸ˜‰', 'ğŸ˜', 'Ã”', 'ÌŠ', '\\u200e', 'â˜º', 'ã', 'ã‚‹', 'ç”Ÿ', 'Ã ', 'Ì˜', 'Ì³', 'Í•', 'Ñ†', 'Ø®', 'â€½', 'â‚‡', 'âˆ', 'ã', 'ã‘', 'ã', 'ã‚', 'ã‚¿', 'å¥³', 'çª“', 'ğŸ‡©', 'ğŸ½', 'ğŸ˜Š', 'Â¡', 'Â¢', 'Ä°', 'Ì¤', 'Í”', 'Íœ', 'Ğ', 'Ù', 'áº¿', 'å°', 'Ë', 'Ì™', 'Ì¹', 'Ğ’', 'ÙŠ', 'â–€', 'â ', 'æ—¥', 'æœ¬', 'é¢', 'ğŸ‡°', 'ğŸ‡±', 'ğŸ‡²', 'ğŸ‡½', 'Ä', 'Ä±', 'Ì–', 'Í', 'Í«', 'Î”', 'Ğ', '\\u2060', 'â¶', 'â˜', 'â ƒ', 'â ‹', 'â£„', 'æ”¾', 'é‡‹', 'ê·¼', 'ë¬¸', 'ë°•', 'ë²•', 'ì–´', 'ì´', 'í˜œ', 'ğŸ’¡', 'ğŸ˜«', 'Â¤', 'Ã°', 'Æ¡', 'Ìª', 'Ì®', 'Ì°', 'Íˆ', 'Í’', 'Í¤', 'Í¨', 'Ğ£', 'Ø­', 'áº£', 'á»…', 'á»‡', 'á»‹', 'á»', 'á»­', 'ä¸‹', 'å€‘', 'å‚·', 'å›', 'å¤§', 'å­¸', 'æ’¤', 'æš´', 'æ—', 'é„­', 'ğŸ™ƒ', 'ÃŸ', 'Ì‡', 'Ì', 'Ì²', 'Í', 'Í', 'Î', 'Ø·', 'Ù¾', 'áƒ•', 'áƒ ', '\\u200c', '\\u200f', 'â ¿', 'ã“', 'ğŸ˜¤', 'ğŸ˜®', 'Â²', 'Ê–', 'Ì€', 'Ìˆ', 'Ì’', 'Ì”', 'Ì', 'Ì ', 'Ì»', 'Ì¾', 'Í‡', 'Í“', 'Î•', 'Î—', 'Îš', 'Î ', 'Î¥', 'Î¦', 'Ğ”', 'Ğ›', 'Ğ', 'áƒš', 'áƒ', 'â€ ', 'â†“', 'â™€', 'â™©', 'â¡€', 'â¡Ÿ', 'â£·', 'ä¼', 'å¯', 'æ¬º', 'æ­»', 'æ¸¯', 'ç‚º', 'çƒ', 'ç¡¬', 'ç½²', 'è¯', 'é¦™', 'ìŠ¤', 'ğŸŒ³', 'ğŸ†', 'ğŸ˜‹', 'ğŸ¤£', 'Ã–', 'Ì†', 'Ì—', 'Ìœ', 'Ì«', 'Ì¬', 'Ì±', 'Ì¶', 'Í„', 'Í–', 'Ğ˜', 'à¤°', 'à¤¹', 'ã•', 'ã‚ˆ', 'ã‚“', 'ãƒ—', 'ì€', 'ï¼‰', 'ğŸ˜•', 'ğŸ™‚', 'ğŸ¤—', 'ğŸ¤¦', 'Ãˆ', 'Ãª', 'Ã®', 'Ã²', 'Ì„', 'Ì…', 'ÌŒ', 'Ì‘', 'Ì', 'ÌŸ', 'Ì­', 'Ì¼', 'Ì½', 'Í…', 'Í™', 'Í›', 'Ğ ', 'Ø²', 'à¤ˆ', 'à¤¤', 'à¤¦', 'à¤®', 'à¥‡', 'à¼¼', 'à¼½', 'áƒ˜', 'â¢ ', 'ã¯', 'ã‚’', 'ãƒ¬', 'ãƒ³', 'ç„¶', 'è…¦', 'è©±', 'é–‹', 'ê¸¸', 'ë‹¤', 'ë‘ ', 'ë¹›', 'ìˆ˜', 'ì—†', 'ì„', 'ğŸ‘€', 'ğŸ’¯', 'ğŸ–', 'ğŸ–‘', 'ğŸ˜”', 'ğŸ˜˜', 'ğŸ˜¡', 'ğŸ˜£', 'ğŸ˜³', 'ğŸ™', 'Ã¯', 'Ä“', 'Ì', 'Ì', 'Ìš', 'Ì£', 'Ì¦', 'Ì§', 'Í—', 'Í', 'Íª', 'Í­', 'Ñ„', 'Ø¬', 'Ùƒ', 'â€•', 'âš ', 'â£¾', 'ã€', 'ã€', 'ãˆ', 'ãŠ', 'ã³', 'ã‚', 'ã‚¤', 'ã‚¯', 'ã‚±', 'ã‚³', 'ã‚µ', 'ãƒ†', 'ãƒ‘', 'ãƒ˜', 'ãƒª', 'ä»˜', 'ä»²', 'ä½“', 'ä½•', 'åŠ©', 'å–', 'å£', 'åº¦', 'å»º', 'æ€¥', 'æ•‘', 'æ®‹', 'æ±‚', 'æ³£', 'ç„¼', 'ç…¤', 'ç‰©', 'ç”·', 'ç¹°', 'è', 'è¦‹', 'èµ¤', 'èº«', 'è»Š', 'è¿”', 'é¡”', 'é£›', 'é¨’', 'é»’', 'ï¼ˆ', 'ğŸ‹', 'ğŸ¤', 'ğŸ˜ƒ', 'ğŸ¦„', 'Ì', 'Ìµ', 'Ì·', 'Ìº', 'Í‹', 'Í', 'Íš', 'ÍŸ', 'Í§', 'Í©', 'Ğ¢', 'ÑŠ', 'Ø©', 'â–¤', 'â¡„', 'â¡¿', 'â¢¿', 'â£¼', 'ã ', 'ã¬', 'ã‚', 'æ€', 'è‰¯', 'é•', 'ï¼Ÿ', 'ğŸ‡¿', 'ğŸ¿', 'ğŸ¬', 'ğŸ’œ', 'ğŸ’¨', 'ğŸ˜…', 'ğŸ™Œ', 'Â¿', 'Ä—', 'Å', 'Ì', 'Ì¡', 'Ì¥', 'Í‚', 'Í‰', 'ÍŠ', 'Í£', 'Í¥', 'Í¬', 'Í¯', 'ĞŸ', 'â„…', 'â ‡', 'â ˆ', 'â ˜', 'â ™', 'â¢°', 'ã€', 'ã€‘', 'é–“', 'êµ­', 'ì•„', 'í†µ', 'í•œ', 'ğŸŒ', 'ğŸ‘', 'ğŸ˜‘', 'Â©', 'Â½', 'Ä¹', 'Ì‚', 'Ìƒ', 'Ì“', 'Ì•', 'Ì›', 'Ì¨', 'Ì©', 'Íƒ', 'Í', 'Í‘', 'Í®', 'Ò‰', '×', '×•', '×™', 'Ø£', 'Ø¦', 'Øµ', 'Ú†', 'áƒ”', 'áƒ›', 'â‚¹', 'â˜', 'â˜¹', 'â™«', 'â™¬', 'â ¦', 'â »', 'â¢¤', 'â¢¹', 'â£‡', 'â£ ', 'å®¶', 'æˆ', 'æ¬Š', 'æ¸¸', 'ç©', 'ç·š', 'é™£', 'ğŸ‡¨', 'ğŸ‰', 'ğŸ‘', 'ğŸ“£', 'ğŸ•´', 'ğŸ˜€', 'ğŸ˜°', 'ğŸ™Š', 'ğŸš¨', '\\U0001f92d', 'Â±', 'Ã´', 'Ãµ', 'Ì‰', 'Ì‹', 'Ì¢', 'Í†', 'ÍŒ', 'Í ', 'Í¢', 'Ğ“', 'Ğ—', 'Ğ¡', 'Û•', 'áƒ¡', '\\u202c', 'âœ‹', 'â Ÿ', 'â¬‡', 'å…´', 'åˆ°', 'å›½', 'å¥‹', 'æ„Ÿ', 'è‰', 'é ­', 'ë…•', 'ì•ˆ', 'ì¬', 'ğŸ…±', 'ğŸ', 'ğŸ‘©', 'ğŸ’–', 'ğŸ’—', 'ğŸ’Ÿ', 'ğŸ“', 'ğŸ˜', 'ğŸ˜“', 'ğŸ˜¶', 'ğŸ™ˆ', 'ğŸ™‰', '\\U0001f92e', '\\x8f', 'Ã¢', 'Ã¸', 'ÅŸ', 'Í€', 'Í', 'Ğœ', 'Ğ¨', 'Ğ«', 'Ğ¬', 'Ø¢', 'Ø¥', 'Ø¸', 'à¸‡', 'à¸§', 'áƒ’', 'áƒ—', 'áƒœ', 'áƒ£', 'áƒ¥', 'áƒª', 'áƒ«', 'áƒ®', 'âŠ™', 'â–', 'â„', 'â Š', 'â ¢', 'â ³', 'â ´', 'â ¸', 'â¡¶', 'â¡¼', 'â¡¾', 'â¢¸', 'â¢»', 'äº‹', 'åŠ´', 'åŒ—', 'å—', 'æœ', 'çµ±', 'é', 'éŸ“', 'é ˜', 'é®®', 'êµ', 'ëŒ€', 'ë‘', 'ë¦¬', 'ì„¸', 'ì—', 'ì—¬', 'ì˜', '\\uf06f', 'ğŸŒ¸', 'ğŸ›', 'ğŸ´', 'ğŸ‘Š', 'ğŸ‘¨', 'ğŸ‘­', 'ğŸ‘»', 'ğŸ’€', 'ğŸ’', 'ğŸ’“', 'ğŸ“…', 'ğŸ“²', 'ğŸ—£', 'ğŸ—³', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜¦', 'ğŸ˜©', 'ğŸ˜¯', 'ğŸ˜²', 'ğŸ™…', 'ğŸ™', 'ğŸ¥”', '\\U0001f9b9', '\\U000e0062', '\\U000e0063', '\\U000e0067', '\\U000e0073', '\\U000e0074', '\\U000e007f', 'Äº', 'Å„', 'Ì´', 'Ì¸', 'Í˜', 'Í¦', 'Ğš', '×“', '×”', '×˜', '×œ', '×£', '×¨', '×©', '×ª', 'Ø¤', 'Øº', 'Ù', 'Û°', 'Ûµ', 'á›', 'á••', 'á•—', 'â€', 'âœ¨', 'âŒ', 'â£´', 'ã™', 'å…«', 'å†‡', 'å¦', 'æ¶ˆ', 'ç£', 'ç„¡', 'è€—', 'é‡', 'ê±°', 'ì‹¸', 'ì˜¤', 'ìš”', 'ì˜', 'ğŸŒŠ', 'ğŸŒ±', 'ğŸ·', 'ğŸ…', 'ğŸ¾', 'ğŸ’›', 'ğŸ’ª', 'ğŸ’°', 'ğŸ’µ', 'ğŸ˜œ', 'ğŸ˜¨', 'ğŸ¤š', 'ğŸ¤¢', '\\U0001f92a', '\\U0001f92b', 'ğŸ¥‡', '\\U0001f970', 'Âµ', 'Ã…', 'Ã‡', 'Ã', 'Ã', 'Ã£', 'Ä', 'É¡', 'ËŒ', 'Ğ¤', 'Ğ­', 'Ñ‘', 'à¤¸', 'â€¿', 'â„', 'â‰¥', 'â–ƒ', 'â–¥', 'â–¦', 'â–º', 'â˜­', 'âš«', 'âœŠ', 'âœ¿', 'â „', 'â ', 'â “', 'â š', 'â ', 'â «', 'â ¹', 'â ¾', 'â¡†', 'â¡‡', 'â¡', 'â¡œ', 'â¡·', 'â¢£', 'â¢§', 'â£†', 'â£•', 'â£˜', 'â£¦', 'â£§', 'â£¹', 'â£»', 'ã€Š', 'ã€‹', 'ã¸', 'ã‚¢', 'ä»•', 'ä»¶', 'å†…', 'å‡º', 'å‹™', 'å²', 'åŒ', 'å“ˆ', 'åœ‹', 'å­', 'å°‡', 'å¿—', 'æ‰€', 'æ–‡', 'æ–¹', 'æ¤¿', 'æ­·', 'ç‰ˆ', 'ç‹', 'ç”²', 'ç›®', 'ç¤º', 'ç§‘', 'è‡ª', 'è¡Œ', 'è¡¨', 'è¦', 'è»', 'ëƒ…', 'ëœ¨', 'ë¼', 'ë§™', 'ë°”', 'ì‚¬', 'ì†Œ', 'ì „', '\\uf0b7', 'ï¼š', 'ğŸ”', 'ğŸ‚', 'ğŸ¢', 'ğŸ»', 'ğŸ¼', 'ğŸ‘‰', 'ğŸ’¿', 'ğŸ–•', 'ğŸ˜¼', 'ğŸ¤', 'ğŸ¤', '\\U0001f928', 'Â³', 'Ã¦', 'Ã¬', 'Ã¹', 'Ã¾', 'Ä€', 'Å«', 'Å¼', 'É', 'É’', 'Ê‡', 'Ê', 'Ê', 'Î’', 'Ğ‘', 'Ğ¥', 'Ø¡', 'Ø¶', 'Ù‰', 'à¤‚', 'à¤¡', 'à¥€', 'à¦¦', 'à¦§', 'à¦¨', 'à¦¬', 'à¦¯', 'à¦¾', 'à§', 'áƒ‘', 'áƒ™', 'áƒ¢', 'áƒ§', 'áºµ', 'á»¨', 'á»¹', 'âˆ€', 'â‰ˆ', 'â–½', 'â›½', 'â —', 'â °', 'â ·', 'â¡', 'â¡›', 'â¢¶', 'â£¥', 'â£°', 'ã€œ', 'ã‚', 'ã”', 'ã–', 'ãš', 'ã­', 'ã‚ƒ', 'ã‚¬', 'ãƒˆ', 'ãƒ‹', 'ãƒ', 'ãƒ™', 'ãƒ¡', 'ãƒ©', 'ãƒ«', 'ãƒ¾', 'ä¸‰', 'ä¸²', 'äº†', 'äºŒ', 'äº›', 'ä¼š', 'ä¼¯', 'ä½³', 'ä¾†', 'å€™', 'å´', 'åˆ†', 'åŠ ', 'å˜', 'å§', 'å“¡', 'å››', 'å› ', 'å¦', 'å ±', 'å¤–', 'å®˜', 'å°‘', 'å°”', 'å±‹', 'å¾“', 'æ€', 'æ‚²', 'æ†²', 'æ‰‰', 'æ–­', 'æ–¯', 'æ™‚', 'æš«', 'æœƒ', 'æœ‰', 'æœ¨', 'æœ', 'æ¤', 'æ¥­', 'æ¨‚', 'æ­¢', 'æ±‰', 'æ²¹', 'æ´‹', 'æ¼”', 'ç«', 'çˆ±', 'ç‰¹', 'ç„', 'ç”¨', 'çœ‹', 'ç¤¾', 'ç¦', 'ç¬‘', 'çµ', 'ç·©', 'è‰¾', 'èª', 'è®š', 'è³', 'è»¢', 'è¼‰', 'é‚„', 'éƒ¨', 'é…¸', 'é‡Œ', 'éµ', 'é–‰', 'é–¢', 'é˜¿', 'é›–', 'ê°€', 'ê¸°', 'ë…„', 'ë§', 'ë¬´', 'ì‹', 'ì‹ ', 'ì—ˆ', 'ìœ¡', 'ìˆ', 'ìŸ', 'ì§„', 'ì§œ', 'í• ', 'í•´', 'í—', 'í˜„', 'í¬', 'ï½€', 'ï¾‰', 'ï¿¼', 'ğŸ‡³', 'ğŸŒ™', 'ğŸ‘', 'ğŸ¶', 'ğŸ¼', 'ğŸ¾', 'ğŸŠ', 'ğŸ…', 'ğŸ‰', 'ğŸ£', 'ğŸ‘†', 'ğŸ’™', 'ğŸ’š', 'ğŸ’²', 'ğŸ“‰', 'ğŸ“·', 'ğŸ•¸', 'ğŸ˜ˆ', 'ğŸ˜¾', 'ğŸš€', 'ğŸ¤“', 'ğŸ¤¤', '\\U0001f92f', 'ğŸ¥', '\\U0001f967', '\\U0001f9d0', '\\U0001f9d8', '\\U0001f9e0', '\\U0001f9e1']\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'y', 'g', 'p', 'w', 'f', '.', 'b', 'v', 'k', ',', 'I', 'T', '/', '-', 'A', 'S', '0', 'x', 'j', 'C', '?', 'W', '1', 'N', '*', 'E', 'H', 'M', 'P', 'R', 'B', '2', ':', 'U', 'O', 'D', 'â€™', ')', '(', 'z', 'F', ';', 'Y', 'G', 'L', '!', 'q', '5', '3', 'K', '_', '9', '4', '8', 'J', '6', '7', ']', '[', 'V', '%', 'â€œ', 'â€', '=', '&', '^', '~', '$', '#', 'Q', 'Z', 'X', '\\\\', '+', '|', 'â€”', 'â€˜', 'â€“', 'Ã©', 'Â£', '@', 'â£¿', 'â €', 'Ğ¾', 'â€¦', 'ï¸', 'ğŸ˜‚', 'Ğµ', 'Ğ°', 'Ø§', 'Ã¼', 'Â°', 'Ğ¸', 'Ñ‚', 'Ğ½', 'â€¢', 'â‚¬', 'Ñ', 'ğŸ¤”', '`', 'ğŸ‘€', 'Ù„', 'Ñ€', 'ğŸ‘', 'Ğ²', 'Â¯', 'Ğ»', 'Ù†', 'Ã¡', 'Ã¤', 'Ã¶', 'Ã³', 'Ø±', 'Ğº', '\\u200b', 'Ù…', 'â”€', 'ÙŠ', 'Ã±', '\\u200d', 'Í¡', 'Ğ´', 'Ğ¼', 'ğŸ‘', 'Øª', '×™', 'Ñƒ', 'Ø¨', 'Ùˆ', 'ğŸ‘Œ', 'Ğ¿', 'Â´', 'Ñ', 'ğŸ»', 'Ñ‹', 'Ã­', 'ğŸ™„', 'â„¢', '×•', 'Ù‡', 'ãƒ„', 'Ø¯', 'â¤', 'Ì¶', '×ª', 'Â§', 'Ğ±', '×', 'Â·', 'ğŸ¶', '×”', 'Ùƒ', 'ÑŒ', 'Íœ', '×œ', 'Ø¹', 'â ›', 'ğŸ˜', 'ÄŸ', 'Ø³', 'ğŸ¤£', 'ğŸ¤·', 'Ê–', 'ÛŒ', 'ğŸ‡º', 'ğŸ˜­', 'â€', 'â™‚', 'Ğ¹', 'Ğ·', 'Ù', 'Ù‚', 'Ã§', 'ğŸ¼', 'Ã¨', '×', 'Â¥', 'Ø©', 'Ñ‡', '×¨', 'à² ', 'â– ', 'â£€', 'ï¼Œ', 'ğŸ‡¸', '{', '}', 'Â«', 'Â»', 'Ø£', 'Ğ³', 'Ñ…', '×©', 'ğŸ‡§', 'ğŸ‡¦', 'â£¤', '\\u2060', 'ğŸ˜‰', 'Ã¥', '×‘', 'â£¶', 'Ã ', 'Ä', 'ØŒ', 'âœ”', 'ã€‚', 'äºº', 'ğŸ‡¬', 'ğŸŒˆ', 'ğŸµ', 'ğŸ˜”', 'Ø­', 'ğŸ˜', 'ğŸ˜Š', 'ğŸ˜¢', 'â™€', 'ğŸ’©', 'â€•', 'ğŸ’¦', 'Ğ¶', 'â¢€', 'ã„', 'ãŒ', 'áƒ', 'ğŸ™', 'ÃŸ', 'Ã¯', 'Ì˜', 'Ì¯', 'â™ª', 'â ¿', 'å¤§', 'Â¡', 'Âº', 'Ãº', 'Ø´', 'â ‰', 'ğŸ‡¨', 'ğŸ˜', 'ğŸ˜¤', 'É™', 'Í•', 'Ñ†', 'â ¶', 'ã®', 'ğŸ½', 'ğŸ”¥', 'Â²', 'Ã—', 'Ì¤', '× ', 'Ø®', 'âŒ', 'ãª', 'æ°‘', 'Ì«', 'Ì¼', 'â˜…', 'â ', 'ğŸ¤¦', 'Ã¢', 'Ëˆ', 'ÌŠ', 'Ì™', 'Ñ', 'ã€', 'ã¨', '\\ufeff', 'ğŸ‘‰', 'ğŸ˜¬', 'Ã„', 'Ã‰', 'Ì³', 'Ì¿', 'Í„', 'Í”', 'Ñ', '×›', 'Ú©', 'â˜', 'â „', 'â ‹', 'ãŸ', 'ãƒ»', 'ğŸ˜', 'ğŸ™ƒ', 'Ì', 'Ìª', 'Ì±', 'Ì²', 'Ì¹', 'Í', 'Ñ‰', 'âœŒ', 'ã¦', 'ğŸ‰', 'ğŸ³', 'ğŸ˜', 'ğŸ˜¡', 'Ã¸', 'Ì‡', 'Ì', 'Ì’', 'Í‰', 'Í', 'Í–', 'âœ±', 'â ƒ', 'â¡¿', 'â£„', 'â£·', 'â£¾', 'ã—', 'ä¸', 'ğŸ‡ª', 'ğŸ˜©', 'Ã´', 'Ì„', 'Ì–', 'Ì°', 'Í’', 'Ğ', 'ĞŸ', 'â€½', 'â‰ ', 'ã‹', 'å¤©', 'å®‰', 'æ³•', 'çš„', 'ğŸ’•', 'ğŸ˜…', 'ğŸ˜•', 'Â¢', 'Â¶', 'Ã£', 'Ñˆ', '×“', '×¢', 'ã¯', 'è‡ª', 'ğŸŒŠ', 'ğŸ˜€', 'ğŸ˜’', 'Ãª', 'Ä±', 'Ì', 'Ì ', 'Ì®', 'Ì¾', 'Í…', 'Í‡', 'ÍŠ', 'Í', 'Í“', 'Ğ ', 'Ğ¡', '×—', '×', 'Ø°', 'Øµ', 'Ø·', 'â€š', 'â˜º', 'ã§', 'ã«', 'å', 'ğŸ…±', 'ğŸ»', 'ğŸ˜±', 'ğŸ˜³', 'ğŸ¦€', 'Â¿', 'Ì€', 'Ì…', 'Ì†', 'Ìˆ', 'ÌŒ', 'Ì“', 'Ìš', 'Ì£', 'Íˆ', 'Í™', 'Íš', 'Ğ', 'Ğ”', 'âœŠ', 'â¡€', 'â¡Ÿ', 'ä¸­', 'ğŸ‡¹', 'ğŸ˜‘', 'Ãœ', 'Ì', 'Ì', 'Ì‘', 'Ì”', 'Ìœ', 'Ì¦', 'Ì©', 'Ì»', 'Í«', 'Ø¬', 'âœ…', 'â¢¿', 'ã£', 'ä¸€', 'åˆ¶', 'ğŸ‡©', 'ğŸ‡²', 'ğŸ‡·', 'ğŸ¿', 'ğŸ‘‹', 'ğŸ˜®', 'Â®', 'É', 'Ì•', 'Ì—', 'Ì¥', 'Ì§', 'Ì¬', 'Ì­', 'Ìº', 'Í', 'Í—', 'Í', 'Í¤', 'Í¨', 'Ğ˜', 'Ğ', 'Ñ„', '×§', 'Ø¥', 'âˆ¼', 'â˜†', 'â˜¹', 'âœ‹', 'â ˆ', 'ã†', 'ã‚Š', 'æ˜¯', 'ï¼', 'ğŸ’€', 'ğŸ’¯', 'ğŸ’°', 'ğŸ˜Ÿ', 'ğŸ™Œ', 'Ëš', 'Ì‚', 'ÌŸ', 'Í›', 'Ğ•', '×’', '×–', 'â˜‘', 'âœ“', 'â¢ ', 'ã¤', 'ã‚‰', 'ã‚Œ', 'å¹³', 'æˆ‘', 'é–€', 'ğŸ‡½', 'ğŸº', 'ğŸ‘', 'ğŸ’œ', 'Ã', 'Éª', 'Ì‰', 'Ì½', 'Í ', 'Ğ’', 'Ğ¯', '×˜', 'Ù‰', 'áƒ ', 'â—•', 'â™«', 'â Ÿ', 'â£¼', 'ã‚‹', 'ãƒ¼', 'å¿—', 'ç£', 'è¼ª', 'é—¨', 'ğŸ‡®', 'ğŸ’˜', 'ğŸ˜«', '\\x03', 'Ã°', 'Ìƒ', 'Ì', 'Ì', 'Í‚', 'Í†', 'Í‹', 'ÍŒ', 'Ğ‘', 'Ğ¢', '×š', '×Ÿ', '×¤', 'à¤¾', 'áƒ˜', 'â »', 'â¡„', 'â£ ', 'ã', 'ã™', 'ğŸ˜¥', 'ğŸ¤™', 'Ã®', 'Ì›', 'Ì¡', 'Ì¨', 'Ìµ', 'Ì¸', 'Íƒ', 'Í‘', 'Ú¯', '\\u200f', 'â„', 'â™¥', 'âŒ', 'â¢°', 'ã•', 'ã¾', 'å°', 'æ', 'æ³¢', 'çœŸ', 'êµ­', 'ì€', 'ì´', 'ğŸ’ª', 'ğŸ˜ƒ', 'ğŸ˜', 'ğŸ¥š', 'Ã¦', 'Ë', 'Ì·', 'Í€', 'Í˜', 'ÍŸ', 'Íª', 'Í­', '×¡', 'Øº', 'áƒš', 'áƒ›', 'â ™', 'â£´', 'ã“', 'ã ', 'ã‚„', 'ã‚¤', 'äº‚', 'åŠŸ', 'å’Œ', 'å¤š', 'æŠ—', 'æ´ª', 'ç”Ÿ', 'ç”±', 'ì¼', 'ğŸ‡«', 'ğŸ‡°', 'ğŸ‡±', 'ğŸ†', 'ğŸ¾', 'ğŸ‘»', 'ğŸ“£', 'ğŸ–•', 'ğŸ˜‹', 'ğŸ˜˜', 'ğŸ™‚', 'ğŸ¤—', '\\U0001f92e', 'Â½', 'Ã–', 'Ã«', 'Ä“', 'Ä«', 'Ä°', 'Ê²', 'Ì‹', 'Ì¢', 'Ì´', 'Í', 'Í§', 'Í©', 'Ğš', 'Ğ£', 'Ğ¤', 'Ñ™', 'Ø¶', 'áƒ•', '\\u200e', 'â€', 'âˆ’', 'â–¤', 'â ‡', 'â ˜', 'â¢¸', 'â£‡', 'ã‚’', 'ã‚µ', 'ã‚¹', 'ãƒ«', 'äº‹', 'å‹•', 'æ™‚', 'æš´', 'æ¬Š', 'è«–', 'ì–´', 'ï¾Ÿ', 'ğŸ‡­', 'ğŸ‚', 'ğŸ‘ˆ', 'ğŸ’›', 'ğŸ˜„', 'ğŸ˜¹', '\\x80', '\\xad', 'Å', 'É¥', 'Í£', 'Í¥', 'Í¬', 'Í¯', '×¦', 'Ø¢', 'Ø¦', 'Ù¾', 'áƒ”', 'â‚‚', 'âƒ£', 'â£‰', 'ã‚', 'ã‚“', 'ãƒ', 'ãƒŸ', 'äº†', 'äº¬', 'å…±', 'åŒ–', 'å‘½', 'åœ‹', 'åœ¨', 'å­', 'å¼·', 'å¿ƒ', 'æ€', 'æ€§', 'æœ‰', 'æ­»', 'æ®º', 'ç‰¹', 'ç¶­', 'ç½‘', 'è¨€', 'é', 'ë³´', 'í™”', 'ğŸ‡¼', 'ğŸ‡¿', 'ğŸŒ', 'ğŸ’…', 'ğŸ’™', 'ğŸ’š', 'ğŸ˜“', 'ğŸ™', 'ğŸ™Š', 'ğŸš¨', 'ğŸ¤‘', 'ğŸ¤š', 'ğŸ¤¤', 'ğŸ¦†', 'Â±', 'Ãµ', 'Ã¹', 'Å“', 'ÅŸ', 'Ë£', 'Í', 'Í®', 'Ò‰', 'Ø¡', 'à¼¼', 'à¼½', 'áƒœ', 'áƒ', 'á›', 'áµ', 'á»™', '\\u202c', 'â‚', 'â‚‡', 'â‚¹', 'â„…', 'â†“', 'â˜', 'â ¦', 'â¢¤', 'â¢¹', 'ã€Œ', 'ã€', 'ã‚‚', 'ã‚ˆ', 'ã‚¯', 'ãƒ³', 'ä¸»', 'ä»¶', 'å…¨', 'å…­', 'åŠ‰', 'åŠ¨', 'åŒ—', 'åˆ', 'åŒ', 'å ±', 'å£“', 'å± ', 'æ€', 'æ“¾', 'æ”¿', 'æ–‡', 'æ˜¥', 'ç‚º', 'çˆ¾', 'ç¨', 'ç™º', 'ç§', 'ç­‰', 'è¦‹', 'è³£', 'é€²', 'é‹', 'é‚„', 'é©', 'é¨·', 'é»¨', 'ë¯¼', '\\uf8ff', 'ï¿¼', 'ğŸ‘‘', 'ğŸ’', 'ğŸ’“', 'ğŸ˜†', 'ğŸ™…', 'ğŸ™ˆ', 'ğŸ™‰', '\\x9d', 'Â©', 'Ä‘', 'Í¢', 'Î”', 'Ğ“', 'Ğ›', 'Ø«', 'à¨¾', 'à¸‡', 'á••', 'á•—', 'â€ ', 'â¶', 'â‚ƒ', 'â‰¥', 'â˜ ', 'â˜»', 'â ´', 'â¢', 'â¢»', 'ã‘', 'ã‚¿', 'ãƒŠ', 'ä¸‹', 'ä½†', 'å®³', 'æ„', 'æ—¥', 'æœƒ', 'ç‰©', 'ç†', 'ç¤º', 'çµ±', 'è¡Œ', 'è­°', 'é€', 'é€š', 'é¢', 'ìŠ¤', 'í†µ', 'í•œ', 'ï¼Ÿ', 'ï¾‰', 'ğŸŒ³', 'ğŸ‘', 'ğŸ„', 'ğŸŠ', 'ğŸ¼', 'ğŸ´', 'ğŸ', 'ğŸ‘', 'ğŸ‘¨', 'ğŸ’–', 'ğŸ’¨', 'ğŸ“·', 'ğŸ””', 'ğŸ˜Œ', 'ğŸ˜§', 'ğŸ˜°', 'ğŸ˜¸', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¦', '\\U000e0062', '\\U000e0063', '\\U000e0067', '\\U000e0073', '\\U000e0074', '\\U000e007f', 'Â¤', 'Âµ', 'Ã‡', 'Ã', 'Ã”', 'Ã²', 'Ä—', 'Å«', 'Ç', 'Ê‡', 'Ë', 'Î™', 'Ğ¥', 'Ğ¨', 'Ñ‘', 'Ø²', 'Ø¸', 'à¤°', 'à¤¸', 'à¤¹', 'à¥‡', 'à¨•', 'à¸¢', 'áƒ¡', 'áƒ£', 'â‰ˆ', 'â”»', 'â•¯', 'â–€', 'â–ˆ', 'â–', 'â™©', 'âš•', 'âš¾', 'â„', 'â Š', 'â —', 'â ¢', 'â ³', 'â ¸', 'â¡¶', 'â¡¼', 'â¡¾', 'â£†', 'â£¦', 'â£°', 'â­', 'ã€‹', 'ã€', 'ã€‘', 'ãŠ', 'ã‚', 'ãƒ—', 'ãƒ®', 'ä¸Š', 'ä¼¯', 'å‡º', 'åŠ´', 'åš', 'å£', 'å•', 'å››', 'åœ°', 'å¨', 'å®˜', 'å°„', 'æ—', 'æœ¬', 'æ¸¯', 'æ¸¸', 'æ¼”', 'ç„¶', 'ç”¨', 'ç›£', 'çœ‹', 'çŸ¥', 'èƒ½', 'è…¦', 'è¦–', 'è©•', 'èª', 'è­·', 'è®Š', 'èµ°', 'é“', 'é›£', 'é£Ÿ', 'é«˜', 'ë‹¤', 'ëŒ€', 'ë¬¸', 'ìˆ˜', 'ì—', 'ìš”', 'ì£¼', '\\uf06f', 'ï¼‰', 'ï¼', 'ï½…', 'ï½ˆ', 'ğŸŒ²', 'ğŸº', 'ğŸ‰', 'ğŸ±', 'ğŸ‘©', 'ğŸ”«', 'ğŸ˜ˆ', 'ğŸ˜–', 'ğŸ˜œ', 'ğŸ˜¨', 'ğŸ˜¯', 'ğŸ¤–', '\\U0001f928', '\\U0001f9e1', 'Â³', 'Â¹', 'Ã“', 'Ä€', 'Ä', 'Ä™', 'Æ¡', 'É‘', 'É¹', 'Ê', 'ËŒ', 'Í¦', 'Î‘', 'Î£', 'Îµ', 'Ï„', 'Ğ—', 'Ğœ', 'Ğ¬', 'Ğ®', '×£', 'Ø¤', 'Ú†', 'à¤¿', 'à¨¦', 'à¨°', 'à©‡', 'à©‹', 'à¸§', 'à¹€', 'áƒ—', 'áƒ®', 'áº£', 'áº¿', 'á»‹', 'á»¹', '\\u202a', '\\u202d', 'âˆ€', 'âˆ', 'â–º', 'â˜•', 'â˜­', 'â˜®', 'âš°', 'âœ¨', 'â ', 'â š', 'â ¹', 'â¡‡', 'â¡', 'â¢¶', 'â¬†', 'ã€Š', 'ã', 'ã¡', 'ã­', 'ã¿', 'ã‚¢', 'ã‚°', 'ã‚³', 'ãƒƒ', 'ãƒ‘', 'ãƒ©', 'ãƒª', 'ãƒ¬', 'ä¸¹', 'ä¹ˆ', 'ä¹‹', 'ä¹', 'ä¹Ÿ', 'äº›', 'äº¤', 'ä»€', 'ä»–', 'ä»£', 'ä¾µ', 'å€‘', 'å€™', 'å…ƒ', 'å…«', 'åŠ ', 'å€', 'å¦', 'å¤', 'å¯', 'å³', 'å¾', 'å“', 'å”', 'å•Š', 'å–‡', 'å–œ', 'å˜›', 'å™¨', 'å›½', 'åœŸ', 'å •', 'å£', 'å£«', 'å¥ª', 'å¥³', 'å®¶', 'å¯¦', 'å°ˆ', 'å°', 'å°±', 'åº¦', 'å»º', 'å¼Ÿ', 'å½©', 'å¾ˆ', 'å¾Œ', 'æƒ³', 'æ‹', 'æ‹’', 'æ‹·', 'æ ', 'æ‘˜', 'æ–°', 'æ–·', 'æ–¹', 'æ–¼', 'æš', 'æ›‰', 'æœ', 'æ ·', 'æ¯’', 'æ²¹', 'æ²»', 'æ´»', 'æ´¾', 'æµ', 'æ¶ˆ', 'æ·¨', 'æ·«', 'æ¸…', 'æ¿€', 'ç„¡', 'çˆ­', 'ç', 'ç‹', 'ç”£', 'ç•¥', 'ç•«', 'ç–†', 'ç›§', 'ç ´', 'ç¨®', 'ç«‹', 'ç­–', 'ç´€', 'ç´«', 'è€€', 'è€…', 'è‚…', 'èƒ', 'èƒ¡', 'è‡º', 'è¯', 'è—', 'è£', 'è¥¿', 'è¦', 'è©±', 'èª˜', 'èªª', 'è«¾', 'è²', 'è²·', 'è³­', 'è³´', 'è¶™', 'èº', 'èº«', 'è¿™', 'è¿«', 'éŠ', 'é”', 'é‚£', 'é‚¦', 'éƒ½', 'é„‰', 'é®', 'é–‹', 'é–“', 'é˜²', 'é™½', 'é¡Œ', 'é¦™', 'é©—', 'é«”', 'é¬¥', 'é­', 'é®®', 'ê¸¸', 'ê¹€', 'ë‘ ', 'ì•„', 'ì˜¤', 'ì˜', 'ìˆ', 'ì¬', 'í• ', 'ï¼š', 'ï½‰', 'ğ”', 'ğ”¢', 'ğ”¥', 'ğŸ‡¯', 'ğŸ‡³', 'ğŸ‡µ', 'ğŸŒ¸', 'ğŸŒ¿', 'ğŸ„', 'ğŸ‹', 'ğŸ¿', 'ğŸ¤', 'ğŸ¦', 'ğŸ¸', 'ğŸ‘†', 'ğŸ‘Š', 'ğŸ“', 'ğŸ–', 'ğŸ—£', 'ğŸ˜£', 'ğŸ˜²', 'ğŸ˜¶', 'ğŸ¤“', 'ğŸ¤˜', 'ğŸ¤›', 'ğŸ¤œ', 'ğŸ¤', '\\U0001f92a', 'ğŸ¥‡', 'ğŸ¥‘', '\\U0001f9d0', '\\U0001f9e0', '\\x8f', '\\x9c', 'Â¨', 'Ã…', 'Ãˆ', 'Ã˜', 'Ãš', 'Ã·', 'Ã¾', 'Å„', 'Å', 'Å¼', 'Æ°', 'È›', 'É’', 'É”', 'É´', 'Ê', 'Ê”', 'Ê•', 'Ê°', 'Ë˜', 'Î“', 'Îœ', 'Î', 'ÎŸ', 'Î¡', 'Î¸', 'Î¹', 'Î¿', 'Ï†', 'Ï‰', 'Ğ­', 'ÑŠ', 'ØŸ', 'Ù', 'Ù ', 'Û°', 'Ûµ', 'à¤ˆ', 'à¤¡', 'à¤¤', 'à¤¦', 'à¤®', 'à¤¯', 'à¤²', 'à¨ˆ', 'à¨—', 'à¨®', 'à¨µ', 'à¨¿', 'à©€', 'à²¥', 'à¸—', 'à¸¡', 'à¸­', 'à¸µ', 'áƒ‘', 'áƒ’', 'áƒ¥', 'áƒ§', 'áƒ©', 'áƒª', 'áƒ«', 'á¶ ', 'á»…', 'á»‡', 'á»', 'á»›', 'á»­', '\\u200c', 'â€³', 'â€¿', 'â„ƒ', 'âŠ™', 'âŒ’', 'â”', 'â–”', 'â–¡', 'â–¥', 'â–¦', 'â–½', 'â—„', 'â˜', 'â˜‰', 'â˜¼', 'â™¡', 'â™¬', 'âš†', 'âš¡', 'âš«', 'â›½', 'âœ§', 'â—', 'â “', 'â ', 'â «', 'â °', 'â ·', 'â ¾', 'â¡', 'â¡†', 'â¡›', 'â¡œ', 'â¡·', 'â¢£', 'â¢§', 'â£•', 'â£˜', 'â£¥', 'â£§', 'â£¹', 'â£»', 'ãˆ', 'ã–', 'ãš', 'ã›', 'ã', 'ã³', 'ã¹', 'ã‚ƒ', 'ã‚ª', 'ã‚®', 'ã‚¸', 'ãƒ†', 'ãƒˆ', 'ãƒ‹', 'ãƒ”', 'ãƒ•', 'ãƒ', 'ãƒ¡', 'ãƒ¥', 'ãƒ§', 'ã„', 'ã„’', 'ã……', 'ä¸ˆ', 'ä¸–', 'ä¸º', 'ä»¥', 'ä»²', 'ä»·', 'ä¼', 'ä¼š', 'ä½“', 'ä½ ', 'ä½¿', 'ä¾†', 'å€‹', 'å‚·', 'å†…', 'å†™', 'åˆ†', 'åˆ¥', 'åˆ°', 'åŒš', 'å—', 'å»', 'å‘', 'å–', 'å§', 'å›', 'å ´', 'å¤–', 'å¤ª', 'å¦‚', 'å®š', 'å¯¹', 'å°‡', 'å°', 'å°‘', 'å±±', 'å¸', 'å½±', 'æ€ª', 'æ€»', 'æ­', 'æƒ…', 'æ„›', 'æˆ¦', 'æˆ¿', 'æ‰€', 'æ‰“', 'æ‰°', 'æ¥', 'æ”¾', 'æ•°', 'æ–¯', 'æ˜', 'æœ', 'æ§˜', 'æ¨¡', 'æ¨£', 'æ¬º', 'æ­¢', 'æ­£', 'æ¯”', 'æ°´', 'æ±‰', 'ç«', 'çˆ±', 'ç‰ˆ', 'çŠ¬', 'ç¾', 'çƒ', 'ç”²', 'ç”·', 'ç•Œ', 'çš‡', 'ç¡¬', 'ç¢º', 'ç§‘', 'çª', 'çª“', 'çµ¡', 'ç½ª', 'ç½²', 'è€Œ', 'è¯', 'è½', 'è‰', 'è¡¨', 'è¢«', 'è§€', 'è¬', 'è­¦', 'è®š', 'è®©', 'è¯­', 'èµ·', 'è¶Š', 'é€™', 'é€£', 'éƒ¨', 'é‡‹', 'é‡', 'é–¢', 'éšœ', 'é›–', 'éŸ“', 'é ‚', 'é ˜', 'é ­', 'é»', 'ê±°', 'êµ', 'ê¶Œ', 'ê¸°', 'ë‚˜', 'ë–¡', 'ë¼', 'ë¡œ', 'ë¦¬', 'ë§ˆ', 'ë§Œ', 'ë°”', 'ë¹›', 'ì„¸', 'ì‹', 'ì—†', 'ì˜', 'ì„', 'ì „', 'ì¢‹', '\\uf0b7', 'ï¸µ', 'ï¼ˆ', 'ï½', 'ï½', 'ï½”', 'ï½¥', 'ğ”©', 'ğ”«', 'ğ”°', 'ğ”±', 'ğ˜¼', 'ğŸ€„', 'ğŸŒ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ±', 'ğŸŒ´', 'ğŸ€', 'ğŸ', 'ğŸ…', 'ğŸŠ', 'ğŸŒ', 'ğŸ¾', 'ğŸ©', 'ğŸ¯', 'ğŸ…', 'ğŸ®', 'ğŸ„', 'ğŸ•', 'ğŸ–', 'ğŸ¢', 'ğŸ£', 'ğŸ·', 'ğŸ‘¦', 'ğŸ‘­', 'ğŸ‘´', 'ğŸ’‰', 'ğŸ’Š', 'ğŸ’¥', 'ğŸ’²', 'ğŸ’µ', 'ğŸ“…', 'ğŸ“²', 'ğŸ”ª', 'ğŸ•´', 'ğŸ–‘', 'ğŸ˜›', 'ğŸ˜ ', 'ğŸ˜¦', 'ğŸ˜µ', 'ğŸ™', 'ğŸ¤', '\\U0001f92b', '\\U0001f92c', '\\U0001f92d', 'ğŸ¥', 'ğŸ¥”', 'ğŸ¦', '\\x14', 'Ã€', 'Ã‚', 'Ã†', 'Ã', 'Ã', 'Ã™', 'Ã¬', 'Ã»', 'Äƒ', 'Ä', 'Ä', 'Ä¹', 'Äº', 'Å›', 'Ç', 'Ç', 'É¡', 'É¢', 'É¨', 'É¶', 'Ê‚', 'ÊŠ', 'Ê', 'Ê', 'Ê’', 'Ê', 'ÊŸ', 'Ë€', 'Ë™', 'Ë¢', 'Î’', 'Î•', 'Î—', 'Îš', 'Î ', 'Î¥', 'Î¦', 'Î±', 'Î³', 'Î»', 'Ï', 'Ï‚', 'Ïƒ', 'Ğ‡', 'Ğ–', 'Ğ¦', 'Ğ§', 'Ğ«', 'Ñ–', 'Ù£', 'Ú¡', 'Û•', 'à¤‚', 'à¤•', 'à¤›', 'à¤œ', 'à¤Ÿ', 'à¤¬', 'à¥€', 'à¥', 'à¥‹', 'à¦¦', 'à¦§', 'à¦¨', 'à¦¬', 'à¦¯', 'à¦¾', 'à§', 'à¨‚', 'à¨‰', 'à¨š', 'à¨œ', 'à¨', 'à¨¡', 'à¨£', 'à¨¤', 'à¨¬', 'à¨²', 'à¨¹', 'à©‚', 'à©ˆ', 'à©œ', 'à©°', 'à¸‚', 'à¸„', 'à¸“', 'à¸™', 'à¸š', 'à¸›', 'à¸£', 'à¸¥', 'à¸¨', 'à¸«', 'à¸°', 'à¸¸', 'à¹„', 'à¹‰', 'áƒ“', 'áƒ™', 'áƒ', 'áƒ¢', 'á´€', 'á´‰', 'á´', 'á´˜', 'á´¡', 'á´¥', 'á´°', 'á´´', 'áµƒ', 'áµˆ', 'áµ˜', 'á¶œ', 'á¸¥', 'á¸«', 'á¹£', 'áºµ', 'áº·', 'á»¨', 'â€²', 'â¿', 'â„‘', 'â…”', 'â…', 'â†‘', 'â†’', 'â†¼', 'âˆ‡', 'âˆš', 'âˆ', 'â‰¤', 'âŠ', 'âŠ', 'âŠ±', 'â‹¯', 'â–ƒ', 'â–¾', 'â—‹', 'â—', 'â—Ÿ', 'â˜¯', 'âš–', 'âš™', 'âš ', 'âšª', 'âœ', 'âœ', 'âœ¿', 'â', 'â•', 'â ”', 'â  ', 'â º', 'â¡‰', 'â¡´', 'â¢‰', 'â£ˆ', 'â£¡', 'â¬‡', 'â¬œ', 'ã€', 'ã€', 'ã€œ', 'ã’', 'ã”', 'ã©', 'ã¬', 'ã¸', 'ã‚', 'ã‚‡', 'ã‚', 'ã‚£', 'ã‚©', 'ã‚«', 'ã‚¬', 'ã‚±', 'ãƒ‡', 'ãƒ', 'ãƒ', 'ãƒ˜', 'ãƒ™', 'ãƒ¢', 'ãƒ¾', 'ã„Ÿ', 'ä¸‰', 'ä¸“', 'ä¸”', 'ä¸¦', 'ä¸¨', 'ä¸ª', 'ä¸²', 'ä¹', 'ä¹‚', 'ä¹‡', 'ä¹‰', 'ä¹', 'ä¹ ', 'ä¹¦', 'äºŒ', 'äº’', 'äº', 'äº¡', 'ä»Š', 'ä»', 'ä»•', 'ä»˜', 'ä»¬', 'ä½', 'ä½•', 'ä½³', 'ä¿„', 'ä¿', 'å€¼', 'å‡', 'å', 'åš', 'åœ', 'å´', 'å·', 'å‚²', 'åƒ', 'åƒ¹', 'å„’', 'å…ˆ', 'å…¥', 'å…¬', 'å…´', 'å…½', 'å†‡', 'å†²', 'å‰›', 'å‰²', 'åŠ©', 'å‹™', 'åŒº', 'åŒ»', 'å‚', 'å„', 'å', 'å‰', 'åŸ', 'åƒ', 'åˆ', 'åŠ', 'å—', 'å˜', 'åª', 'å²', 'å·', 'å‰', 'å‘', 'å›', 'å‘€', 'å“ˆ', 'å“¡', 'å“¦', 'å”±', 'å—', 'å› ', 'åœˆ', 'å‘', 'å¦', 'å’', 'åŸ', 'åŸŸ', 'åŸº', 'å£', 'å¤«', 'å¤´', 'å¥‹', 'å¥½', 'å¦–', 'å­˜', 'å­¸', 'å®—', 'å®¢', 'å¯„', 'å¯Œ', 'å¯Ÿ', 'å¯©', 'å¯¼', 'å°‚', 'å°”', 'å°š', 'å°º', 'å°¼', 'å±‚', 'å±‹', 'å±¤', 'å·®', 'å·±', 'å¸Œ', 'å¸­', 'å¸¶', 'å¹´', 'åºœ', 'å¼', 'å¼µ', 'å½“', 'å¾€', 'å¾…', 'å¾‹', 'å¾“', 'å¾—', 'å¿…', 'å¿˜', 'æ€', 'æ€•', 'æ€–', 'æ€¥', 'æ‹', 'æ', 'æ‚¨', 'æ‚²', 'æ„Ÿ', 'æ…‹', 'æ…¢', 'æ…£', 'æ†²', 'æ‡‰', 'æˆ', 'æˆ', 'æ‰‰', 'æ‹‹', 'æŒ', 'æŒ‡', 'æ‘§', 'æ’¤', 'æ“', 'æ•‘', 'æ•¸', 'æ–­', 'æ——', 'æ˜Œ', 'æ™®', 'æš«', 'æœ›', 'æœ¨', 'æœª', 'æ¥', 'æ—', 'æ¡¿', 'æ¤', 'æ¤¿', 'æ¥­', 'æ§', 'æ¨‚', 'æ­¡', 'æ­§', 'æ­·', 'æ®‹', 'æ¯', 'æ¯', 'æ¯•', 'æ°”', 'æ±‚', 'æ±º', 'æ³£', 'æ´‹', 'æ´¥', 'æ´²', 'æµ', 'æ··', 'æ¸¡', 'ç‚¹', 'çƒ¦', 'çƒ­', 'ç„š', 'ç„¼', 'ç…¤', 'ç†Š', 'ç‰‡', 'çŠ¶', 'ç‹', 'ç…', 'ç²', 'ç„', 'ç©', 'ç°', 'ç»', 'ç’ƒ', 'ç”°', 'ç”»', 'ç™¼', 'ç™½', 'ç™¾', 'ç›Ÿ', 'ç›®', 'ç›´', 'ç›¸', 'ç¤¼', 'ç¤¾', 'ç¥–', 'ç¥', 'ç¦', 'ç¨‹', 'ç©´', 'ç©º', 'ç«Ÿ', 'ç¬‘', 'ç¬¬', 'ç­”', 'ç®¡', 'ç²¾', 'ç³•', 'ç³Ÿ', 'ç³§', 'ç´°', 'çµ', 'ç·š', 'ç·©', 'ç¹°', 'ç»‡', 'ç»', 'ç»œ', 'ç»', 'ç»§', 'ç»­', 'ç»´', 'ç½—', 'ç½¢', 'ç¾©', 'ç¿’', 'ç¿»', 'è€—', 'è', 'è‚º', 'è‚¿', 'èƒ', 'è„‘', 'èˆ‡', 'èˆ¬', 'èˆ¹', 'è‰¯', 'è‰¾', 'è‹±', 'è¬', 'è‘—', 'è’™', 'è“†', 'è˜‡', 'è¡›', 'è¡', 'è£¡', 'è§’', 'è¨ˆ', 'è¨', 'è¨¼', 'è©³', 'èª¬', 'è¬›', 'è­œ', 'è®¿', 'è²', 'è³', 'è´µ', 'èµ¤', 'è¶…', 'è»Š', 'è»', 'è»¢', 'è¼‰', 'è¿', 'è¿‘', 'è¿”', 'è¿œ', 'è¿', 'é€Ÿ', 'é•', 'é ', 'é¿', 'é„­', 'é…’', 'é…µ', 'é…¸', 'é‡Œ', 'é‡', 'é‡‘', 'éŒ¯', 'éµ', 'é˜', 'é•¿', 'é–‰', 'é—®', 'é˜‚', 'é˜´', 'é˜¿', 'é™£', 'éšŠ', 'éš”', 'é›¢', 'éœ‡', 'éŸ©', 'é ‘', 'é¡”', 'é¡»', 'é¢˜', 'é¢¨', 'é£›', 'é¨’', 'é©•', 'éº¼', 'é»’', 'é¾', 'ê°€', 'ê°±', 'ê±¸', 'ê²Œ', 'ê³ ', 'ê³µ', 'ê³¼', 'êµ¿', 'ê·¼', 'êº³', 'ëƒ…', 'ë…', 'ë…„', 'ë…•', 'ë‹¹', 'ë”', 'ë“ ', 'ëœ¨', 'ë‘', 'ë ¥', 'ë§‰', 'ë§', 'ë§™', 'ë§¥', 'ëª¨', 'ëª©', 'ë¬´', 'ë°•', 'ë°¥', 'ë²„', 'ë²•', 'ë³¶', 'ë¶€', 'ë¶ˆ', 'ë¹„', 'ë¹”', 'ë¹¨', 'ì‚¬', 'ìƒ', 'ì„œ', 'ì„¤', 'ì†Œ', 'ì‹ ', 'ì‹¸', 'ì•ˆ', 'ì–¼', 'ì—„', 'ì—…', 'ì—ˆ', 'ì—¬', 'ì˜¨', 'ì™¸', 'ìœ¡', 'ìœ¼', 'ì¸', 'ì…', 'ì‡', 'ìŸ', 'ì €', 'ì¡´', 'ì§„', 'ì§œ', 'ì¹œ', 'í¼', 'í„°', 'íŠ¸', 'í‹°', 'íŒŒ', 'í•´', 'í—ˆ', 'í—', 'í˜„', 'í˜œ', 'íšŒ', 'í›„', 'í¬', '\\uf0e0', 'ï¼‘', 'ï¼›', 'ï¼ ', 'ï¼°', 'ï¼´', 'ï¼·', 'ï½€', 'ï½', 'ï½ƒ', 'ï½’', 'ï¾›', 'ï¿¦', 'ğ•', 'ğš', 'ğ¢', 'ğ¯', 'ğ” ', 'ğ”£', 'ğ”¦', 'ğ”¬', 'ğ”­', 'ğ”²', 'ğ”³', 'ğ”´', 'ğ”¶', 'ğ˜¾', 'ğ™€', 'ğ™„', 'ğ™ˆ', 'ğ™‰', 'ğ™', 'ğŸ¸', 'ğŸ…¿', 'ğŸ‡´', 'ğŸŒ™', 'ğŸŒ ', 'ğŸŒ¤', 'ğŸŒ®', 'ğŸŒ¶', 'ğŸŒ¼', 'ğŸ‡', 'ğŸ', 'ğŸ”', 'ğŸ', 'ğŸ©', 'ğŸ«', 'ğŸ¶', 'ğŸ·', 'ğŸ¼', 'ğŸ…', 'ğŸ¥', 'ğŸª', 'ğŸ¸', 'ğŸ›', 'ğŸ', 'ğŸµ', 'ğŸ€', 'ğŸ‚', 'ğŸ…', 'ğŸ‡', 'ğŸ', 'ğŸ“', 'ğŸ˜', 'ğŸ¡', 'ğŸ§', 'ğŸ¬', 'ğŸ»', 'ğŸ¼', 'ğŸ‘ƒ', 'ğŸ‘‡', 'ğŸ‘¬', 'ğŸ‘¶', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’”', 'ğŸ’—', 'ğŸ’', 'ğŸ’Ÿ', 'ğŸ’¡', 'ğŸ’§', 'ğŸ’»', 'ğŸ’¿', 'ğŸ“‰', 'ğŸ“œ', 'ğŸ“', 'ğŸ“°', 'ğŸ“´', 'ğŸ”Š', 'ğŸ”š', 'ğŸ”›', 'ğŸ”', 'ğŸ•µ', 'ğŸ•¸', 'ğŸ––', 'ğŸ–¤', 'ğŸ—', 'ğŸ—³', 'ğŸ˜‡', 'ğŸ˜™', 'ğŸ˜', 'ğŸ˜ª', 'ğŸ˜´', 'ğŸ˜¼', 'ğŸ˜¾', 'ğŸ™€', 'ğŸ™‡', 'ğŸš€', 'ğŸšŒ', 'ğŸš‘', 'ğŸš“', 'ğŸš”', 'ğŸš¢', 'ğŸš«', '\\U0001f6f7', 'ğŸ¤', 'ğŸ¤ ', '\\U0001f929', '\\U0001f92f', '\\U0001f932', 'ğŸ¥•', 'ğŸ¥', '\\U0001f967', '\\U0001f970', '\\U0001f974', 'ğŸ¦ƒ', 'ğŸ¦„', 'ğŸ¦Š', 'ğŸ¦', '\\U0001f992', '\\U0001f9b9', '\\U0001f9d8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnH0XefaGkke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = torchtext.data.BucketIterator(train,\n",
        "                                           batch_size=1,\n",
        "                                           sort_key=lambda x: len(x.context), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False,\n",
        "                                          )                  # repeat the iterator for many epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeFntm9UbKB0",
        "colab_type": "code",
        "outputId": "a321d31a-e707-41a2-c86a-236cf73e5d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train_iter)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZVdxcMzHD8i",
        "colab_type": "code",
        "outputId": "88419892-68b8-4529-a233-242b26657497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "k = 0;\n",
        "for batch in train_iter:\n",
        "    \n",
        "    #max_length_seq = batch.context[0].shape[1];\n",
        "    #print(\"-----Batch #\", k+1, \"-----\");\n",
        "    #print(\"Maximum length of the input sequence : \", max_length_seq);\n",
        "    #padding_per_batch = int(torch.sum(max_length_seq - batch.sms[1]))\n",
        "    #print(\"Number of padding for batch : \", padding_per_batch)\n",
        "    \n",
        "    print(\"Batch Size : \", len(batch))\n",
        "    #print(\"Batch Context : \", batch.context)\n",
        "    print(\"Batch Context Shape: \", batch.context[0].shape)\n",
        "    #print(batch.reply)\n",
        "    print(\"Batch Context Shape: \", batch.reply.shape)\n",
        "    k += 1;\n",
        "    \n",
        "    if(k >= 1):\n",
        "        break;"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size :  1\n",
            "Batch Context Shape:  torch.Size([1, 1067])\n",
            "Batch Context Shape:  torch.Size([1, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xQ-kcu4YD6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_to_string(token, context_field, reply_field):\n",
        "    context_array =[];\n",
        "    for i in range(0, token.context[0].shape[1]):\n",
        "        context_array.append(context_field.vocab.itos[(token.context[0][0][i])])\n",
        "\n",
        "    context_str = ''.join(context_array)\n",
        "\n",
        "\n",
        "\n",
        "    reply_array =[];\n",
        "    for i in range(0, token.reply.shape[1]):\n",
        "        reply_array.append(reply_field.vocab.itos[(token.reply[0][i])])\n",
        "\n",
        "    reply_str = ''.join(reply_array)\n",
        "\n",
        "    print(\"Context : \", context_str);\n",
        "    print(\"Reply : \", reply_str);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eMIEJme6VD",
        "colab_type": "code",
        "outputId": "e84596e3-cff0-4af3-e532-0fd433ceef15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "embedding_to_string(batch, text_field, label_field)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context :  <BOS>providence mistype repudiation postdates offenders bankrolls echoed Tutankhamen keynotes furloughed unacceptably entrenched diced cradle nutritionists doodlers abroad Vonda soliloquized garnisheed murderesss biennial reawakening zenith crappy Tartary Goren appendicitis scrods style basilica boozy captivated gored dubietys mannishly drill midwife machinations inventiveness bellyaches Marquez bys fetch joggling emaciating hurtled varnished awarded keypunch chicory taught showers freeloaders cervix dailies rearwards descendant Duran boastfulnesss bowsprits Bulfinch timekeepers shaved imposed underwear ejected putts Antichrists stalkings Shinto superegos newsboy Blanche eights bludgeon perfections watermark nab pigpen slipping cosmoses sunbathings chauvinist reign cubicles watched paunchiest youngish subtotals minuscule stripper stateroom campsites decapitates monkeys tallnesss cosmopolitans welled easterlies Snell enslave gazettes ogled peripherys Pickwicks straggler record airlifted suspensions Windsors pipping deadwood screwdriver intercepting nadirs<EOS>\n",
            "Reply :  <BOS>Frothing.<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCv9loPeVGKZ",
        "colab_type": "code",
        "outputId": "0874d434-c56b-45ba-91e3-9099a7b536aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Vocab Size:  1277\n",
            "Reply Vocab size:  2232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Kkzlo9ijXY",
        "colab_type": "code",
        "outputId": "c59e2ef4-0d97-4226-ed10-0f7d04765c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(batch.context[0][0])\n",
        "print(batch.reply)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2, 19, 12,  ..., 12, 11,  3])\n",
            "tensor([[ 2, 59, 12,  8,  6, 13,  9, 10, 20, 24,  3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10hkqBrfE6Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaZEAioohThl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2WbsjshJnY",
        "colab_type": "code",
        "outputId": "4e66726e-b0b9-48a0-99c1-c12e59ab9e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#>>> BEGIN: Encoding Region\n",
        "\n",
        "context_ident = torch.eye(input_vocab_size)\n",
        "print(\"Shape 1 : \", batch.context[0].shape) # (batch size, sequence size)\n",
        "context_tensor = context_ident[batch.context[0]]\n",
        "\n",
        "print(\"Shape 2 : \", context_tensor.shape) # (batch size, sequence size, one hot embedding per size)\n",
        "encode_rnn = nn.GRU(input_vocab_size, 100, 1, batch_first=True)\n",
        "h0 = torch.zeros(1, batch.context[0].shape[0], 100); # (num layers * direction, batch size, hidden size)\n",
        "out, last_hidden = encode_rnn(context_tensor, h0)\n",
        "print(\"Shape 3 : \", out.shape)\n",
        "print(\"Shape 4 : \", last_hidden.shape)\n",
        "\n",
        "#<<< END: Encoding Region\n",
        "\n",
        "#>>> BEGIN: Teach forcing Generation Region\n",
        "reply_ident = torch.eye(reply_vocab_size)\n",
        "print(\"Shape 5 : \", batch.reply.shape)\n",
        "reply_tensor = reply_ident[batch.reply]\n",
        "\n",
        "print(\"Shape 6 : \", reply_tensor.shape)\n",
        "decode_rnn = nn.GRU(reply_vocab_size, 100, 1, batch_first=True)\n",
        "out2, last_hidden2 = decode_rnn(reply_tensor[:,:-1,:], last_hidden) # Don't pass in <EOS> token\n",
        "print(\"Shape 7 : \", out2.shape)\n",
        "print(\"Shape 8 : \", last_hidden2.shape)\n",
        "\n",
        "target = reply_tensor[:,1:,:]\n",
        "print(\"Shape 9 : \", target.shape)\n",
        "\n",
        "fcnn = nn.Linear(100, reply_vocab_size)\n",
        "\n",
        "final_destination = fcnn(out2)\n",
        "\n",
        "print(\"Shape 10 : \", final_destination.shape)\n",
        "\n",
        "\n",
        "temp1 = final_destination.reshape(-1, reply_vocab_size);\n",
        "temp2 = batch.reply[:,1:].reshape(-1)\n",
        "\n",
        "print(\"Shape 11 : \", temp1.shape)\n",
        "print(\"Shape 12 : \", temp2.shape)\n",
        "\n",
        "loss = criterion(temp1, temp2)\n",
        "print(\"Loss : \", loss.item())\n",
        "\n",
        "#<<< END: Teach forcing Generation Region\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape 1 :  torch.Size([1, 1067])\n",
            "Shape 2 :  torch.Size([1, 1067, 1277])\n",
            "Shape 3 :  torch.Size([1, 1067, 100])\n",
            "Shape 4 :  torch.Size([1, 1, 100])\n",
            "Shape 5 :  torch.Size([1, 11])\n",
            "Shape 6 :  torch.Size([1, 11, 2232])\n",
            "Shape 7 :  torch.Size([1, 10, 100])\n",
            "Shape 8 :  torch.Size([1, 1, 100])\n",
            "Shape 9 :  torch.Size([1, 10, 2232])\n",
            "Shape 10 :  torch.Size([1, 10, 2232])\n",
            "Shape 11 :  torch.Size([10, 2232])\n",
            "Shape 12 :  torch.Size([10])\n",
            "Loss :  7.7212066650390625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt4d75QfhaBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size,  \n",
        "                 reply_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ChatBot, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers =generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.LSTM(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Generator\n",
        "        self.reply_ident = torch.eye(reply_vocab_size)\n",
        "        self.decode_rnn = nn.LSTM(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, reply_vocab_size)\n",
        "        \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        h0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size); # (num layers * direction, batch size, hidden size)\n",
        "        c0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size);\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor, (h0,c0))\n",
        "        \n",
        "        # >>> Generator\n",
        "        reply_tensor = self.reply_ident[response] #Type: batch.reply\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkS9nq_Y9ou4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ChatBot(input_vocab_size, reply_vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KJOIBMC90hz",
        "colab_type": "code",
        "outputId": "8e6c386b-3e46-4899-eaf6-b697d6fb516c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(0,100):\n",
        "    optimizer.zero_grad();\n",
        "    out, hidden = model(batch.context[0], batch.reply[:, :-1]) # no eos\n",
        "    #print(batch.reply[:, :-1])\n",
        "    out_reshaped = out.reshape(-1,reply_vocab_size)\n",
        "    target = (batch.reply[:,1:]).reshape(-1) # no bos\n",
        "    #print(batch.reply[:,1:])\n",
        "    loss = criterion(out_reshaped, target)\n",
        "    print(loss)\n",
        "    loss.backward();\n",
        "    optimizer.step();"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.7210, grad_fn=<NllLossBackward>)\n",
            "tensor(7.5697, grad_fn=<NllLossBackward>)\n",
            "tensor(7.2976, grad_fn=<NllLossBackward>)\n",
            "tensor(6.0054, grad_fn=<NllLossBackward>)\n",
            "tensor(4.0020, grad_fn=<NllLossBackward>)\n",
            "tensor(2.6061, grad_fn=<NllLossBackward>)\n",
            "tensor(2.0337, grad_fn=<NllLossBackward>)\n",
            "tensor(1.8325, grad_fn=<NllLossBackward>)\n",
            "tensor(1.6938, grad_fn=<NllLossBackward>)\n",
            "tensor(1.6092, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5348, grad_fn=<NllLossBackward>)\n",
            "tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3814, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3395, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2764, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2310, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1848, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1503, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0925, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0589, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7539, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5353, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3834, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3699, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3568, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3436, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2982, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2426, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2345, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1608, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1492, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1066, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0649, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0526, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCrN6skeKpdw",
        "colab_type": "code",
        "outputId": "52bb9a30-9fe4-44e4-82d3-399fe6c66a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "label_field.vocab.stoi[\"<BOS>\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py2TZslcKcFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    inp = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None;\n",
        "    for p in range(max_len):\n",
        "        #print(inp)\n",
        "        output, hidden = model(batch.context[0], inp.unsqueeze(0), hidden)\n",
        "        #print(output)\n",
        "        #output = F.softmax(output, dim=2)\n",
        "        #print(torch.argmax(output, dim=2))\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        #print(output_dist)\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = label_field.vocab.itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPcQXnBVLWmh",
        "colab_type": "code",
        "outputId": "46bccf30-fb3b-410b-e3b3-80f624ed28c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "sample_sequence(model, temperature=1.0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Frothinn.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXlPwVbUdfmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = torch.Tensor([label_field.vocab.stoi[\"<BOS>\"]]).long()\n",
        "print(inp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGAUaJwfFW6",
        "colab_type": "text"
      },
      "source": [
        "#Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh3e2nfMsSro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContextAE(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ContextAE, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers = generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.LSTM(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Decoder\n",
        "        self.reply_ident = torch.eye(context_vocab_size)\n",
        "        self.decode_rnn = nn.LSTM(context_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, context_vocab_size)\n",
        "        \n",
        "    def forward(self, context, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        h0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size); # (num layers * direction, batch size, hidden size)\n",
        "        c0 = torch.zeros(self.encoder_layers, context.shape[0], self.encoder_hidden_size);\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor, (h0,c0))\n",
        "        \n",
        "        # >>> Decoder\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(context_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(context_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Aqu-yFwu9Au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8ff59430-8be2-45e8-c45c-93c36343014c"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuCr641-jIMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = torchtext.data.BucketIterator(train,\n",
        "                                           batch_size=16,\n",
        "                                           sort_key=lambda x: len(x.context), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False,\n",
        "                                          )                  # repeat the iterator for many epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSGhBvWnx3cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_AE = ContextAE(input_vocab_size, encoder_hidden_size = 200, generator_hidden_size = 200 )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_AE = torch.optim.Adam(model_AE.parameters(), lr = 0.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnW4UVz6xyT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55b3be65-7182-43ed-cc8f-34aa0b204c1e"
      },
      "source": [
        "for batch in train_iter:\n",
        "    \n",
        "    #max_length_seq = batch.context[0].shape[1];\n",
        "    #print(\"-----Batch #\", k+1, \"-----\");\n",
        "    #print(\"Maximum length of the input sequence : \", max_length_seq);\n",
        "    #padding_per_batch = int(torch.sum(max_length_seq - batch.sms[1]))\n",
        "    #print(\"Number of padding for batch : \", padding_per_batch)\n",
        "    \n",
        "    print(\"Batch Size : \", len(batch))\n",
        "    #print(\"Batch Context : \", batch.context)\n",
        "    print(\"Batch Context Shape: \", batch.context[0].shape)\n",
        "    #print(batch.reply)\n",
        "    print(\"Batch Context Shape: \", batch.reply.shape)\n",
        "    \n",
        "    to_encode = batch.context[0];\n",
        "    \n",
        "    optimizer_AE.zero_grad();\n",
        "    out, hidden = model_AE(to_encode[:,1:]) # no eos\n",
        "    #print(batch.reply[:, :-1])\n",
        "    out_reshaped = out.reshape(-1,input_vocab_size)\n",
        "    target = (to_encode[:,1:]).reshape(-1) # no bos\n",
        "    #print(batch.reply[:,1:])\n",
        "    loss = criterion(out_reshaped, target)\n",
        "    print(loss)\n",
        "    loss.backward();\n",
        "    optimizer_AE.step();"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 123])\n",
            "Batch Context Shape:  torch.Size([16, 832])\n",
            "tensor(7.1478, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 144])\n",
            "Batch Context Shape:  torch.Size([16, 1716])\n",
            "tensor(6.9274, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 635])\n",
            "Batch Context Shape:  torch.Size([16, 1097])\n",
            "tensor(4.3070, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 9849])\n",
            "Batch Context Shape:  torch.Size([16, 1472])\n",
            "tensor(3.2265, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 434])\n",
            "Batch Context Shape:  torch.Size([16, 1310])\n",
            "tensor(3.2517, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 201])\n",
            "Batch Context Shape:  torch.Size([16, 352])\n",
            "tensor(3.4143, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 81])\n",
            "Batch Context Shape:  torch.Size([16, 457])\n",
            "tensor(3.5591, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 21])\n",
            "Batch Context Shape:  torch.Size([16, 1283])\n",
            "tensor(3.8370, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 90])\n",
            "Batch Context Shape:  torch.Size([16, 307])\n",
            "tensor(3.3340, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 135])\n",
            "Batch Context Shape:  torch.Size([16, 418])\n",
            "tensor(3.2967, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 83])\n",
            "Batch Context Shape:  torch.Size([16, 662])\n",
            "tensor(3.2216, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 45])\n",
            "Batch Context Shape:  torch.Size([16, 1256])\n",
            "tensor(3.3864, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 164])\n",
            "Batch Context Shape:  torch.Size([16, 785])\n",
            "tensor(3.2324, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 47])\n",
            "Batch Context Shape:  torch.Size([16, 358])\n",
            "tensor(3.3514, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 119])\n",
            "Batch Context Shape:  torch.Size([16, 488])\n",
            "tensor(3.1365, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 377])\n",
            "Batch Context Shape:  torch.Size([16, 816])\n",
            "tensor(3.0962, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 98])\n",
            "Batch Context Shape:  torch.Size([16, 505])\n",
            "tensor(3.1806, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 32])\n",
            "Batch Context Shape:  torch.Size([16, 812])\n",
            "tensor(3.4280, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 53])\n",
            "Batch Context Shape:  torch.Size([16, 428])\n",
            "tensor(3.0048, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 16])\n",
            "Batch Context Shape:  torch.Size([16, 423])\n",
            "tensor(3.9376, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 358])\n",
            "Batch Context Shape:  torch.Size([16, 1857])\n",
            "tensor(3.0771, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 109])\n",
            "Batch Context Shape:  torch.Size([16, 476])\n",
            "tensor(3.1397, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 268])\n",
            "Batch Context Shape:  torch.Size([16, 2051])\n",
            "tensor(3.1459, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 275])\n",
            "Batch Context Shape:  torch.Size([16, 703])\n",
            "tensor(3.0326, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 103])\n",
            "Batch Context Shape:  torch.Size([16, 460])\n",
            "tensor(2.9694, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 25])\n",
            "Batch Context Shape:  torch.Size([16, 653])\n",
            "tensor(3.4852, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 60])\n",
            "Batch Context Shape:  torch.Size([16, 391])\n",
            "tensor(2.9818, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 129])\n",
            "Batch Context Shape:  torch.Size([16, 690])\n",
            "tensor(2.8618, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 171])\n",
            "Batch Context Shape:  torch.Size([16, 463])\n",
            "tensor(2.8595, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 216])\n",
            "Batch Context Shape:  torch.Size([16, 893])\n",
            "tensor(3.2148, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 997])\n",
            "Batch Context Shape:  torch.Size([16, 652])\n",
            "tensor(2.7752, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 156])\n",
            "Batch Context Shape:  torch.Size([16, 967])\n",
            "tensor(2.7385, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 196])\n",
            "Batch Context Shape:  torch.Size([16, 814])\n",
            "tensor(2.5274, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 147])\n",
            "Batch Context Shape:  torch.Size([16, 475])\n",
            "tensor(2.5391, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 105])\n",
            "Batch Context Shape:  torch.Size([16, 429])\n",
            "tensor(2.4885, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 244])\n",
            "Batch Context Shape:  torch.Size([16, 587])\n",
            "tensor(2.2996, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 72])\n",
            "Batch Context Shape:  torch.Size([16, 2876])\n",
            "tensor(2.3284, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 292])\n",
            "Batch Context Shape:  torch.Size([16, 875])\n",
            "tensor(2.2013, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 207])\n",
            "Batch Context Shape:  torch.Size([16, 549])\n",
            "tensor(2.0913, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 61])\n",
            "Batch Context Shape:  torch.Size([16, 691])\n",
            "tensor(2.2190, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 253])\n",
            "Batch Context Shape:  torch.Size([16, 295])\n",
            "tensor(2.0156, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 64])\n",
            "Batch Context Shape:  torch.Size([16, 509])\n",
            "tensor(1.9405, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 138])\n",
            "Batch Context Shape:  torch.Size([16, 1163])\n",
            "tensor(1.7728, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 74])\n",
            "Batch Context Shape:  torch.Size([16, 605])\n",
            "tensor(1.9154, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 62])\n",
            "Batch Context Shape:  torch.Size([16, 1628])\n",
            "tensor(1.7006, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 180])\n",
            "Batch Context Shape:  torch.Size([16, 1010])\n",
            "tensor(1.7297, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1355])\n",
            "Batch Context Shape:  torch.Size([16, 450])\n",
            "tensor(1.8246, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 828])\n",
            "Batch Context Shape:  torch.Size([16, 531])\n",
            "tensor(1.4046, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 88])\n",
            "Batch Context Shape:  torch.Size([16, 597])\n",
            "tensor(1.2277, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 191])\n",
            "Batch Context Shape:  torch.Size([16, 551])\n",
            "tensor(1.1276, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 29])\n",
            "Batch Context Shape:  torch.Size([16, 267])\n",
            "tensor(1.4667, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 108])\n",
            "Batch Context Shape:  torch.Size([16, 476])\n",
            "tensor(1.0802, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 116])\n",
            "Batch Context Shape:  torch.Size([16, 355])\n",
            "tensor(0.9386, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 304])\n",
            "Batch Context Shape:  torch.Size([16, 3361])\n",
            "tensor(0.9092, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 96])\n",
            "Batch Context Shape:  torch.Size([16, 1026])\n",
            "tensor(0.8621, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 99])\n",
            "Batch Context Shape:  torch.Size([16, 687])\n",
            "tensor(0.9470, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 570])\n",
            "Batch Context Shape:  torch.Size([16, 766])\n",
            "tensor(0.8123, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 285])\n",
            "Batch Context Shape:  torch.Size([16, 1066])\n",
            "tensor(0.6374, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 152])\n",
            "Batch Context Shape:  torch.Size([16, 1886])\n",
            "tensor(0.6015, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 55])\n",
            "Batch Context Shape:  torch.Size([16, 571])\n",
            "tensor(0.7506, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 140])\n",
            "Batch Context Shape:  torch.Size([16, 1573])\n",
            "tensor(0.5193, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 70])\n",
            "Batch Context Shape:  torch.Size([16, 216])\n",
            "tensor(0.6058, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 212])\n",
            "Batch Context Shape:  torch.Size([16, 1125])\n",
            "tensor(0.4271, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 126])\n",
            "Batch Context Shape:  torch.Size([16, 2793])\n",
            "tensor(0.4456, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 229])\n",
            "Batch Context Shape:  torch.Size([16, 702])\n",
            "tensor(0.3473, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 321])\n",
            "Batch Context Shape:  torch.Size([16, 503])\n",
            "tensor(0.3532, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 225])\n",
            "Batch Context Shape:  torch.Size([16, 1145])\n",
            "tensor(0.2842, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 87])\n",
            "Batch Context Shape:  torch.Size([16, 339])\n",
            "tensor(0.2739, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 160])\n",
            "Batch Context Shape:  torch.Size([16, 389])\n",
            "tensor(0.2252, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 67])\n",
            "Batch Context Shape:  torch.Size([16, 347])\n",
            "tensor(0.2906, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 238])\n",
            "Batch Context Shape:  torch.Size([16, 776])\n",
            "tensor(0.2237, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 65])\n",
            "Batch Context Shape:  torch.Size([16, 1538])\n",
            "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 58])\n",
            "Batch Context Shape:  torch.Size([16, 513])\n",
            "tensor(0.2413, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 78])\n",
            "Batch Context Shape:  torch.Size([16, 423])\n",
            "tensor(0.2488, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 231])\n",
            "Batch Context Shape:  torch.Size([16, 957])\n",
            "tensor(0.1780, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 76])\n",
            "Batch Context Shape:  torch.Size([16, 643])\n",
            "tensor(0.2229, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 186])\n",
            "Batch Context Shape:  torch.Size([16, 494])\n",
            "tensor(0.1941, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 91])\n",
            "Batch Context Shape:  torch.Size([16, 382])\n",
            "tensor(0.1860, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 176])\n",
            "Batch Context Shape:  torch.Size([16, 767])\n",
            "tensor(0.2082, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 111])\n",
            "Batch Context Shape:  torch.Size([16, 299])\n",
            "tensor(0.1764, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 260])\n",
            "Batch Context Shape:  torch.Size([16, 1371])\n",
            "tensor(0.1382, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 709])\n",
            "Batch Context Shape:  torch.Size([16, 1847])\n",
            "tensor(0.1544, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 338])\n",
            "Batch Context Shape:  torch.Size([16, 572])\n",
            "tensor(0.1568, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 94])\n",
            "Batch Context Shape:  torch.Size([16, 438])\n",
            "tensor(0.1658, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 35])\n",
            "Batch Context Shape:  torch.Size([16, 415])\n",
            "tensor(0.1664, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 404])\n",
            "Batch Context Shape:  torch.Size([16, 984])\n",
            "tensor(0.1100, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 113])\n",
            "Batch Context Shape:  torch.Size([16, 618])\n",
            "tensor(0.1290, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 38])\n",
            "Batch Context Shape:  torch.Size([16, 270])\n",
            "tensor(0.1332, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 465])\n",
            "Batch Context Shape:  torch.Size([16, 2066])\n",
            "tensor(0.0891, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 132])\n",
            "Batch Context Shape:  torch.Size([16, 688])\n",
            "tensor(0.1014, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 57])\n",
            "Batch Context Shape:  torch.Size([16, 590])\n",
            "tensor(0.1633, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 50])\n",
            "Batch Context Shape:  torch.Size([16, 1337])\n",
            "tensor(0.2632, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1834])\n",
            "Batch Context Shape:  torch.Size([16, 912])\n",
            "tensor(0.1132, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 71])\n",
            "Batch Context Shape:  torch.Size([16, 560])\n",
            "tensor(0.1300, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 44])\n",
            "Batch Context Shape:  torch.Size([16, 322])\n",
            "tensor(0.0798, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 79])\n",
            "Batch Context Shape:  torch.Size([16, 764])\n",
            "tensor(0.0845, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 515])\n",
            "Batch Context Shape:  torch.Size([16, 1794])\n",
            "tensor(0.0671, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 489])\n",
            "Batch Context Shape:  torch.Size([16, 1743])\n",
            "tensor(0.0664, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 41])\n",
            "Batch Context Shape:  torch.Size([16, 511])\n",
            "tensor(0.0744, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 85])\n",
            "Batch Context Shape:  torch.Size([16, 337])\n",
            "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 123])\n",
            "Batch Context Shape:  torch.Size([16, 1297])\n",
            "tensor(0.0418, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 73])\n",
            "Batch Context Shape:  torch.Size([16, 433])\n",
            "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 107])\n",
            "Batch Context Shape:  torch.Size([16, 869])\n",
            "tensor(0.0679, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 115])\n",
            "Batch Context Shape:  torch.Size([16, 405])\n",
            "tensor(0.0635, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 64])\n",
            "Batch Context Shape:  torch.Size([16, 852])\n",
            "tensor(0.0562, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 263])\n",
            "Batch Context Shape:  torch.Size([16, 436])\n",
            "tensor(0.0351, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 99])\n",
            "Batch Context Shape:  torch.Size([16, 391])\n",
            "tensor(0.0432, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 104])\n",
            "Batch Context Shape:  torch.Size([16, 301])\n",
            "tensor(0.0479, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1259])\n",
            "Batch Context Shape:  torch.Size([16, 1157])\n",
            "tensor(0.0876, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 320])\n",
            "Batch Context Shape:  torch.Size([16, 675])\n",
            "tensor(0.0400, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 96])\n",
            "Batch Context Shape:  torch.Size([16, 926])\n",
            "tensor(0.0581, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 338])\n",
            "Batch Context Shape:  torch.Size([16, 718])\n",
            "tensor(0.0414, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 443])\n",
            "Batch Context Shape:  torch.Size([16, 347])\n",
            "tensor(0.0538, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 43])\n",
            "Batch Context Shape:  torch.Size([16, 1159])\n",
            "tensor(0.0537, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 422])\n",
            "Batch Context Shape:  torch.Size([16, 414])\n",
            "tensor(0.0490, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 201])\n",
            "Batch Context Shape:  torch.Size([16, 308])\n",
            "tensor(0.0381, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 226])\n",
            "Batch Context Shape:  torch.Size([16, 466])\n",
            "tensor(0.0470, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 807])\n",
            "Batch Context Shape:  torch.Size([16, 1226])\n",
            "tensor(0.0334, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 215])\n",
            "Batch Context Shape:  torch.Size([16, 533])\n",
            "tensor(0.0315, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 105])\n",
            "Batch Context Shape:  torch.Size([16, 499])\n",
            "tensor(0.0363, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 600])\n",
            "Batch Context Shape:  torch.Size([16, 896])\n",
            "tensor(0.0297, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 294])\n",
            "Batch Context Shape:  torch.Size([16, 415])\n",
            "tensor(0.0346, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 78])\n",
            "Batch Context Shape:  torch.Size([16, 545])\n",
            "tensor(0.0283, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 68])\n",
            "Batch Context Shape:  torch.Size([16, 9386])\n",
            "tensor(0.0354, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 90])\n",
            "Batch Context Shape:  torch.Size([16, 539])\n",
            "tensor(0.0367, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 31])\n",
            "Batch Context Shape:  torch.Size([16, 555])\n",
            "tensor(0.0292, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 155])\n",
            "Batch Context Shape:  torch.Size([16, 1772])\n",
            "tensor(0.0377, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 277])\n",
            "Batch Context Shape:  torch.Size([16, 893])\n",
            "tensor(0.0261, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 248])\n",
            "Batch Context Shape:  torch.Size([16, 968])\n",
            "tensor(0.0220, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 40])\n",
            "Batch Context Shape:  torch.Size([16, 464])\n",
            "tensor(0.0354, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 112])\n",
            "Batch Context Shape:  torch.Size([16, 2256])\n",
            "tensor(0.0214, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 165])\n",
            "Batch Context Shape:  torch.Size([16, 3069])\n",
            "tensor(0.0188, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 109])\n",
            "Batch Context Shape:  torch.Size([16, 821])\n",
            "tensor(0.0275, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 101])\n",
            "Batch Context Shape:  torch.Size([16, 387])\n",
            "tensor(0.0272, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 172])\n",
            "Batch Context Shape:  torch.Size([16, 566])\n",
            "tensor(0.0258, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 143])\n",
            "Batch Context Shape:  torch.Size([16, 427])\n",
            "tensor(0.0153, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 66])\n",
            "Batch Context Shape:  torch.Size([16, 424])\n",
            "tensor(0.0261, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 75])\n",
            "Batch Context Shape:  torch.Size([16, 1948])\n",
            "tensor(0.0316, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 473])\n",
            "Batch Context Shape:  torch.Size([16, 531])\n",
            "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 289])\n",
            "Batch Context Shape:  torch.Size([16, 859])\n",
            "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 127])\n",
            "Batch Context Shape:  torch.Size([16, 480])\n",
            "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 82])\n",
            "Batch Context Shape:  torch.Size([16, 259])\n",
            "tensor(0.0168, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 57])\n",
            "Batch Context Shape:  torch.Size([16, 433])\n",
            "tensor(0.0276, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 971])\n",
            "Batch Context Shape:  torch.Size([16, 1541])\n",
            "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 37])\n",
            "Batch Context Shape:  torch.Size([16, 584])\n",
            "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 72])\n",
            "Batch Context Shape:  torch.Size([16, 225])\n",
            "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 196])\n",
            "Batch Context Shape:  torch.Size([16, 1487])\n",
            "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 98])\n",
            "Batch Context Shape:  torch.Size([16, 391])\n",
            "tensor(0.0265, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 503])\n",
            "Batch Context Shape:  torch.Size([16, 1159])\n",
            "tensor(0.0211, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 130])\n",
            "Batch Context Shape:  torch.Size([16, 813])\n",
            "tensor(0.0073, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 261])\n",
            "Batch Context Shape:  torch.Size([16, 4131])\n",
            "tensor(0.0166, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 55])\n",
            "Batch Context Shape:  torch.Size([16, 829])\n",
            "tensor(0.0250, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 554])\n",
            "Batch Context Shape:  torch.Size([16, 699])\n",
            "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 397])\n",
            "Batch Context Shape:  torch.Size([16, 1095])\n",
            "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 25])\n",
            "Batch Context Shape:  torch.Size([16, 331])\n",
            "tensor(0.0330, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 88])\n",
            "Batch Context Shape:  torch.Size([16, 706])\n",
            "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 15])\n",
            "Batch Context Shape:  torch.Size([16, 100])\n",
            "tensor(0.0523, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 61])\n",
            "Batch Context Shape:  torch.Size([16, 810])\n",
            "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 51])\n",
            "Batch Context Shape:  torch.Size([16, 151])\n",
            "tensor(0.0509, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 193])\n",
            "Batch Context Shape:  torch.Size([16, 620])\n",
            "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 133])\n",
            "Batch Context Shape:  torch.Size([16, 545])\n",
            "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 269])\n",
            "Batch Context Shape:  torch.Size([16, 511])\n",
            "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 91])\n",
            "Batch Context Shape:  torch.Size([16, 1440])\n",
            "tensor(0.0235, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 160])\n",
            "Batch Context Shape:  torch.Size([16, 2006])\n",
            "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 254])\n",
            "Batch Context Shape:  torch.Size([16, 1107])\n",
            "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 45])\n",
            "Batch Context Shape:  torch.Size([16, 2790])\n",
            "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 84])\n",
            "Batch Context Shape:  torch.Size([16, 2459])\n",
            "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 86])\n",
            "Batch Context Shape:  torch.Size([16, 738])\n",
            "tensor(0.0073, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 695])\n",
            "Batch Context Shape:  torch.Size([16, 414])\n",
            "tensor(0.0223, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 80])\n",
            "Batch Context Shape:  torch.Size([16, 837])\n",
            "tensor(0.0165, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 185])\n",
            "Batch Context Shape:  torch.Size([16, 304])\n",
            "tensor(0.0240, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 181])\n",
            "Batch Context Shape:  torch.Size([16, 469])\n",
            "tensor(0.0158, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1744])\n",
            "Batch Context Shape:  torch.Size([16, 1432])\n",
            "tensor(0.0204, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 233])\n",
            "Batch Context Shape:  torch.Size([16, 1363])\n",
            "tensor(0.0157, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 77])\n",
            "Batch Context Shape:  torch.Size([16, 724])\n",
            "tensor(0.0088, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 95])\n",
            "Batch Context Shape:  torch.Size([16, 320])\n",
            "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 152])\n",
            "Batch Context Shape:  torch.Size([16, 460])\n",
            "tensor(0.0184, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 70])\n",
            "Batch Context Shape:  torch.Size([16, 657])\n",
            "tensor(0.0056, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 92])\n",
            "Batch Context Shape:  torch.Size([16, 712])\n",
            "tensor(0.0159, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 148])\n",
            "Batch Context Shape:  torch.Size([16, 658])\n",
            "tensor(0.0084, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 177])\n",
            "Batch Context Shape:  torch.Size([16, 335])\n",
            "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 6214])\n",
            "Batch Context Shape:  torch.Size([16, 1461])\n",
            "tensor(0.0219, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 48])\n",
            "Batch Context Shape:  torch.Size([16, 614])\n",
            "tensor(0.0060, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 139])\n",
            "Batch Context Shape:  torch.Size([16, 1874])\n",
            "tensor(0.0065, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 35])\n",
            "Batch Context Shape:  torch.Size([16, 594])\n",
            "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 20])\n",
            "Batch Context Shape:  torch.Size([16, 587])\n",
            "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 46])\n",
            "Batch Context Shape:  torch.Size([16, 649])\n",
            "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 59])\n",
            "Batch Context Shape:  torch.Size([16, 471])\n",
            "tensor(0.0453, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 136])\n",
            "Batch Context Shape:  torch.Size([16, 1560])\n",
            "tensor(0.0046, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 209])\n",
            "Batch Context Shape:  torch.Size([16, 1287])\n",
            "tensor(0.0070, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 110])\n",
            "Batch Context Shape:  torch.Size([16, 200])\n",
            "tensor(0.0045, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 299])\n",
            "Batch Context Shape:  torch.Size([16, 726])\n",
            "tensor(0.0078, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 361])\n",
            "Batch Context Shape:  torch.Size([16, 659])\n",
            "tensor(0.0074, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 81])\n",
            "Batch Context Shape:  torch.Size([16, 1668])\n",
            "tensor(0.0045, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 120])\n",
            "Batch Context Shape:  torch.Size([16, 1512])\n",
            "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 63])\n",
            "Batch Context Shape:  torch.Size([16, 678])\n",
            "tensor(0.0067, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 117])\n",
            "Batch Context Shape:  torch.Size([16, 641])\n",
            "tensor(0.0045, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 28])\n",
            "Batch Context Shape:  torch.Size([16, 203])\n",
            "tensor(0.0058, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 240])\n",
            "Batch Context Shape:  torch.Size([16, 523])\n",
            "tensor(0.0062, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 221])\n",
            "Batch Context Shape:  torch.Size([16, 687])\n",
            "tensor(0.0067, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 149])\n",
            "Batch Context Shape:  torch.Size([16, 952])\n",
            "tensor(0.0039, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 606])\n",
            "Batch Context Shape:  torch.Size([16, 710])\n",
            "tensor(0.0258, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 216])\n",
            "Batch Context Shape:  torch.Size([16, 288])\n",
            "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 31])\n",
            "Batch Context Shape:  torch.Size([16, 4467])\n",
            "tensor(0.0064, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 75])\n",
            "Batch Context Shape:  torch.Size([16, 487])\n",
            "tensor(0.0039, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 127])\n",
            "Batch Context Shape:  torch.Size([16, 564])\n",
            "tensor(0.0043, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 248])\n",
            "Batch Context Shape:  torch.Size([16, 831])\n",
            "tensor(0.0035, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 78])\n",
            "Batch Context Shape:  torch.Size([16, 480])\n",
            "tensor(0.0090, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 231])\n",
            "Batch Context Shape:  torch.Size([16, 331])\n",
            "tensor(0.0082, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 86])\n",
            "Batch Context Shape:  torch.Size([16, 414])\n",
            "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 307])\n",
            "Batch Context Shape:  torch.Size([16, 423])\n",
            "tensor(0.0073, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 40])\n",
            "Batch Context Shape:  torch.Size([16, 170])\n",
            "tensor(0.0036, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 70])\n",
            "Batch Context Shape:  torch.Size([16, 260])\n",
            "tensor(0.0040, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 2536])\n",
            "Batch Context Shape:  torch.Size([16, 453])\n",
            "tensor(0.0090, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 87])\n",
            "Batch Context Shape:  torch.Size([16, 2459])\n",
            "tensor(0.0029, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 122])\n",
            "Batch Context Shape:  torch.Size([16, 514])\n",
            "tensor(0.0037, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 112])\n",
            "Batch Context Shape:  torch.Size([16, 468])\n",
            "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 326])\n",
            "Batch Context Shape:  torch.Size([16, 672])\n",
            "tensor(0.0053, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 141])\n",
            "Batch Context Shape:  torch.Size([16, 5141])\n",
            "tensor(0.0086, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 55])\n",
            "Batch Context Shape:  torch.Size([16, 430])\n",
            "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 820])\n",
            "Batch Context Shape:  torch.Size([16, 327])\n",
            "tensor(0.0079, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 225])\n",
            "Batch Context Shape:  torch.Size([16, 520])\n",
            "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 65])\n",
            "Batch Context Shape:  torch.Size([16, 635])\n",
            "tensor(0.0038, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 237])\n",
            "Batch Context Shape:  torch.Size([16, 694])\n",
            "tensor(0.0532, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 26])\n",
            "Batch Context Shape:  torch.Size([16, 407])\n",
            "tensor(0.0057, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 72])\n",
            "Batch Context Shape:  torch.Size([16, 414])\n",
            "tensor(0.0051, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 488])\n",
            "Batch Context Shape:  torch.Size([16, 641])\n",
            "tensor(0.0056, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 104])\n",
            "Batch Context Shape:  torch.Size([16, 655])\n",
            "tensor(0.0065, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 137])\n",
            "Batch Context Shape:  torch.Size([16, 9921])\n",
            "tensor(0.0078, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 22])\n",
            "Batch Context Shape:  torch.Size([16, 328])\n",
            "tensor(0.0187, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 91])\n",
            "Batch Context Shape:  torch.Size([16, 202])\n",
            "tensor(0.0222, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 108])\n",
            "Batch Context Shape:  torch.Size([16, 622])\n",
            "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 59])\n",
            "Batch Context Shape:  torch.Size([16, 867])\n",
            "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 102])\n",
            "Batch Context Shape:  torch.Size([16, 1843])\n",
            "tensor(0.0058, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 190])\n",
            "Batch Context Shape:  torch.Size([16, 624])\n",
            "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 77])\n",
            "Batch Context Shape:  torch.Size([16, 371])\n",
            "tensor(0.0027, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 97])\n",
            "Batch Context Shape:  torch.Size([16, 541])\n",
            "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 47])\n",
            "Batch Context Shape:  torch.Size([16, 1190])\n",
            "tensor(0.0031, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 380])\n",
            "Batch Context Shape:  torch.Size([16, 515])\n",
            "tensor(0.0041, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 182])\n",
            "Batch Context Shape:  torch.Size([16, 665])\n",
            "tensor(0.0027, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 80])\n",
            "Batch Context Shape:  torch.Size([16, 416])\n",
            "tensor(0.0028, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 63])\n",
            "Batch Context Shape:  torch.Size([16, 286])\n",
            "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 212])\n",
            "Batch Context Shape:  torch.Size([16, 615])\n",
            "tensor(0.0062, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 95])\n",
            "Batch Context Shape:  torch.Size([16, 181])\n",
            "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 99])\n",
            "Batch Context Shape:  torch.Size([16, 376])\n",
            "tensor(0.0031, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 241])\n",
            "Batch Context Shape:  torch.Size([16, 1704])\n",
            "tensor(0.0022, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 286])\n",
            "Batch Context Shape:  torch.Size([16, 441])\n",
            "tensor(0.0069, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 153])\n",
            "Batch Context Shape:  torch.Size([16, 649])\n",
            "tensor(0.0037, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 53])\n",
            "Batch Context Shape:  torch.Size([16, 389])\n",
            "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 411])\n",
            "Batch Context Shape:  torch.Size([16, 1220])\n",
            "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 16])\n",
            "Batch Context Shape:  torch.Size([16, 381])\n",
            "tensor(0.0077, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 68])\n",
            "Batch Context Shape:  torch.Size([16, 301])\n",
            "tensor(0.0027, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 441])\n",
            "Batch Context Shape:  torch.Size([16, 667])\n",
            "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 42])\n",
            "Batch Context Shape:  torch.Size([16, 323])\n",
            "tensor(0.0031, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 105])\n",
            "Batch Context Shape:  torch.Size([16, 478])\n",
            "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 118])\n",
            "Batch Context Shape:  torch.Size([16, 365])\n",
            "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 82])\n",
            "Batch Context Shape:  torch.Size([16, 306])\n",
            "tensor(0.0026, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 744])\n",
            "Batch Context Shape:  torch.Size([16, 388])\n",
            "tensor(0.0040, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 110])\n",
            "Batch Context Shape:  torch.Size([16, 1584])\n",
            "tensor(0.0043, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 295])\n",
            "Batch Context Shape:  torch.Size([16, 1370])\n",
            "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 158])\n",
            "Batch Context Shape:  torch.Size([16, 530])\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 920])\n",
            "Batch Context Shape:  torch.Size([16, 1482])\n",
            "tensor(0.0065, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1293])\n",
            "Batch Context Shape:  torch.Size([16, 874])\n",
            "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 49])\n",
            "Batch Context Shape:  torch.Size([16, 446])\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 194])\n",
            "Batch Context Shape:  torch.Size([16, 790])\n",
            "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 165])\n",
            "Batch Context Shape:  torch.Size([16, 1298])\n",
            "tensor(0.0043, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 186])\n",
            "Batch Context Shape:  torch.Size([16, 6214])\n",
            "tensor(0.0028, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 35])\n",
            "Batch Context Shape:  torch.Size([16, 536])\n",
            "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 274])\n",
            "Batch Context Shape:  torch.Size([16, 516])\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 89])\n",
            "Batch Context Shape:  torch.Size([16, 457])\n",
            "tensor(0.0024, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 90])\n",
            "Batch Context Shape:  torch.Size([16, 508])\n",
            "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 171])\n",
            "Batch Context Shape:  torch.Size([16, 391])\n",
            "tensor(0.0035, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 265])\n",
            "Batch Context Shape:  torch.Size([16, 466])\n",
            "tensor(0.0050, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 116])\n",
            "Batch Context Shape:  torch.Size([16, 542])\n",
            "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 539])\n",
            "Batch Context Shape:  torch.Size([16, 2359])\n",
            "tensor(0.0042, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 94])\n",
            "Batch Context Shape:  torch.Size([16, 712])\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 204])\n",
            "Batch Context Shape:  torch.Size([16, 550])\n",
            "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 51])\n",
            "Batch Context Shape:  torch.Size([16, 252])\n",
            "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 134])\n",
            "Batch Context Shape:  torch.Size([16, 363])\n",
            "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 179])\n",
            "Batch Context Shape:  torch.Size([16, 2825])\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 256])\n",
            "Batch Context Shape:  torch.Size([16, 382])\n",
            "tensor(0.0050, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 1025])\n",
            "Batch Context Shape:  torch.Size([16, 2060])\n",
            "tensor(0.0022, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 280])\n",
            "Batch Context Shape:  torch.Size([16, 1200])\n",
            "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 125])\n",
            "Batch Context Shape:  torch.Size([16, 490])\n",
            "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 84])\n",
            "Batch Context Shape:  torch.Size([16, 494])\n",
            "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 662])\n",
            "Batch Context Shape:  torch.Size([16, 675])\n",
            "tensor(0.0035, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 145])\n",
            "Batch Context Shape:  torch.Size([16, 832])\n",
            "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 45])\n",
            "Batch Context Shape:  torch.Size([16, 441])\n",
            "tensor(0.0070, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 163])\n",
            "Batch Context Shape:  torch.Size([16, 947])\n",
            "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
            "Batch Size :  16\n",
            "Batch Context Shape:  torch.Size([16, 9738])\n",
            "Batch Context Shape:  torch.Size([16, 337])\n",
            "tensor(0.0047, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-ce0c4edc7d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0moptimizer_AE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}