{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "I_CWymNAONAu"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hood-boi/world-news-chatbot/blob/master/Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9C6JMOs28bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj_alb-48wYJ",
        "colab_type": "code",
        "outputId": "9eb36a28-a476-4927-aa57-359e8c38241b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVqrqFbAMQQ6",
        "colab_type": "code",
        "outputId": "2f618523-97ff-41cc-d069-c1400921f48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\"\n",
        "                                 )       # to turn each character into an integer index\n",
        "label_field = torchtext.data.Field(sequential=True,    # text sequence\n",
        "                                   use_vocab=True,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   tokenize=lambda x: x,\n",
        "                                   preprocessing=lambda x: x,\n",
        "                                   init_token=\"<BOS>\",\n",
        "                                   eos_token=\"<EOS>\"\n",
        "                                  ) \n",
        "\n",
        "fields = [('reply', label_field), ('context', text_field)]\n",
        "dataset1 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rtodayilearned.txt\", \"tsv\", fields); # name of the file\n",
        "dataset2 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rAskReddit.txt\", \"tsv\", fields); # name of the file\n",
        "dataset3 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rnews.txt\", \"tsv\", fields); # name of the file\n",
        "dataset4 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rworldnews.txt\", \"tsv\", fields); # name of the file\n",
        "dataset5 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/routoftheloop.txt\", \"tsv\", fields); # name of the file\n",
        "dataset6 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rnottheonion.txt\", \"tsv\", fields); # name of the file\n",
        "dataset7 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/rCasualConversation.txt\", \"tsv\", fields); # name of the file\n",
        "dataset8 = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/text.txt\", \"tsv\", fields);\n",
        "dataset = torch.utils.data.ConcatDataset([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8])\n",
        "\n",
        "train = torchtext.data.Dataset(dataset, fields)\n",
        "text_field.build_vocab(train)\n",
        "label_field.build_vocab(train)\n",
        "print(text_field.vocab.itos)\n",
        "print(label_field.vocab.itos)\n",
        "input_vocab_size = len(text_field.vocab.itos)\n",
        "reply_vocab_size = len(label_field.vocab.itos)\n",
        "print(\"Input Vocab Size: \", input_vocab_size)\n",
        "print(\"Reply Vocab size: \", reply_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'g', 'y', 'p', 'f', 'w', 'b', '.', 'k', 'v', ',', 'I', 'T', 'S', 'A', 'C', 'W', '-', '/', '0', '?', 'M', 'x', 'j', 'P', 'H', 'B', 'N', 'D', 'E', '1', ':', 'R', 'F', 'O', '2', 'z', 'G', 'L', 'U', '*', '!', 'Y', 'J', 'K', '5', ')', '(', 'q', '3', '4', ';', '9', '8', '6', '7', 'V', '_', '$', ']', '[', '%', '=', '^', '~', 'Z', '&', 'Q', 'X', '#', '+', '\\\\', '|', '@', '`', '{', '}']\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'y', 'g', 'w', 'p', 'f', '.', 'b', 'k', 'v', 'I', ',', 'T', 'A', 'S', '/', '?', 'j', 'W', '-', 'x', 'H', 'C', '0', 'M', 'N', '!', 'B', 'E', 'O', 'D', 'P', 'R', '1', 'Y', ':', '*', 'F', 'z', 'L', 'G', '2', 'U', ')', '(', 'q', ';', 'J', '5', '3', 'K', '4', '9', '_', '8', '6', '7', 'V', ']', '[', '^', '$', '%', '=', '~', '&', 'Q', 'Z', 'X', '#', '\\\\', '+', '@', '|', '`', '}', '{']\n",
            "Input Vocab Size:  95\n",
            "Reply Vocab size:  95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_CWymNAONAu",
        "colab_type": "text"
      },
      "source": [
        "# >>> SEQ2SEQ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87d6AzBR73J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size,  \n",
        "                 reply_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ChatBot, self).__init__()\n",
        "        \n",
        "        self.encoder_layers = encoder_layers;\n",
        "        self.generator_layers =generator_layers;\n",
        "        self.encoder_hidden_size = encoder_hidden_size;\n",
        "        self.generator_hidden_size = generator_hidden_size;\n",
        "        \n",
        "        # >>> Encoder\n",
        "        self.context_ident = torch.eye(context_vocab_size)\n",
        "        self.encode_rnn = nn.LSTM(context_vocab_size, encoder_hidden_size, encoder_layers, batch_first=True)\n",
        "        \n",
        "        # >>> Generator\n",
        "        self.reply_ident = torch.eye(reply_vocab_size)\n",
        "        self.decode_rnn = nn.LSTM(reply_vocab_size, generator_hidden_size, generator_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(generator_hidden_size, reply_vocab_size)\n",
        "        \n",
        "    def forward(self, context, response, hidden=None):\n",
        "        \n",
        "        # >>> Encoder\n",
        "        context_tensor = self.context_ident[context] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        encode_out, encode_last_hidden = self.encode_rnn(context_tensor)\n",
        "        \n",
        "        # >>> Generator\n",
        "        reply_tensor = self.reply_ident[response] #Type: batch.reply\n",
        "        if(hidden == None):\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, encode_last_hidden)\n",
        "        else:\n",
        "            gen_out, gen_last_hidden = self.decode_rnn(reply_tensor, hidden)\n",
        "        out = self.fcnn(gen_out)\n",
        "        return out, gen_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kH5NXCO8Bqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_sequence(model, context, text_field_itos, label_field_itos, text_field_stoi, label_field_stoi, max_len=100, temperature=0.1):\n",
        "    generated_sequence = \"\"\n",
        "    \n",
        "    data_inp = [\"<BOS>\"] + list(context) + [\"<EOS>\"];\n",
        "    data_inp_indices = [text_field_stoi[ch] for ch in data_inp];\n",
        "    data_inp_tensor = torch.Tensor(data_inp_indices).long().unsqueeze(0);\n",
        "    \n",
        "    inp = torch.Tensor([label_field_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None;\n",
        "    for p in range(max_len):\n",
        "        #print(inp)\n",
        "        output, hidden = model(data_inp_tensor, inp.unsqueeze(0), hidden)\n",
        "        #print(output)\n",
        "        #print(output.shape)\n",
        "        \n",
        "        #output = F.softmax(output, dim=2)\n",
        "        #print(torch.argmax(output, dim=2))\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        #print(output_dist)\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = label_field_itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZAhutlh8Des",
        "colab_type": "code",
        "outputId": "33e37643-a2b5-4af8-d5d8-2bf4112dc191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_model = ChatBot(input_vocab_size, reply_vocab_size, encoder_layers = 2, generator_layers = 2)\n",
        "test_model.load_state_dict(torch.load('/content/gdrive/My Drive/Chatbot/MM_WEIGHTS_12'))\n",
        "test_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatBot(\n",
              "  (encode_rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "  (decode_rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "  (fcnn): Linear(in_features=100, out_features=95, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6uDDv1tbIuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq(input):\n",
        "    response = sample_sequence(test_model, input, text_field.vocab.itos, label_field.vocab.itos, text_field.vocab.stoi, label_field.vocab.stoi, temperature = 0.5, max_len=500);\n",
        "    return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF8_W8o1NwuA",
        "colab_type": "code",
        "outputId": "0e35f03d-8468-4142-b73c-da7a78c44a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seq2seq(\"Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai. Ajith Pai.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yeah, I got a real state and the proper confused that was to say, and they are not saying it is such a surprise in the end of the control the thread in the last pain. I think they were a cops are supposedly assume it has a full of pain for profit. Its a different day where it was from the most part of the people to open a fun that are allowed to pay personal college people on the case then because the manufacturers was a lot of the shit are saying other than the same stuff and stuff a lot of peo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoyawQ5gOIOn",
        "colab_type": "text"
      },
      "source": [
        "# >>> BASELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpNoFxsEN81k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size = 100,\n",
        "                hidden_layers = 1):\n",
        "        \n",
        "        super(EncoderAE, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size;\n",
        "        self.layers = hidden_layers;\n",
        "        self.hidden_size = hidden_size;\n",
        "        \n",
        "        self.ident = torch.eye(vocab_size)\n",
        "        self.rnn = nn.LSTM(vocab_size, hidden_size, hidden_layers, batch_first=True)\n",
        "        \n",
        "    def forward(self, target, hidden=None):\n",
        "        target_tensor = self.ident[target] # Type: batch.context[0] | Size: (batch size, sequence size)\n",
        "        out, last_hidden = self.rnn(target_tensor)\n",
        "        \n",
        "        return out, last_hidden;\n",
        "    \n",
        "class DecoderAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size = 100,\n",
        "                hidden_layers = 1):\n",
        "        \n",
        "        super(DecoderAE, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size;\n",
        "        self.layers = hidden_layers;\n",
        "        self.hidden_size = hidden_size;\n",
        "        \n",
        "        self.ident = torch.eye(vocab_size)\n",
        "        self.rnn = nn.LSTM(vocab_size, hidden_size, hidden_layers, batch_first=True)\n",
        "        self.fcnn = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, target, embedding, hidden=None):\n",
        "        target_tensor =  self.ident[target]\n",
        "        if(hidden == None):\n",
        "            out, last_hidden = self.rnn(target_tensor, embedding)\n",
        "        else:\n",
        "            out, last_hidden = self.rnn(target_tensor, hidden)\n",
        "        out_final = self.fcnn(out)\n",
        "        return out_final, last_hidden\n",
        "    \n",
        "class ContextAE(nn.Module):\n",
        "    def __init__(self, \n",
        "                 context_vocab_size, \n",
        "                 encoder_hidden_size = 100,\n",
        "                 generator_hidden_size = 100, \n",
        "                 encoder_layers = 1, \n",
        "                 generator_layers = 1):\n",
        "        \n",
        "        super(ContextAE, self).__init__()\n",
        "        \n",
        "        self.encoder = EncoderAE(context_vocab_size,encoder_hidden_size,encoder_layers)\n",
        "        self.decoder = DecoderAE(context_vocab_size,generator_hidden_size,generator_layers)\n",
        "        \n",
        "    def forward(self, target, hidden = None):\n",
        "        encode_out, encode_last_hidden = self.encoder(target, hidden)\n",
        "        decode_out, decode_last_hidden = self.decoder(target, encode_last_hidden,hidden)\n",
        "        \n",
        "        return decode_out, decode_last_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYwYo5QGO4-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ContextAEModel = ContextAE(input_vocab_size, encoder_hidden_size = 100, generator_hidden_size =100, encoder_layers = 2, generator_layers = 2)\n",
        "ContextAEModel.load_state_dict(torch.load('/content/gdrive/My Drive/Chatbot/CONTEXT_AE_WEIGHTS_12'))\n",
        "ContextAEModel = ContextAEModel.cuda();\n",
        "ContextAEModel.encoder.ident = ContextAEModel.encoder.ident.cuda();\n",
        "ContextAEModel.decoder.ident = ContextAEModel.decoder.ident.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpms_6B6PEQ6",
        "colab_type": "code",
        "outputId": "c83d9703-4820-490a-cb36-6bf0c562e04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "ContextAEModel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ContextAE(\n",
              "  (encoder): EncoderAE(\n",
              "    (rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "  )\n",
              "  (decoder): DecoderAE(\n",
              "    (rnn): LSTM(95, 100, num_layers=2, batch_first=True)\n",
              "    (fcnn): Linear(in_features=100, out_features=95, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovjJC_C1PPku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###WARNING : THIS IS THE SLOWEST FUNCTION\n",
        "def baseLineModel(model, inp):\n",
        "    \n",
        "\n",
        "    #Get input embedding    \n",
        "    inp = [\"<BOS>\"] + list(inp) + [\"<EOS>\"]; # inp is a string\n",
        "    inp_indices = [text_field.vocab.stoi[ch] for ch in inp];\n",
        "    inp_tensor = torch.Tensor(inp_indices).long().unsqueeze(0);\n",
        "    _, inp_embeddings = model(inp_tensor)\n",
        "    inp_embeddings = inp_embeddings[0].squeeze(0)\n",
        "    inp_embeddings = torch.reshape(inp_embeddings, (-1, 200))\n",
        "    \n",
        "    \n",
        "    \n",
        "    max_similarity = 0;\n",
        "    reply = \"\";\n",
        "    \n",
        "    i = 0;\n",
        "    \n",
        "    for data in dataset:\n",
        "        #print(\"here 1\")\n",
        "        data_inp = [\"<BOS>\"] + list(data.context) + [\"<EOS>\"]; # inp is a string\n",
        "        data_inp_indices = [text_field.vocab.stoi[ch] for ch in data_inp];\n",
        "        data_inp_tensor = torch.Tensor(data_inp_indices).long().unsqueeze(0);\n",
        "        _, emb = model(data_inp_tensor);\n",
        "        embed = emb[0].squeeze(0)\n",
        "        embed = torch.reshape(embed, (-1, 200))\n",
        "        #print(\"here 2\")\n",
        "        cosine_sim = torch.cosine_similarity(inp_embeddings, embed);\n",
        "        #print(\"here 3\")\n",
        "        #print(cosine_sim)\n",
        "        #print(cosine_sim , \"Context : \", context)\n",
        "        if(cosine_sim > max_similarity):\n",
        "            max_similarity = cosine_sim;\n",
        "            reply = data.reply;\n",
        "            #print(reply);\n",
        "\n",
        "    return reply;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPlupFjYPsgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseLineModel(ContextAEModel.encoder, \"Who makes the call? Congress or the fcc?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIDtK0-kPz7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline(input):\n",
        "    response = baseLineModel(ContextAEModel.encoder, \"Who makes the call? Congress or the fcc?\")\n",
        "    return response;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZE989weQBUE",
        "colab_type": "text"
      },
      "source": [
        "# >>> Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX6OLjdNQsMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ResponseAEModel = ContextAE(input_vocab_size, encoder_hidden_size = 100, generator_hidden_size =100, encoder_layers = 1, generator_layers = 1)\n",
        "ResponseAEModel.load_state_dict(torch.load('/content/gdrive/My Drive/Chatbot/RESPONSE_AE_WEIGHTS_17'))\n",
        "ResponseAEModel = ResponseAEModel.cuda();\n",
        "ResponseAEModel.encoder.ident = ResponseAEModel.encoder.ident.cuda();\n",
        "ResponseAEModel.decoder.ident = ResponseAEModel.decoder.ident.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFcVFd9PTnl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = torchtext.data.TabularDataset(\"/content/gdrive/My Drive/Chatbot/validation.txt\", \"tsv\", fields);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBFnmE0NT8-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = 0;\n",
        "seq2seq_score = 0;\n",
        "baseline_score = 0;\n",
        "\n",
        "for data in val_data:\n",
        "    total += 1;\n",
        "    seq2seq_response = seq2seq(data.context)\n",
        "    baseline_response = baseline(data.context);\n",
        "    actual_response = data.reply;\n",
        "    \n",
        "    seq2seq_response_onehot = torch.Tensor([label_field.vocab.stoi[ch] for ch in ([\"<BOS>\"] + list(seq2seq_response) + [\"<EOS>\"])]).long().unsqueeze(0);\n",
        "    baseline_response_onehot = torch.Tensor([label_field.vocab.stoi[ch] for ch in ([\"<BOS>\"] + list(baseline_response) + [\"<EOS>\"])]).long().unsqueeze(0);\n",
        "    actual_response_onehot = torch.Tensor([label_field.vocab.stoi[ch] for ch in ([\"<BOS>\"] + list(actual_response) + [\"<EOS>\"])]).long().unsqueeze(0);\n",
        "    \n",
        "    _, seq2seq_response_emb = ResponseAEModel(seq2seq_response_onehot)\n",
        "    _, baseline_response_emb = ResponseAEModel(baseline_response_onehot)\n",
        "    _, actual_response_emb =ResponseAEModel(actual_response_onehot)\n",
        "    \n",
        "    if (torch.cosine_similarity(seq2seq_response_emb, actual_response_emb) > torch.cosine_similarity(baseline_response_emb, actual_response_emb)):\n",
        "        seq2seq_score += 1;\n",
        "    else:\n",
        "        baseline_score += 1;\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY0aZ43OXJwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Seq2Seq % : \", (seq2seq_score/total) * 100);\n",
        "print(\"Baseline % : \", (baseline_score/total) * 100);"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}